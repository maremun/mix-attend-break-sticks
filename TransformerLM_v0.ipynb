{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised and fixed code from https://github.com/JayParks/transformer (MT) for LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripped the code from the JayParks repo for MT Transformer. Introduced a few updates and changes for speed, but it's still frustratingly slow. Possible improvement - speed it up.\n",
    "\n",
    "Another issue - hyperparameter search for language modelling (number of heads, number of self-attention layers, etc). Does not work well from the box. This might be of help https://arxiv.org/pdf/1804.00247.pdf.\n",
    "\n",
    "Also consider parallelizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* Clean up\n",
    "* Add MoS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence-wise batching\n",
    "\n",
    "This version of Transformer LM uses sentence-wise batching (each sentence is an inpdependent example).\n",
    "\n",
    "**NB** Before running make sure src code accounts for PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "torch.cuda.device(0)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as pad\n",
    "from torch.nn.utils import clip_grad_norm_ as clip\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import const\n",
    "from data import *\n",
    "from transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "Loading data from data/penn/train.txt ...\n",
      "=========================================================================================\n",
      "Loading data from data/penn/valid.txt ...\n",
      "=========================================================================================\n",
      "Loading data from data/penn/test.txt ...\n"
     ]
    }
   ],
   "source": [
    "ptb_datapath_train = 'data/penn/train.txt'\n",
    "ptb_datapath_valid = 'data/penn/valid.txt'\n",
    "ptb_datapath_test = 'data/penn/test.txt'\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ptb_train = DataSet(ptb_datapath_train, batch_size, display_freq=0, max_len=90, trunc_len=90)\n",
    "ptb_valid = DataSet(ptb_datapath_valid, batch_size, display_freq=0, max_len=90, trunc_len=90)\n",
    "ptb_test = DataSet(ptb_datapath_test, batch_size, display_freq=0, max_len=90, trunc_len=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary...\n",
      "Done.\n",
      "Save dictionary at data/penn/train.txt.dict\n",
      "Index tokens ...\n",
      "42068 sentences were processed, 0 longer than maximum length,0 were ignored because zero length\n",
      "=========================================================================================\n",
      "Data discription:\n",
      "Data name : data/penn/train.txt\n",
      "Number of sentence : 42068\n",
      "Number of tokens : 887521\n",
      "Vocabulary size : 10000\n",
      "Number of batches : 328\n",
      "Batch size : 128\n",
      "Done.\n",
      "Index tokens ...\n",
      "3370 sentences were processed, 0 longer than maximum length,0 were ignored because zero length\n",
      "=========================================================================================\n",
      "Data discription:\n",
      "Data name : data/penn/valid.txt\n",
      "Number of sentence : 3370\n",
      "Number of tokens : 70390\n",
      "Vocabulary size : 10000\n",
      "Number of batches : 26\n",
      "Batch size : 128\n",
      "Done.\n",
      "Index tokens ...\n",
      "3761 sentences were processed, 0 longer than maximum length,0 were ignored because zero length\n",
      "=========================================================================================\n",
      "Data discription:\n",
      "Data name : data/penn/test.txt\n",
      "Number of sentence : 3761\n",
      "Number of tokens : 78669\n",
      "Vocabulary size : 10000\n",
      "Number of batches : 29\n",
      "Batch size : 128\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "ptb_train.build_dict()\n",
    "ptb_valid.change_dict(ptb_train.dictionary)\n",
    "ptb_test.change_dict(ptb_train.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ Optional: get data by tokens ###############\n",
    "# corpus = Corpus('data/penn')\n",
    "# eval_batch_size = 10\n",
    "# test_batch_size = 1\n",
    "# batch_size = 128\n",
    "# train_data = batchify(corpus.train, batch_size)\n",
    "# val_data = batchify(corpus.valid, eval_batch_size)\n",
    "# test_data = batchify(corpus.test, test_batch_size)\n",
    "\n",
    "# #### how to take a batch ####\n",
    "# # the data is already splitten into batch_size(now we need to decide about seq length)\n",
    "# batch_num = 2\n",
    "# batch = get_batch(train_data, batch_num, seq_len=35)\n",
    "\n",
    "\n",
    "# #### TODO (if needed) ###\n",
    "# # 1) repackage hiddens for learning by tokens\n",
    "# # 2) learn not every step (depends on 1st point)\n",
    "# # 3) add grad clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = ptb_train.num_vocb\n",
    "emb_dim = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "d_ff = 2048\n",
    "max_tgt_seq_len = 90\n",
    "dropout = 0.1\n",
    "weighted_model = False\n",
    "share_proj_weight = True\n",
    "lr = 1e-6\n",
    "n_epochs = 10\n",
    "clip_grad = 5\n",
    "warmup_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharing target embedding and projection..\n"
     ]
    }
   ],
   "source": [
    "model = LMTransformer(n_layers, d_k, d_v, emb_dim, d_ff,\n",
    "                      n_heads, max_tgt_seq_len, voc_size,\n",
    "                      dropout, weighted_model, share_proj_weight)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=const.PAD)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "#opt = optim.Adam(model.trainable_params(), lr=lr)\n",
    "# lr_lambda = lambda epoch: 0.99 ** epoch\n",
    "#lrsched = StepLR(opt, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/328 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1, learning rate 0.000001 \n",
      "2\n",
      "emb\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[-1.3810e-01,  7.9967e-02, -8.8474e-01,  ..., -5.7051e-02,\n",
      "         -4.4142e-02, -9.6475e-01],\n",
      "        [ 1.0865e+00, -1.1131e+00,  8.1322e-01,  ...,  6.8507e-01,\n",
      "          3.6231e-01, -1.7098e-01],\n",
      "        [-5.3852e-01,  8.0949e-01, -2.0556e-01,  ...,  6.1222e-01,\n",
      "         -6.3589e-01, -8.0007e-01],\n",
      "        ...,\n",
      "        [-3.7327e-02, -6.5427e-01, -1.6921e+00,  ..., -6.6768e-01,\n",
      "          9.1436e-04, -1.0795e-01],\n",
      "        [-5.0567e-02, -7.0362e-01,  2.4117e+00,  ...,  6.2272e-01,\n",
      "         -1.8694e+00,  2.4068e-01],\n",
      "        [-4.7632e-01,  1.7441e+00, -5.9936e-02,  ...,  7.9750e-01,\n",
      "          7.0828e-01,  2.1999e+00]], device='cuda:0')\n",
      "pos emb\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[-1.3810e-01,  1.0800e+00, -8.8474e-01,  ...,  9.4295e-01,\n",
      "         -4.4142e-02,  3.5245e-02],\n",
      "        [ 1.9280e+00, -5.7280e-01,  1.6351e+00,  ...,  1.6851e+00,\n",
      "          3.6241e-01,  8.2902e-01],\n",
      "        [ 3.7078e-01,  3.9334e-01,  7.3085e-01,  ...,  1.6122e+00,\n",
      "         -6.3568e-01,  1.9993e-01],\n",
      "        ...,\n",
      "        [-2.9970e-01,  3.1069e-01, -2.5874e+00,  ...,  3.3231e-01,\n",
      "          6.0975e-03,  8.9204e-01],\n",
      "        [ 6.1966e-01,  3.8530e-02,  1.5356e+00,  ...,  1.6227e+00,\n",
      "         -1.8641e+00,  1.2407e+00],\n",
      "        [ 5.1030e-01,  1.5811e+00, -1.6283e-01,  ...,  1.7975e+00,\n",
      "          7.1367e-01,  3.1999e+00]], device='cuda:0')\n",
      "dropout emb\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[-0.0000,  1.2000, -0.9830,  ...,  1.0477, -0.0490,  0.0392],\n",
      "        [ 2.1422, -0.6364,  1.8168,  ...,  1.8723,  0.4027,  0.9211],\n",
      "        [ 0.4120,  0.0000,  0.8121,  ...,  1.7914, -0.7063,  0.2221],\n",
      "        ...,\n",
      "        [-0.0000,  0.3452, -2.8749,  ...,  0.3692,  0.0068,  0.9912],\n",
      "        [ 0.6885,  0.0428,  1.7062,  ...,  1.8030, -2.0712,  1.3785],\n",
      "        [ 0.5670,  1.7568, -0.1809,  ...,  1.9972,  0.7930,  3.5554]], device='cuda:0')\n",
      "bmm\n",
      "tensor([[ 0.1253,  0.4559, -0.2704,  ...,  0.0453,  0.1764,  0.0152],\n",
      "        [-0.0038, -0.0077, -0.2067,  ...,  0.2255,  0.2578, -0.3591],\n",
      "        [-0.2771, -0.0627,  0.0117,  ..., -0.0798, -0.3294,  0.0458],\n",
      "        ...,\n",
      "        [-0.5288,  0.0288,  0.1044,  ...,  0.2158,  0.2769, -0.2225],\n",
      "        [-0.2175,  0.2721, -0.1432,  ..., -0.0322, -0.0238, -0.1014],\n",
      "        [-0.1776,  0.3475,  0.1190,  ...,  0.0516, -0.4652, -0.0299]], device='cuda:0')\n",
      "scaleddotprodattn\n",
      "torch.Size([512, 53, 53])\n",
      "tensor([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1], dtype=torch.uint8, device='cuda:0')\n",
      "tensor(1.00000e-02 *\n",
      "       [-2.5836,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf], device='cuda:0')\n",
      "tensor([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
      "attention\n",
      "4\n",
      "torch.Size([128, 53, 64])\n",
      "[tensor([[-0.8362, -0.0572, -0.4923,  ...,  0.1998,  0.0990, -0.2664],\n",
      "        [-0.3873, -0.2266, -0.0349,  ...,  0.0522, -0.1044, -0.1066],\n",
      "        [-0.4763, -0.1283,  0.0349,  ...,  0.0873, -0.0837, -0.1712],\n",
      "        ...,\n",
      "        [-0.1093,  0.1810,  0.1243,  ..., -0.0214, -0.1431, -0.0599],\n",
      "        [-0.1505,  0.1878,  0.1285,  ..., -0.0027, -0.1793, -0.0495],\n",
      "        [-0.1416,  0.1606,  0.1218,  ...,  0.0014, -0.1556, -0.0544]], device='cuda:0'), tensor([[ 0.0354,  0.2396,  0.0899,  ...,  0.0931, -0.0161, -0.0564],\n",
      "        [-0.0552,  0.2334,  0.1400,  ...,  0.0145,  0.0659, -0.1618],\n",
      "        [-0.0058,  0.0925,  0.2068,  ...,  0.1522,  0.0784, -0.1438],\n",
      "        ...,\n",
      "        [ 0.0033,  0.0967,  0.0427,  ..., -0.1338, -0.0770, -0.0365],\n",
      "        [-0.0017,  0.0996,  0.0517,  ..., -0.1358, -0.0699, -0.0318],\n",
      "        [-0.0019,  0.1040,  0.0496,  ..., -0.1367, -0.0934, -0.0127]], device='cuda:0'), tensor([[-0.2001,  0.2777, -0.5521,  ...,  0.3189,  0.1947,  0.1826],\n",
      "        [-0.2945,  0.2233, -0.4798,  ...,  0.1564, -0.0555, -0.2187],\n",
      "        [-0.2159,  0.1087, -0.4664,  ...,  0.2282, -0.0066, -0.2384],\n",
      "        ...,\n",
      "        [ 0.0766,  0.2116, -0.3457,  ...,  0.0304, -0.1388, -0.1972],\n",
      "        [ 0.0684,  0.2176, -0.3407,  ...,  0.0303, -0.1415, -0.2196],\n",
      "        [ 0.0617,  0.2338, -0.3520,  ...,  0.0278, -0.1427, -0.2222]], device='cuda:0'), tensor([[-0.1578, -0.1215,  0.8979,  ..., -0.7033, -0.0631,  0.1210],\n",
      "        [-0.0894, -0.0691,  0.5122,  ..., -0.2633, -0.0652,  0.2428],\n",
      "        [-0.1406, -0.1205,  0.3992,  ..., -0.2051, -0.0656,  0.2792],\n",
      "        ...,\n",
      "        [-0.2172,  0.2543,  0.2305,  ..., -0.0640, -0.0282,  0.1306],\n",
      "        [-0.2538,  0.2966,  0.2455,  ..., -0.0738, -0.0165,  0.1184],\n",
      "        [-0.2631,  0.2907,  0.2486,  ..., -0.0681, -0.0111,  0.1380]], device='cuda:0')]\n",
      "proj\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[ 5.8096e-01,  1.4132e-01,  2.8139e-01,  ..., -3.9872e-01,\n",
      "         -2.6899e-01,  3.7956e-02],\n",
      "        [ 3.6612e-01,  1.5473e-02,  1.7280e-01,  ..., -1.6539e-01,\n",
      "         -2.7673e-01,  1.6494e-01],\n",
      "        [ 3.7895e-01, -3.6739e-02,  2.3342e-01,  ..., -2.5628e-01,\n",
      "         -1.7168e-01,  1.3980e-02],\n",
      "        ...,\n",
      "        [ 6.3566e-02, -1.4696e-01,  4.4210e-02,  ..., -9.7521e-02,\n",
      "         -1.5568e-01, -4.4258e-02],\n",
      "        [ 5.3623e-02, -1.6245e-01,  2.5768e-02,  ..., -1.1155e-01,\n",
      "         -1.6546e-01, -3.5029e-02],\n",
      "        [ 5.8073e-02, -1.6079e-01,  2.6614e-02,  ..., -9.3301e-02,\n",
      "         -1.7301e-01, -4.7154e-02]], device='cuda:0')\n",
      "dropout\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[ 0.6455,  0.1570,  0.3127,  ..., -0.4430, -0.2989,  0.0422],\n",
      "        [ 0.4068,  0.0172,  0.1920,  ..., -0.1838, -0.3075,  0.1833],\n",
      "        [ 0.4211, -0.0408,  0.2594,  ..., -0.0000, -0.1908,  0.0155],\n",
      "        ...,\n",
      "        [ 0.0706, -0.1633,  0.0491,  ..., -0.1084, -0.1730, -0.0492],\n",
      "        [ 0.0596, -0.1805,  0.0286,  ..., -0.1239, -0.1838, -0.0389],\n",
      "        [ 0.0645, -0.0000,  0.0296,  ..., -0.1037, -0.1922, -0.0524]], device='cuda:0')\n",
      "dec_self_attn\n",
      "pos_ffn\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[-4.4381e-02,  6.0723e-01, -9.9373e-01,  ..., -6.1196e-02,\n",
      "         -7.4821e-01, -7.1833e-01],\n",
      "        [ 1.2093e+00, -4.1059e-01,  1.3388e+00,  ...,  9.5715e-01,\n",
      "         -2.7104e-01,  3.5014e-01],\n",
      "        [ 1.1396e-01, -4.9844e-01,  6.9938e-01,  ...,  1.1074e+00,\n",
      "         -1.2908e+00, -5.8097e-01],\n",
      "        ...,\n",
      "        [-4.7735e-01, -1.1133e-01, -2.4497e+00,  ...,  1.1931e-01,\n",
      "         -5.5300e-01,  3.3393e-01],\n",
      "        [ 1.2856e-01, -2.6370e-01,  1.3723e+00,  ...,  1.2246e+00,\n",
      "         -2.1332e+00,  6.5049e-01],\n",
      "        [-3.2128e-02,  1.1053e+00, -7.8075e-02,  ...,  1.2190e+00,\n",
      "          1.0608e-01,  2.1640e+00]], device='cuda:0')\n",
      "bmm\n",
      "tensor([[-0.0975, -0.0103,  0.1161,  ..., -0.0157, -0.0071, -0.0694],\n",
      "        [ 0.2787,  0.0731,  0.1413,  ...,  0.3108,  0.1727,  0.2097],\n",
      "        [ 0.0019, -0.0390,  0.2705,  ...,  0.0402, -0.0686, -0.3361],\n",
      "        ...,\n",
      "        [ 0.0935,  0.0157, -0.2002,  ...,  0.0875, -0.1660, -0.1027],\n",
      "        [ 0.0581,  0.1303, -0.1574,  ...,  0.2401,  0.2626,  0.1485],\n",
      "        [-0.0995,  0.3161,  0.1200,  ...,  0.4109,  0.2212,  0.0323]], device='cuda:0')\n",
      "scaleddotprodattn\n",
      "torch.Size([512, 53, 53])\n",
      "tensor([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1], dtype=torch.uint8, device='cuda:0')\n",
      "tensor(1.00000e-03 *\n",
      "       [ 9.7278,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "           -inf,    -inf,    -inf,    -inf], device='cuda:0')\n",
      "tensor([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.], device='cuda:0')\n",
      "attention\n",
      "4\n",
      "torch.Size([128, 53, 64])\n",
      "[tensor([[ 0.2941,  0.3675, -0.2630,  ..., -0.1450,  0.3303,  0.0108],\n",
      "        [ 0.0853,  0.2775, -0.0652,  ..., -0.0183,  0.2804, -0.0926],\n",
      "        [ 0.1758,  0.1738, -0.0103,  ...,  0.0913,  0.1436, -0.0220],\n",
      "        ...,\n",
      "        [-0.1046,  0.0383, -0.1129,  ...,  0.0194,  0.0319, -0.2003],\n",
      "        [-0.1052,  0.0384, -0.1228,  ...,  0.0117,  0.0276, -0.1955],\n",
      "        [-0.1215,  0.0365, -0.1310,  ...,  0.0172,  0.0390, -0.2120]], device='cuda:0'), tensor([[-0.1257, -0.0232,  0.4650,  ..., -0.0620, -0.3827,  0.0100],\n",
      "        [ 0.0429, -0.1590,  0.2392,  ..., -0.1752, -0.3611,  0.1444],\n",
      "        [-0.0401, -0.0413,  0.2467,  ..., -0.1024, -0.3197,  0.0913],\n",
      "        ...,\n",
      "        [-0.0209,  0.1244,  0.1754,  ...,  0.0448, -0.1218,  0.1595],\n",
      "        [-0.0288,  0.1501,  0.1797,  ...,  0.0443, -0.1108,  0.1627],\n",
      "        [-0.0378,  0.1408,  0.1826,  ...,  0.0475, -0.0946,  0.1683]], device='cuda:0'), tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0270,  0.0006,  0.0951,  ...,  0.0979, -0.1613,  0.1945],\n",
      "        [-0.0788, -0.0642,  0.0068,  ...,  0.1977, -0.1800,  0.0505],\n",
      "        ...,\n",
      "        [ 0.0612,  0.0817, -0.0795,  ...,  0.1208, -0.1602,  0.0257],\n",
      "        [ 0.0661,  0.0791, -0.0845,  ...,  0.1349, -0.1685,  0.0285],\n",
      "        [ 0.0577,  0.0748, -0.0790,  ...,  0.1360, -0.1589,  0.0294]], device='cuda:0'), tensor([[ 0.3501,  0.0707, -0.0519,  ..., -0.2174, -0.2767,  0.2556],\n",
      "        [ 0.0818, -0.0393,  0.0361,  ..., -0.1926, -0.3482,  0.1715],\n",
      "        [ 0.1435,  0.0121,  0.0929,  ..., -0.0901, -0.2468,  0.1718],\n",
      "        ...,\n",
      "        [ 0.1922, -0.1313,  0.0053,  ..., -0.1565,  0.0417, -0.0002],\n",
      "        [ 0.1965, -0.1328,  0.0076,  ..., -0.1540,  0.0381, -0.0038],\n",
      "        [ 0.1949, -0.1358,  0.0076,  ..., -0.1567,  0.0433, -0.0016]], device='cuda:0')]\n",
      "proj\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[-4.7389e-02,  1.4655e-01, -6.7251e-02,  ..., -9.8346e-02,\n",
      "         -7.2962e-02, -1.6453e-01],\n",
      "        [ 1.2132e-01,  1.6121e-01, -1.5773e-01,  ..., -3.7642e-02,\n",
      "         -6.1106e-02, -1.0963e-01],\n",
      "        [ 2.1116e-02,  9.8136e-02, -9.7628e-03,  ..., -4.2289e-02,\n",
      "          5.8962e-03, -1.1808e-01],\n",
      "        ...,\n",
      "        [ 3.4050e-02, -1.3317e-01, -1.9821e-01,  ...,  1.0854e-02,\n",
      "         -1.1087e-01, -1.5069e-01],\n",
      "        [ 2.2650e-02, -1.3041e-01, -1.9975e-01,  ...,  1.3380e-02,\n",
      "         -1.1675e-01, -1.6411e-01],\n",
      "        [ 3.5453e-02, -1.2960e-01, -2.0108e-01,  ...,  1.9072e-02,\n",
      "         -1.0710e-01, -1.6528e-01]], device='cuda:0')\n",
      "dropout\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[-0.0527,  0.1628, -0.0747,  ..., -0.1093, -0.0811, -0.1828],\n",
      "        [ 0.0000,  0.1791, -0.1753,  ..., -0.0418, -0.0679, -0.1218],\n",
      "        [ 0.0235,  0.1090, -0.0108,  ..., -0.0470,  0.0066, -0.1312],\n",
      "        ...,\n",
      "        [ 0.0378, -0.1480, -0.2202,  ...,  0.0121, -0.1232, -0.0000],\n",
      "        [ 0.0000, -0.1449, -0.2219,  ...,  0.0149, -0.1297, -0.1823],\n",
      "        [ 0.0394, -0.1440, -0.2234,  ...,  0.0212, -0.1190, -0.1836]], device='cuda:0')\n",
      "dec_self_attn\n",
      "pos_ffn\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[ 4.3651e-01,  7.4155e-01, -7.9324e-01,  ..., -4.3021e-01,\n",
      "         -6.2088e-01, -8.2468e-01],\n",
      "        [ 1.6173e+00, -1.7648e-01,  9.5391e-01,  ...,  6.5519e-01,\n",
      "         -2.3702e-01,  1.8263e-01],\n",
      "        [ 1.8524e-01, -3.5952e-01,  8.9811e-01,  ...,  8.3515e-01,\n",
      "         -1.0608e+00, -7.8259e-01],\n",
      "        ...,\n",
      "        [-4.3526e-01, -3.0870e-01, -2.6042e+00,  ...,  1.3353e-01,\n",
      "         -6.5152e-01,  3.1665e-01],\n",
      "        [ 2.0931e-01, -1.0298e-01,  8.5151e-01,  ...,  1.0827e+00,\n",
      "         -2.4067e+00,  4.6301e-01],\n",
      "        [ 4.3850e-02,  1.1420e+00, -3.1871e-01,  ...,  1.1950e+00,\n",
      "          1.1666e-01,  1.9127e+00]], device='cuda:0')\n",
      "torch.Size([128, 53, 512])\n",
      "tensor([[ 4.3651e-01,  7.4155e-01, -7.9324e-01,  ..., -4.3021e-01,\n",
      "         -6.2088e-01, -8.2468e-01],\n",
      "        [ 1.6173e+00, -1.7648e-01,  9.5391e-01,  ...,  6.5519e-01,\n",
      "         -2.3702e-01,  1.8263e-01],\n",
      "        [ 1.8524e-01, -3.5952e-01,  8.9811e-01,  ...,  8.3515e-01,\n",
      "         -1.0608e+00, -7.8259e-01],\n",
      "        ...,\n",
      "        [-4.3526e-01, -3.0870e-01, -2.6042e+00,  ...,  1.3353e-01,\n",
      "         -6.5152e-01,  3.1665e-01],\n",
      "        [ 2.0931e-01, -1.0298e-01,  8.5151e-01,  ...,  1.0827e+00,\n",
      "         -2.4067e+00,  4.6301e-01],\n",
      "        [ 4.3850e-02,  1.1420e+00, -3.1871e-01,  ...,  1.1950e+00,\n",
      "          1.1666e-01,  1.9127e+00]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt = optim.Adam(model.trainable_params(),betas=(0.9, 0.98), eps=1e-09, lr=lr)\n",
    "i=0\n",
    "for epoch in range(n_epochs):\n",
    "    #lrsched.step()\n",
    "    acc_loss = 0\n",
    "    print('Start epoch %d, learning rate %f '%(epoch + 1, opt.state_dict()['param_groups'][0]['lr']))\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    ptb_train.shuffle()\n",
    "    for batch_idx in tqdm(range(ptb_train.num_batch), unit='batches'):\n",
    "        data, lengths, target = ptb_train.get_batch(batch_idx)\n",
    "        opt.zero_grad()\n",
    "        output, self_attn = model.forward(data, lengths)\n",
    "        break\n",
    "    break\n",
    "#         loss = criterion(output, target.view(-1))\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         acc_loss += loss.item()\n",
    "#         i+=1\n",
    "#         new_lr = np.power(emb_dim, -0.5) * np.min([\n",
    "#             np.power((i), -0.5),\n",
    "#             np.power(warmup_steps, -1.5) * (i)])\n",
    "#         for param_group in opt.param_groups:\n",
    "#             param_group['lr'] = new_lr\n",
    "        \n",
    "#     avg_loss = acc_loss / ptb_train.num_batch\n",
    "#     print('Epoch : %d, Batch : %d / %d, Loss : %f, Perplexity : %f, Time : %f' \n",
    "#           % (epoch + 1, batch_idx, ptb_train.num_batch,\n",
    "#              avg_loss, math.exp(avg_loss),\n",
    "#              time.time() - start_time))\n",
    "\n",
    "#     acc_loss = 0\n",
    "#     model.eval()\n",
    "#     for batch_idx in tqdm(range(ptb_test.num_batch), unit='batches'):\n",
    "#         data, lengths, target = ptb_test[batch_idx]\n",
    "#         output, self_attn = model.forward(data, lengths)\n",
    "#         loss = criterion(output, target.view(-1))\n",
    "#         acc_loss += loss.item()\n",
    "\n",
    "#     val_loss = acc_loss / ptb_test.num_batch\n",
    "#     print('Validation Loss : %f' % val_loss)\n",
    "#     print('Validation Perplexity : %f' % math.exp(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
