{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out Gauss-Logit parametrization from here https://arxiv.org/pdf/1605.06197.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import modelsb as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\n",
    "from telepyth import TelepythClient\n",
    "tp = TelepythClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24308708\n",
      "Model total parameters: 24308708\n"
     ]
    }
   ],
   "source": [
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden, mu, logvar = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def klloss(mu, logvar):\n",
    "    KL = -0.0005 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs, mu, logvar = model(cur_data.cuda(), hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "            kl_loss = klloss(mu, logvar)\n",
    "            loss = raw_loss + kl_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            kl = kl_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f} | kl {:5.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss), kl), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180526-101035\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py:491: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 99.45 | loss  7.54 | ppl  1882.61 | kl  0.01\n",
      "| epoch   1 | 400/1106 batches | lr 20.0000 | ms/batch 96.29 | loss  6.77 | ppl   875.34 | kl  0.01\n",
      "| epoch   1 | 600/1106 batches | lr 20.0000 | ms/batch 93.59 | loss  6.68 | ppl   796.80 | kl  0.02\n",
      "| epoch   1 | 800/1106 batches | lr 20.0000 | ms/batch 93.36 | loss  6.67 | ppl   789.98 | kl  0.01\n",
      "| epoch   1 | 1000/1106 batches | lr 20.0000 | ms/batch 97.48 | loss  6.65 | ppl   771.40 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 112.16s | valid loss  7.03 | valid ppl  1133.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   2 | 200/1106 batches | lr 20.0000 | ms/batch 93.47 | loss  6.64 | ppl   763.82 | kl  0.01\n",
      "| epoch   2 | 400/1106 batches | lr 20.0000 | ms/batch 96.20 | loss  6.57 | ppl   710.99 | kl  0.03\n",
      "| epoch   2 | 600/1106 batches | lr 20.0000 | ms/batch 95.37 | loss  6.59 | ppl   730.87 | kl  0.02\n",
      "| epoch   2 | 800/1106 batches | lr 20.0000 | ms/batch 96.75 | loss  6.66 | ppl   783.54 | kl  0.00\n",
      "| epoch   2 | 1000/1106 batches | lr 20.0000 | ms/batch 96.90 | loss  6.59 | ppl   726.68 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 111.92s | valid loss  6.55 | valid ppl   696.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 | 200/1106 batches | lr 20.0000 | ms/batch 90.64 | loss  6.63 | ppl   756.72 | kl  0.01\n",
      "| epoch   3 | 400/1106 batches | lr 20.0000 | ms/batch 94.50 | loss  6.55 | ppl   701.42 | kl  0.02\n",
      "| epoch   3 | 600/1106 batches | lr 20.0000 | ms/batch 98.90 | loss  6.55 | ppl   702.16 | kl  0.03\n",
      "| epoch   3 | 800/1106 batches | lr 20.0000 | ms/batch 97.58 | loss  6.58 | ppl   719.05 | kl  0.01\n",
      "| epoch   3 | 1000/1106 batches | lr 20.0000 | ms/batch 95.02 | loss  6.60 | ppl   732.81 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 112.02s | valid loss  6.63 | valid ppl   755.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 | 200/1106 batches | lr 20.0000 | ms/batch 94.55 | loss  6.59 | ppl   725.23 | kl  0.02\n",
      "| epoch   4 | 400/1106 batches | lr 20.0000 | ms/batch 91.95 | loss  6.54 | ppl   690.33 | kl  0.03\n",
      "| epoch   4 | 600/1106 batches | lr 20.0000 | ms/batch 94.82 | loss  6.57 | ppl   710.96 | kl  0.01\n",
      "| epoch   4 | 800/1106 batches | lr 20.0000 | ms/batch 97.01 | loss  6.59 | ppl   728.33 | kl  0.03\n",
      "| epoch   4 | 1000/1106 batches | lr 20.0000 | ms/batch 94.06 | loss  6.59 | ppl   728.07 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 110.13s | valid loss  6.53 | valid ppl   687.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 | 200/1106 batches | lr 20.0000 | ms/batch 97.04 | loss  6.58 | ppl   721.87 | kl  0.03\n",
      "| epoch   5 | 400/1106 batches | lr 20.0000 | ms/batch 94.37 | loss  6.54 | ppl   694.91 | kl  0.01\n",
      "| epoch   5 | 600/1106 batches | lr 20.0000 | ms/batch 95.12 | loss  6.54 | ppl   690.50 | kl  0.02\n",
      "| epoch   5 | 800/1106 batches | lr 20.0000 | ms/batch 95.95 | loss  6.58 | ppl   718.00 | kl  0.02\n",
      "| epoch   5 | 1000/1106 batches | lr 20.0000 | ms/batch 93.84 | loss  6.56 | ppl   708.60 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 111.95s | valid loss  6.54 | valid ppl   692.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 | 200/1106 batches | lr 20.0000 | ms/batch 95.46 | loss  6.59 | ppl   725.70 | kl  0.00\n",
      "| epoch   6 | 400/1106 batches | lr 20.0000 | ms/batch 98.90 | loss  6.55 | ppl   697.73 | kl  0.01\n",
      "| epoch   6 | 600/1106 batches | lr 20.0000 | ms/batch 95.61 | loss  6.54 | ppl   693.52 | kl  0.03\n",
      "| epoch   6 | 800/1106 batches | lr 20.0000 | ms/batch 93.97 | loss  6.58 | ppl   721.22 | kl  0.00\n",
      "| epoch   6 | 1000/1106 batches | lr 20.0000 | ms/batch 93.92 | loss  6.56 | ppl   707.81 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 111.58s | valid loss  6.54 | valid ppl   694.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 | 200/1106 batches | lr 20.0000 | ms/batch 97.50 | loss  6.58 | ppl   718.78 | kl  0.02\n",
      "| epoch   7 | 400/1106 batches | lr 20.0000 | ms/batch 92.83 | loss  6.55 | ppl   700.05 | kl  0.01\n",
      "| epoch   7 | 600/1106 batches | lr 20.0000 | ms/batch 94.60 | loss  6.55 | ppl   698.04 | kl  0.03\n",
      "| epoch   7 | 800/1106 batches | lr 20.0000 | ms/batch 97.17 | loss  6.57 | ppl   711.64 | kl  0.00\n",
      "| epoch   7 | 1000/1106 batches | lr 20.0000 | ms/batch 94.02 | loss  6.56 | ppl   707.22 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 111.47s | valid loss  6.60 | valid ppl   731.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 | 200/1106 batches | lr 20.0000 | ms/batch 96.49 | loss  6.59 | ppl   727.91 | kl  0.00\n",
      "| epoch   8 | 400/1106 batches | lr 20.0000 | ms/batch 91.89 | loss  6.55 | ppl   696.65 | kl  0.00\n",
      "| epoch   8 | 600/1106 batches | lr 20.0000 | ms/batch 92.34 | loss  6.54 | ppl   690.91 | kl  0.05\n",
      "| epoch   8 | 800/1106 batches | lr 20.0000 | ms/batch 94.42 | loss  6.57 | ppl   709.88 | kl  0.04\n",
      "| epoch   8 | 1000/1106 batches | lr 20.0000 | ms/batch 97.61 | loss  6.58 | ppl   717.81 | kl  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 111.28s | valid loss  6.55 | valid ppl   696.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 | 200/1106 batches | lr 20.0000 | ms/batch 96.00 | loss  6.58 | ppl   717.11 | kl  0.01\n",
      "| epoch   9 | 400/1106 batches | lr 20.0000 | ms/batch 95.10 | loss  6.54 | ppl   688.84 | kl  0.04\n",
      "| epoch   9 | 600/1106 batches | lr 20.0000 | ms/batch 93.48 | loss  6.54 | ppl   690.41 | kl  0.03\n",
      "| epoch   9 | 800/1106 batches | lr 20.0000 | ms/batch 95.45 | loss  6.58 | ppl   722.08 | kl  0.02\n",
      "| epoch   9 | 1000/1106 batches | lr 20.0000 | ms/batch 96.71 | loss  6.56 | ppl   704.64 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 111.84s | valid loss  6.58 | valid ppl   724.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 | 200/1106 batches | lr 20.0000 | ms/batch 96.42 | loss  6.59 | ppl   731.40 | kl  0.01\n",
      "| epoch  10 | 400/1106 batches | lr 20.0000 | ms/batch 96.37 | loss  6.54 | ppl   694.16 | kl  0.02\n",
      "| epoch  10 | 600/1106 batches | lr 20.0000 | ms/batch 96.34 | loss  6.54 | ppl   689.21 | kl  0.00\n",
      "| epoch  10 | 800/1106 batches | lr 20.0000 | ms/batch 93.50 | loss  6.57 | ppl   713.52 | kl  0.00\n",
      "| epoch  10 | 1000/1106 batches | lr 20.0000 | ms/batch 97.21 | loss  6.56 | ppl   707.43 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 112.80s | valid loss  6.60 | valid ppl   737.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 | 200/1106 batches | lr 20.0000 | ms/batch 95.84 | loss  6.58 | ppl   717.10 | kl  0.00\n",
      "| epoch  11 | 400/1106 batches | lr 20.0000 | ms/batch 97.68 | loss  6.53 | ppl   685.02 | kl  0.00\n",
      "| epoch  11 | 600/1106 batches | lr 20.0000 | ms/batch 97.08 | loss  6.53 | ppl   685.79 | kl  0.00\n",
      "| epoch  11 | 800/1106 batches | lr 20.0000 | ms/batch 95.93 | loss  6.56 | ppl   703.57 | kl  0.02\n",
      "| epoch  11 | 1000/1106 batches | lr 20.0000 | ms/batch 93.87 | loss  6.56 | ppl   707.26 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 112.74s | valid loss  6.54 | valid ppl   695.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 | 200/1106 batches | lr 20.0000 | ms/batch 97.42 | loss  6.59 | ppl   727.89 | kl  0.01\n",
      "| epoch  12 | 400/1106 batches | lr 20.0000 | ms/batch 96.70 | loss  6.52 | ppl   680.44 | kl  0.01\n",
      "| epoch  12 | 600/1106 batches | lr 20.0000 | ms/batch 95.12 | loss  6.53 | ppl   682.62 | kl  0.03\n",
      "| epoch  12 | 800/1106 batches | lr 20.0000 | ms/batch 96.96 | loss  6.56 | ppl   704.34 | kl  0.02\n",
      "| epoch  12 | 1000/1106 batches | lr 20.0000 | ms/batch 95.69 | loss  6.55 | ppl   700.41 | kl  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 112.45s | valid loss  6.58 | valid ppl   722.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 | 200/1106 batches | lr 20.0000 | ms/batch 91.04 | loss  6.57 | ppl   711.38 | kl  0.01\n",
      "| epoch  13 | 400/1106 batches | lr 20.0000 | ms/batch 93.52 | loss  6.55 | ppl   699.42 | kl  0.00\n",
      "| epoch  13 | 600/1106 batches | lr 20.0000 | ms/batch 93.31 | loss  6.55 | ppl   696.82 | kl  0.01\n",
      "| epoch  13 | 800/1106 batches | lr 20.0000 | ms/batch 94.27 | loss  6.56 | ppl   704.06 | kl  0.02\n",
      "| epoch  13 | 1000/1106 batches | lr 20.0000 | ms/batch 96.57 | loss  6.56 | ppl   706.35 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 110.14s | valid loss  6.58 | valid ppl   721.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 | 200/1106 batches | lr 20.0000 | ms/batch 96.93 | loss  6.56 | ppl   706.33 | kl  0.02\n",
      "| epoch  14 | 400/1106 batches | lr 20.0000 | ms/batch 95.03 | loss  6.54 | ppl   689.07 | kl  0.02\n",
      "| epoch  14 | 600/1106 batches | lr 20.0000 | ms/batch 93.78 | loss  6.52 | ppl   681.96 | kl  0.03\n",
      "| epoch  14 | 800/1106 batches | lr 20.0000 | ms/batch 96.67 | loss  6.55 | ppl   698.91 | kl  0.02\n",
      "| epoch  14 | 1000/1106 batches | lr 20.0000 | ms/batch 94.45 | loss  6.55 | ppl   700.93 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 110.44s | valid loss  6.56 | valid ppl   708.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 | 200/1106 batches | lr 20.0000 | ms/batch 95.77 | loss  6.56 | ppl   706.81 | kl  0.01\n",
      "| epoch  15 | 400/1106 batches | lr 20.0000 | ms/batch 95.83 | loss  6.52 | ppl   680.54 | kl  0.00\n",
      "| epoch  15 | 600/1106 batches | lr 20.0000 | ms/batch 93.50 | loss  6.53 | ppl   685.21 | kl  0.01\n",
      "| epoch  15 | 800/1106 batches | lr 20.0000 | ms/batch 93.39 | loss  6.56 | ppl   706.97 | kl  0.05\n",
      "| epoch  15 | 1000/1106 batches | lr 20.0000 | ms/batch 94.15 | loss  6.56 | ppl   705.07 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 110.96s | valid loss  6.54 | valid ppl   689.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 | 200/1106 batches | lr 20.0000 | ms/batch 95.44 | loss  6.57 | ppl   716.25 | kl  0.02\n",
      "| epoch  16 | 400/1106 batches | lr 20.0000 | ms/batch 94.07 | loss  6.53 | ppl   684.01 | kl  0.01\n",
      "| epoch  16 | 600/1106 batches | lr 20.0000 | ms/batch 94.26 | loss  6.53 | ppl   682.47 | kl  0.02\n",
      "| epoch  16 | 800/1106 batches | lr 20.0000 | ms/batch 94.45 | loss  6.56 | ppl   705.07 | kl  0.00\n",
      "| epoch  16 | 1000/1106 batches | lr 20.0000 | ms/batch 94.10 | loss  6.56 | ppl   704.91 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 110.78s | valid loss  6.61 | valid ppl   744.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 | 200/1106 batches | lr 20.0000 | ms/batch 94.99 | loss  6.57 | ppl   710.98 | kl  0.03\n",
      "| epoch  17 | 400/1106 batches | lr 20.0000 | ms/batch 95.95 | loss  6.52 | ppl   679.81 | kl  0.04\n",
      "| epoch  17 | 600/1106 batches | lr 20.0000 | ms/batch 95.23 | loss  6.52 | ppl   680.30 | kl  0.02\n",
      "| epoch  17 | 800/1106 batches | lr 20.0000 | ms/batch 98.03 | loss  6.55 | ppl   696.11 | kl  0.03\n",
      "| epoch  17 | 1000/1106 batches | lr 20.0000 | ms/batch 93.95 | loss  6.54 | ppl   695.33 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 111.27s | valid loss  6.54 | valid ppl   693.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 | 200/1106 batches | lr 20.0000 | ms/batch 93.90 | loss  6.56 | ppl   707.90 | kl  0.01\n",
      "| epoch  18 | 400/1106 batches | lr 20.0000 | ms/batch 92.85 | loss  6.51 | ppl   674.19 | kl  0.02\n",
      "| epoch  18 | 600/1106 batches | lr 20.0000 | ms/batch 93.44 | loss  6.52 | ppl   679.14 | kl  0.02\n",
      "| epoch  18 | 800/1106 batches | lr 20.0000 | ms/batch 96.97 | loss  6.55 | ppl   700.21 | kl  0.01\n",
      "| epoch  18 | 1000/1106 batches | lr 20.0000 | ms/batch 94.61 | loss  6.54 | ppl   694.87 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 110.48s | valid loss  6.53 | valid ppl   685.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 | 200/1106 batches | lr 20.0000 | ms/batch 95.16 | loss  6.58 | ppl   718.11 | kl  0.02\n",
      "| epoch  19 | 400/1106 batches | lr 20.0000 | ms/batch 90.86 | loss  6.52 | ppl   680.62 | kl  0.03\n",
      "| epoch  19 | 600/1106 batches | lr 20.0000 | ms/batch 94.26 | loss  6.52 | ppl   679.64 | kl  0.01\n",
      "| epoch  19 | 800/1106 batches | lr 20.0000 | ms/batch 94.92 | loss  6.55 | ppl   696.97 | kl  0.01\n",
      "| epoch  19 | 1000/1106 batches | lr 20.0000 | ms/batch 93.32 | loss  6.55 | ppl   700.50 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 110.13s | valid loss  6.55 | valid ppl   698.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 | 200/1106 batches | lr 20.0000 | ms/batch 97.57 | loss  6.57 | ppl   711.67 | kl  0.03\n",
      "| epoch  20 | 400/1106 batches | lr 20.0000 | ms/batch 94.18 | loss  6.54 | ppl   690.39 | kl  0.02\n",
      "| epoch  20 | 600/1106 batches | lr 20.0000 | ms/batch 92.58 | loss  6.53 | ppl   685.67 | kl  0.04\n",
      "| epoch  20 | 800/1106 batches | lr 20.0000 | ms/batch 93.22 | loss  6.56 | ppl   703.77 | kl  0.01\n",
      "| epoch  20 | 1000/1106 batches | lr 20.0000 | ms/batch 95.33 | loss  6.55 | ppl   698.63 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 110.54s | valid loss  6.55 | valid ppl   700.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 | 200/1106 batches | lr 20.0000 | ms/batch 93.28 | loss  6.58 | ppl   723.05 | kl  0.03\n",
      "| epoch  21 | 400/1106 batches | lr 20.0000 | ms/batch 95.50 | loss  6.52 | ppl   679.60 | kl  0.01\n",
      "| epoch  21 | 600/1106 batches | lr 20.0000 | ms/batch 94.88 | loss  6.52 | ppl   679.00 | kl  0.01\n",
      "| epoch  21 | 800/1106 batches | lr 20.0000 | ms/batch 93.77 | loss  6.55 | ppl   699.17 | kl  0.01\n",
      "| epoch  21 | 1000/1106 batches | lr 20.0000 | ms/batch 93.93 | loss  6.54 | ppl   693.64 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 110.22s | valid loss  6.55 | valid ppl   698.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 | 200/1106 batches | lr 20.0000 | ms/batch 96.81 | loss  6.56 | ppl   704.22 | kl  0.03\n",
      "| epoch  22 | 400/1106 batches | lr 20.0000 | ms/batch 92.47 | loss  6.52 | ppl   678.29 | kl  0.05\n",
      "| epoch  22 | 600/1106 batches | lr 20.0000 | ms/batch 94.98 | loss  6.52 | ppl   681.55 | kl  0.01\n",
      "| epoch  22 | 800/1106 batches | lr 20.0000 | ms/batch 94.15 | loss  6.55 | ppl   696.63 | kl  0.00\n",
      "| epoch  22 | 1000/1106 batches | lr 20.0000 | ms/batch 94.65 | loss  6.54 | ppl   693.39 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 110.43s | valid loss  6.56 | valid ppl   707.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 200/1106 batches | lr 20.0000 | ms/batch 93.60 | loss  6.57 | ppl   715.32 | kl  0.01\n",
      "| epoch  23 | 400/1106 batches | lr 20.0000 | ms/batch 94.97 | loss  6.51 | ppl   670.96 | kl  0.02\n",
      "| epoch  23 | 600/1106 batches | lr 20.0000 | ms/batch 94.28 | loss  6.52 | ppl   680.85 | kl  0.01\n",
      "| epoch  23 | 800/1106 batches | lr 20.0000 | ms/batch 93.14 | loss  6.54 | ppl   692.82 | kl  0.00\n",
      "| epoch  23 | 1000/1106 batches | lr 20.0000 | ms/batch 92.80 | loss  6.54 | ppl   694.57 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 110.01s | valid loss  6.53 | valid ppl   688.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 | 200/1106 batches | lr 20.0000 | ms/batch 94.23 | loss  6.57 | ppl   716.21 | kl  0.02\n",
      "| epoch  24 | 400/1106 batches | lr 20.0000 | ms/batch 92.81 | loss  6.51 | ppl   670.78 | kl  0.00\n",
      "| epoch  24 | 600/1106 batches | lr 20.0000 | ms/batch 93.25 | loss  6.52 | ppl   676.31 | kl  0.00\n",
      "| epoch  24 | 800/1106 batches | lr 20.0000 | ms/batch 94.04 | loss  6.54 | ppl   695.28 | kl  0.01\n",
      "| epoch  24 | 1000/1106 batches | lr 20.0000 | ms/batch 93.57 | loss  6.54 | ppl   692.95 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 111.10s | valid loss  6.53 | valid ppl   686.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 | 200/1106 batches | lr 20.0000 | ms/batch 98.65 | loss  6.55 | ppl   698.16 | kl  0.01\n",
      "| epoch  25 | 400/1106 batches | lr 20.0000 | ms/batch 94.56 | loss  6.51 | ppl   671.28 | kl  0.01\n",
      "| epoch  25 | 600/1106 batches | lr 20.0000 | ms/batch 93.65 | loss  6.52 | ppl   677.16 | kl  0.01\n",
      "| epoch  25 | 800/1106 batches | lr 20.0000 | ms/batch 96.73 | loss  6.56 | ppl   704.66 | kl  0.01\n",
      "| epoch  25 | 1000/1106 batches | lr 20.0000 | ms/batch 96.50 | loss  6.56 | ppl   705.32 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 112.14s | valid loss  6.55 | valid ppl   700.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 | 200/1106 batches | lr 20.0000 | ms/batch 91.92 | loss  6.57 | ppl   710.08 | kl  0.01\n",
      "| epoch  26 | 400/1106 batches | lr 20.0000 | ms/batch 95.32 | loss  6.51 | ppl   668.67 | kl  0.03\n",
      "| epoch  26 | 600/1106 batches | lr 20.0000 | ms/batch 97.12 | loss  6.51 | ppl   672.06 | kl  0.00\n",
      "| epoch  26 | 800/1106 batches | lr 20.0000 | ms/batch 95.39 | loss  6.54 | ppl   693.90 | kl  0.02\n",
      "| epoch  26 | 1000/1106 batches | lr 20.0000 | ms/batch 94.40 | loss  6.53 | ppl   687.41 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 110.59s | valid loss  6.55 | valid ppl   696.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 | 200/1106 batches | lr 20.0000 | ms/batch 96.58 | loss  6.55 | ppl   699.50 | kl  0.01\n",
      "| epoch  27 | 400/1106 batches | lr 20.0000 | ms/batch 95.44 | loss  6.51 | ppl   672.73 | kl  0.03\n",
      "| epoch  27 | 600/1106 batches | lr 20.0000 | ms/batch 97.53 | loss  6.52 | ppl   678.57 | kl  0.00\n",
      "| epoch  27 | 800/1106 batches | lr 20.0000 | ms/batch 95.44 | loss  6.55 | ppl   700.06 | kl  0.01\n",
      "| epoch  27 | 1000/1106 batches | lr 20.0000 | ms/batch 95.35 | loss  6.53 | ppl   687.89 | kl  0.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 110.83s | valid loss  6.55 | valid ppl   701.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 | 200/1106 batches | lr 20.0000 | ms/batch 96.86 | loss  6.56 | ppl   705.44 | kl  0.02\n",
      "| epoch  28 | 400/1106 batches | lr 20.0000 | ms/batch 93.22 | loss  6.54 | ppl   689.26 | kl  0.00\n",
      "| epoch  28 | 600/1106 batches | lr 20.0000 | ms/batch 93.64 | loss  6.51 | ppl   669.76 | kl  0.03\n",
      "| epoch  28 | 800/1106 batches | lr 20.0000 | ms/batch 93.98 | loss  6.53 | ppl   688.09 | kl  0.01\n",
      "| epoch  28 | 1000/1106 batches | lr 20.0000 | ms/batch 93.53 | loss  6.54 | ppl   691.60 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 110.82s | valid loss  6.56 | valid ppl   706.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 | 200/1106 batches | lr 20.0000 | ms/batch 95.16 | loss  6.55 | ppl   700.87 | kl  0.03\n",
      "| epoch  29 | 400/1106 batches | lr 20.0000 | ms/batch 92.81 | loss  6.52 | ppl   681.41 | kl  0.01\n",
      "| epoch  29 | 600/1106 batches | lr 20.0000 | ms/batch 95.24 | loss  6.52 | ppl   676.59 | kl  0.03\n",
      "| epoch  29 | 800/1106 batches | lr 20.0000 | ms/batch 94.62 | loss  6.54 | ppl   691.65 | kl  0.01\n",
      "| epoch  29 | 1000/1106 batches | lr 20.0000 | ms/batch 93.50 | loss  6.53 | ppl   686.56 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 109.94s | valid loss  6.56 | valid ppl   706.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 | 200/1106 batches | lr 20.0000 | ms/batch 98.14 | loss  6.55 | ppl   701.18 | kl  0.01\n",
      "| epoch  30 | 400/1106 batches | lr 20.0000 | ms/batch 96.41 | loss  6.52 | ppl   681.40 | kl  0.01\n",
      "| epoch  30 | 600/1106 batches | lr 20.0000 | ms/batch 92.92 | loss  6.51 | ppl   672.88 | kl  0.00\n",
      "| epoch  30 | 800/1106 batches | lr 20.0000 | ms/batch 95.06 | loss  6.54 | ppl   690.61 | kl  0.03\n",
      "| epoch  30 | 1000/1106 batches | lr 20.0000 | ms/batch 92.77 | loss  6.54 | ppl   688.98 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 110.25s | valid loss  6.58 | valid ppl   719.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 | 200/1106 batches | lr 20.0000 | ms/batch 93.57 | loss  6.55 | ppl   702.33 | kl  0.02\n",
      "| epoch  31 | 400/1106 batches | lr 20.0000 | ms/batch 96.81 | loss  6.51 | ppl   672.67 | kl  0.00\n",
      "| epoch  31 | 600/1106 batches | lr 20.0000 | ms/batch 97.41 | loss  6.50 | ppl   667.78 | kl  0.02\n",
      "| epoch  31 | 800/1106 batches | lr 20.0000 | ms/batch 91.39 | loss  6.54 | ppl   689.42 | kl  0.01\n",
      "| epoch  31 | 1000/1106 batches | lr 20.0000 | ms/batch 93.07 | loss  6.54 | ppl   694.79 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 110.56s | valid loss  6.71 | valid ppl   818.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 | 200/1106 batches | lr 20.0000 | ms/batch 95.10 | loss  6.56 | ppl   708.02 | kl  0.01\n",
      "| epoch  32 | 400/1106 batches | lr 20.0000 | ms/batch 94.27 | loss  6.51 | ppl   670.68 | kl  0.01\n",
      "| epoch  32 | 600/1106 batches | lr 20.0000 | ms/batch 94.70 | loss  6.51 | ppl   671.37 | kl  0.03\n",
      "| epoch  32 | 800/1106 batches | lr 20.0000 | ms/batch 93.53 | loss  6.53 | ppl   687.12 | kl  0.02\n",
      "| epoch  32 | 1000/1106 batches | lr 20.0000 | ms/batch 94.48 | loss  6.53 | ppl   688.51 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 110.39s | valid loss  6.57 | valid ppl   712.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 | 200/1106 batches | lr 20.0000 | ms/batch 95.55 | loss  6.55 | ppl   699.90 | kl  0.02\n",
      "| epoch  33 | 400/1106 batches | lr 20.0000 | ms/batch 93.35 | loss  6.51 | ppl   673.29 | kl  0.00\n",
      "| epoch  33 | 600/1106 batches | lr 20.0000 | ms/batch 95.53 | loss  6.52 | ppl   677.33 | kl  0.00\n",
      "| epoch  33 | 800/1106 batches | lr 20.0000 | ms/batch 93.20 | loss  6.55 | ppl   702.10 | kl  0.00\n",
      "| epoch  33 | 1000/1106 batches | lr 20.0000 | ms/batch 95.44 | loss  6.54 | ppl   689.13 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 110.40s | valid loss  6.54 | valid ppl   691.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 | 200/1106 batches | lr 20.0000 | ms/batch 95.02 | loss  6.55 | ppl   698.56 | kl  0.02\n",
      "| epoch  34 | 400/1106 batches | lr 20.0000 | ms/batch 93.76 | loss  6.50 | ppl   668.21 | kl  0.00\n",
      "| epoch  34 | 600/1106 batches | lr 20.0000 | ms/batch 93.43 | loss  6.52 | ppl   675.86 | kl  0.01\n",
      "| epoch  34 | 800/1106 batches | lr 20.0000 | ms/batch 92.03 | loss  6.54 | ppl   690.38 | kl  0.00\n",
      "| epoch  34 | 1000/1106 batches | lr 20.0000 | ms/batch 95.31 | loss  6.55 | ppl   700.23 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 110.73s | valid loss  7.15 | valid ppl  1271.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 | 200/1106 batches | lr 20.0000 | ms/batch 95.03 | loss  6.58 | ppl   719.71 | kl  0.02\n",
      "| epoch  35 | 400/1106 batches | lr 20.0000 | ms/batch 94.05 | loss  6.50 | ppl   666.77 | kl  0.04\n",
      "| epoch  35 | 600/1106 batches | lr 20.0000 | ms/batch 94.16 | loss  6.51 | ppl   670.16 | kl  0.01\n",
      "| epoch  35 | 800/1106 batches | lr 20.0000 | ms/batch 93.47 | loss  6.53 | ppl   687.26 | kl  0.01\n",
      "| epoch  35 | 1000/1106 batches | lr 20.0000 | ms/batch 94.62 | loss  6.54 | ppl   693.66 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 110.13s | valid loss  6.57 | valid ppl   713.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 | 200/1106 batches | lr 20.0000 | ms/batch 95.93 | loss  6.56 | ppl   709.68 | kl  0.01\n",
      "| epoch  36 | 400/1106 batches | lr 20.0000 | ms/batch 94.24 | loss  6.52 | ppl   680.80 | kl  0.03\n",
      "| epoch  36 | 600/1106 batches | lr 20.0000 | ms/batch 93.88 | loss  6.52 | ppl   676.15 | kl  0.00\n",
      "| epoch  36 | 800/1106 batches | lr 20.0000 | ms/batch 93.71 | loss  6.54 | ppl   692.87 | kl  0.00\n",
      "| epoch  36 | 1000/1106 batches | lr 20.0000 | ms/batch 95.76 | loss  6.53 | ppl   683.58 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 110.69s | valid loss  6.79 | valid ppl   889.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 | 200/1106 batches | lr 20.0000 | ms/batch 95.11 | loss  6.55 | ppl   702.09 | kl  0.00\n",
      "| epoch  37 | 400/1106 batches | lr 20.0000 | ms/batch 92.04 | loss  6.53 | ppl   684.02 | kl  0.03\n",
      "| epoch  37 | 600/1106 batches | lr 20.0000 | ms/batch 95.07 | loss  6.51 | ppl   672.48 | kl  0.03\n",
      "| epoch  37 | 800/1106 batches | lr 20.0000 | ms/batch 93.06 | loss  6.53 | ppl   686.69 | kl  0.03\n",
      "| epoch  37 | 1000/1106 batches | lr 20.0000 | ms/batch 92.07 | loss  6.53 | ppl   684.13 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 110.24s | valid loss  6.55 | valid ppl   700.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 | 200/1106 batches | lr 20.0000 | ms/batch 94.46 | loss  6.58 | ppl   717.15 | kl  0.02\n",
      "| epoch  38 | 400/1106 batches | lr 20.0000 | ms/batch 90.58 | loss  6.51 | ppl   671.23 | kl  0.01\n",
      "| epoch  38 | 600/1106 batches | lr 20.0000 | ms/batch 92.08 | loss  6.50 | ppl   666.35 | kl  0.04\n",
      "| epoch  38 | 800/1106 batches | lr 20.0000 | ms/batch 92.60 | loss  6.53 | ppl   682.38 | kl  0.02\n",
      "| epoch  38 | 1000/1106 batches | lr 20.0000 | ms/batch 93.53 | loss  6.53 | ppl   686.36 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 109.21s | valid loss  6.54 | valid ppl   694.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 | 200/1106 batches | lr 20.0000 | ms/batch 95.57 | loss  6.55 | ppl   698.92 | kl  0.03\n",
      "| epoch  39 | 400/1106 batches | lr 20.0000 | ms/batch 94.29 | loss  6.52 | ppl   677.47 | kl  0.05\n",
      "| epoch  39 | 600/1106 batches | lr 20.0000 | ms/batch 92.43 | loss  6.51 | ppl   673.34 | kl  0.01\n",
      "| epoch  39 | 800/1106 batches | lr 20.0000 | ms/batch 95.64 | loss  6.55 | ppl   697.34 | kl  0.04\n",
      "| epoch  39 | 1000/1106 batches | lr 20.0000 | ms/batch 92.85 | loss  6.55 | ppl   698.59 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 110.49s | valid loss  6.63 | valid ppl   754.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 | 200/1106 batches | lr 20.0000 | ms/batch 94.61 | loss  6.55 | ppl   702.16 | kl  0.04\n",
      "| epoch  40 | 400/1106 batches | lr 20.0000 | ms/batch 92.40 | loss  6.52 | ppl   676.05 | kl  0.02\n",
      "| epoch  40 | 600/1106 batches | lr 20.0000 | ms/batch 90.32 | loss  6.50 | ppl   666.05 | kl  0.00\n",
      "| epoch  40 | 800/1106 batches | lr 20.0000 | ms/batch 95.34 | loss  6.53 | ppl   687.51 | kl  0.02\n",
      "| epoch  40 | 1000/1106 batches | lr 20.0000 | ms/batch 98.46 | loss  6.54 | ppl   692.73 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 111.41s | valid loss  6.54 | valid ppl   694.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 | 200/1106 batches | lr 20.0000 | ms/batch 96.14 | loss  6.55 | ppl   701.08 | kl  0.02\n",
      "| epoch  41 | 400/1106 batches | lr 20.0000 | ms/batch 98.03 | loss  6.51 | ppl   668.95 | kl  0.00\n",
      "| epoch  41 | 600/1106 batches | lr 20.0000 | ms/batch 96.24 | loss  6.54 | ppl   688.89 | kl  0.01\n",
      "| epoch  41 | 800/1106 batches | lr 20.0000 | ms/batch 94.95 | loss  6.53 | ppl   686.38 | kl  0.00\n",
      "| epoch  41 | 1000/1106 batches | lr 20.0000 | ms/batch 95.03 | loss  6.53 | ppl   686.22 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 111.83s | valid loss  6.67 | valid ppl   791.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 | 200/1106 batches | lr 20.0000 | ms/batch 96.98 | loss  6.55 | ppl   696.46 | kl  0.01\n",
      "| epoch  42 | 400/1106 batches | lr 20.0000 | ms/batch 96.23 | loss  6.53 | ppl   682.38 | kl  0.04\n",
      "| epoch  42 | 600/1106 batches | lr 20.0000 | ms/batch 94.71 | loss  6.51 | ppl   673.27 | kl  0.01\n",
      "| epoch  42 | 800/1106 batches | lr 20.0000 | ms/batch 93.87 | loss  6.54 | ppl   688.85 | kl  0.01\n",
      "| epoch  42 | 1000/1106 batches | lr 20.0000 | ms/batch 94.54 | loss  6.53 | ppl   687.15 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 111.57s | valid loss  6.53 | valid ppl   688.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 | 200/1106 batches | lr 20.0000 | ms/batch 93.66 | loss  6.56 | ppl   703.18 | kl  0.01\n",
      "| epoch  43 | 400/1106 batches | lr 20.0000 | ms/batch 98.61 | loss  6.53 | ppl   688.34 | kl  0.01\n",
      "| epoch  43 | 600/1106 batches | lr 20.0000 | ms/batch 95.64 | loss  6.51 | ppl   673.81 | kl  0.04\n",
      "| epoch  43 | 800/1106 batches | lr 20.0000 | ms/batch 94.36 | loss  6.54 | ppl   694.44 | kl  0.01\n",
      "| epoch  43 | 1000/1106 batches | lr 20.0000 | ms/batch 97.29 | loss  6.53 | ppl   686.61 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 111.49s | valid loss  6.94 | valid ppl  1033.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 | 200/1106 batches | lr 20.0000 | ms/batch 95.05 | loss  6.56 | ppl   706.63 | kl  0.00\n",
      "| epoch  44 | 400/1106 batches | lr 20.0000 | ms/batch 94.55 | loss  6.50 | ppl   666.44 | kl  0.01\n",
      "| epoch  44 | 600/1106 batches | lr 20.0000 | ms/batch 93.12 | loss  6.51 | ppl   670.61 | kl  0.00\n",
      "| epoch  44 | 800/1106 batches | lr 20.0000 | ms/batch 94.30 | loss  6.53 | ppl   684.94 | kl  0.02\n",
      "| epoch  44 | 1000/1106 batches | lr 20.0000 | ms/batch 96.00 | loss  6.54 | ppl   690.20 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 110.93s | valid loss  6.64 | valid ppl   765.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 | 200/1106 batches | lr 20.0000 | ms/batch 94.57 | loss  6.57 | ppl   715.84 | kl  0.00\n",
      "| epoch  45 | 400/1106 batches | lr 20.0000 | ms/batch 93.50 | loss  6.52 | ppl   679.79 | kl  0.01\n",
      "| epoch  45 | 600/1106 batches | lr 20.0000 | ms/batch 93.43 | loss  6.53 | ppl   683.26 | kl  0.01\n",
      "| epoch  45 | 800/1106 batches | lr 20.0000 | ms/batch 92.69 | loss  6.55 | ppl   700.35 | kl  0.01\n",
      "| epoch  45 | 1000/1106 batches | lr 20.0000 | ms/batch 95.85 | loss  6.52 | ppl   680.69 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 110.68s | valid loss  6.57 | valid ppl   711.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 | 200/1106 batches | lr 20.0000 | ms/batch 96.48 | loss  6.56 | ppl   703.96 | kl  0.00\n",
      "| epoch  46 | 400/1106 batches | lr 20.0000 | ms/batch 95.27 | loss  6.50 | ppl   665.92 | kl  0.02\n",
      "| epoch  46 | 600/1106 batches | lr 20.0000 | ms/batch 96.60 | loss  6.51 | ppl   669.46 | kl  0.01\n",
      "| epoch  46 | 800/1106 batches | lr 20.0000 | ms/batch 94.66 | loss  6.54 | ppl   692.03 | kl  0.00\n",
      "| epoch  46 | 1000/1106 batches | lr 20.0000 | ms/batch 93.67 | loss  6.55 | ppl   696.17 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 111.12s | valid loss  6.57 | valid ppl   710.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 | 200/1106 batches | lr 20.0000 | ms/batch 95.62 | loss  6.55 | ppl   697.96 | kl  0.00\n",
      "| epoch  47 | 400/1106 batches | lr 20.0000 | ms/batch 93.85 | loss  6.50 | ppl   666.76 | kl  0.01\n",
      "| epoch  47 | 600/1106 batches | lr 20.0000 | ms/batch 95.76 | loss  6.51 | ppl   673.88 | kl  0.01\n",
      "| epoch  47 | 800/1106 batches | lr 20.0000 | ms/batch 94.91 | loss  6.55 | ppl   702.11 | kl  0.00\n",
      "| epoch  47 | 1000/1106 batches | lr 20.0000 | ms/batch 93.42 | loss  6.53 | ppl   683.91 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 110.47s | valid loss  6.53 | valid ppl   687.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 | 200/1106 batches | lr 20.0000 | ms/batch 94.21 | loss  6.55 | ppl   699.81 | kl  0.02\n",
      "| epoch  48 | 400/1106 batches | lr 20.0000 | ms/batch 93.75 | loss  6.51 | ppl   671.07 | kl  0.02\n",
      "| epoch  48 | 600/1106 batches | lr 20.0000 | ms/batch 92.63 | loss  6.51 | ppl   673.29 | kl  0.01\n",
      "| epoch  48 | 800/1106 batches | lr 20.0000 | ms/batch 92.71 | loss  6.53 | ppl   688.44 | kl  0.01\n",
      "| epoch  48 | 1000/1106 batches | lr 20.0000 | ms/batch 93.72 | loss  6.53 | ppl   688.58 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 109.51s | valid loss  6.56 | valid ppl   705.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 | 200/1106 batches | lr 20.0000 | ms/batch 92.83 | loss  6.55 | ppl   696.55 | kl  0.00\n",
      "| epoch  49 | 400/1106 batches | lr 20.0000 | ms/batch 94.96 | loss  6.50 | ppl   667.77 | kl  0.01\n",
      "| epoch  49 | 600/1106 batches | lr 20.0000 | ms/batch 92.14 | loss  6.52 | ppl   680.70 | kl  0.04\n",
      "| epoch  49 | 800/1106 batches | lr 20.0000 | ms/batch 94.40 | loss  6.54 | ppl   692.46 | kl  0.00\n",
      "| epoch  49 | 1000/1106 batches | lr 20.0000 | ms/batch 95.50 | loss  6.53 | ppl   686.77 | kl  0.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 110.78s | valid loss  6.58 | valid ppl   723.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 | 200/1106 batches | lr 20.0000 | ms/batch 96.10 | loss  6.54 | ppl   691.98 | kl  0.01\n",
      "| epoch  50 | 400/1106 batches | lr 20.0000 | ms/batch 94.58 | loss  6.50 | ppl   665.02 | kl  0.03\n",
      "| epoch  50 | 600/1106 batches | lr 20.0000 | ms/batch 92.62 | loss  6.50 | ppl   664.99 | kl  0.00\n",
      "| epoch  50 | 800/1106 batches | lr 20.0000 | ms/batch 93.71 | loss  6.53 | ppl   687.09 | kl  0.02\n",
      "| epoch  50 | 1000/1106 batches | lr 20.0000 | ms/batch 92.32 | loss  6.53 | ppl   688.58 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 110.72s | valid loss  6.55 | valid ppl   701.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 | 200/1106 batches | lr 20.0000 | ms/batch 95.25 | loss  6.56 | ppl   704.29 | kl  0.00\n",
      "| epoch  51 | 400/1106 batches | lr 20.0000 | ms/batch 94.62 | loss  6.52 | ppl   681.04 | kl  0.00\n",
      "| epoch  51 | 600/1106 batches | lr 20.0000 | ms/batch 93.62 | loss  6.50 | ppl   667.57 | kl  0.03\n",
      "| epoch  51 | 800/1106 batches | lr 20.0000 | ms/batch 93.26 | loss  6.53 | ppl   688.27 | kl  0.01\n",
      "| epoch  51 | 1000/1106 batches | lr 20.0000 | ms/batch 94.11 | loss  6.53 | ppl   686.12 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 110.78s | valid loss  6.74 | valid ppl   844.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 | 200/1106 batches | lr 20.0000 | ms/batch 96.28 | loss  6.55 | ppl   699.84 | kl  0.03\n",
      "| epoch  52 | 400/1106 batches | lr 20.0000 | ms/batch 95.77 | loss  6.50 | ppl   665.97 | kl  0.02\n",
      "| epoch  52 | 600/1106 batches | lr 20.0000 | ms/batch 94.08 | loss  6.49 | ppl   659.97 | kl  0.01\n",
      "| epoch  52 | 800/1106 batches | lr 20.0000 | ms/batch 94.42 | loss  6.54 | ppl   695.35 | kl  0.02\n",
      "| epoch  52 | 1000/1106 batches | lr 20.0000 | ms/batch 94.99 | loss  6.53 | ppl   682.58 | kl  0.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 110.91s | valid loss  6.59 | valid ppl   730.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 | 200/1106 batches | lr 20.0000 | ms/batch 92.26 | loss  6.54 | ppl   691.65 | kl  0.01\n",
      "| epoch  53 | 400/1106 batches | lr 20.0000 | ms/batch 93.75 | loss  6.51 | ppl   673.23 | kl  0.01\n",
      "| epoch  53 | 600/1106 batches | lr 20.0000 | ms/batch 95.82 | loss  6.51 | ppl   671.82 | kl  0.00\n",
      "| epoch  53 | 800/1106 batches | lr 20.0000 | ms/batch 93.05 | loss  6.53 | ppl   683.68 | kl  0.03\n",
      "| epoch  53 | 1000/1106 batches | lr 20.0000 | ms/batch 95.55 | loss  6.53 | ppl   682.51 | kl  0.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 110.21s | valid loss  6.56 | valid ppl   704.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 | 200/1106 batches | lr 20.0000 | ms/batch 94.01 | loss  6.55 | ppl   700.47 | kl  0.04\n",
      "| epoch  54 | 400/1106 batches | lr 20.0000 | ms/batch 94.99 | loss  6.51 | ppl   670.92 | kl  0.03\n",
      "| epoch  54 | 600/1106 batches | lr 20.0000 | ms/batch 93.47 | loss  6.52 | ppl   678.10 | kl  0.02\n",
      "| epoch  54 | 800/1106 batches | lr 20.0000 | ms/batch 94.13 | loss  6.54 | ppl   692.97 | kl  0.00\n",
      "| epoch  54 | 1000/1106 batches | lr 20.0000 | ms/batch 95.21 | loss  6.53 | ppl   682.80 | kl  0.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 111.51s | valid loss  6.59 | valid ppl   728.19\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt)\n",
    "        val_loss = evaluate(val_data, model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            tp.send_text('MOSsb\\nEpoch %d | loss: %.2f | ppl: %.2f' % (epoch, val_loss, math.exp(val_loss)))\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.82605531053599"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.4753582141404555"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_words = 150\n",
    "generated = []\n",
    "temperature = 1.0 # highest temperature increase diversity\n",
    "log_interval = 50 # \n",
    "model.eval()\n",
    "hidden = model.init_hidden(1)\n",
    "input = Variable(torch.rand(1, 1).mul(ntokens).long().cuda(), volatile=True)\n",
    "\n",
    "sent = []\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden, return_prob=True)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    sent.append(word) \n",
    "    if i % 20 == 19:\n",
    "        generated.append(sent)\n",
    "        sent = []\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print('| Generated {}/{} words'.format(i, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
