{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d tensor([[ 39.7290,  30.6525,  19.0764,   8.7156,   0.0000],\n",
      "        [ 38.1882,  28.7598,  19.5834,   8.1750,   0.0000],\n",
      "        [ 39.9409,  31.0106,  21.4423,  10.0411,   0.0000]])\n",
      "remprod tensor([[ 1.0000,  0.8504,  0.6677,  0.4157,  0.1961],\n",
      "        [ 1.0000,  0.8094,  0.5888,  0.4296,  0.1628],\n",
      "        [ 1.0000,  0.7424,  0.5894,  0.3941,  0.2431]])\n",
      "sample tensor([[ 0.1496,  0.2148,  0.3774,  0.5283,  1.0000],\n",
      "        [ 0.1906,  0.2726,  0.2704,  0.6211,  1.0000],\n",
      "        [ 0.2576,  0.2060,  0.3314,  0.3832,  1.0000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1496,  0.1827,  0.2520,  0.2196,  0.1961],\n",
       "        [ 0.1906,  0.2206,  0.1592,  0.2668,  0.1628],\n",
       "        [ 0.2576,  0.1529,  0.1953,  0.1510,  0.2431]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(15, device=device, requires_grad=True).view(3,5) + 10\n",
    "b = torch.sum(a, 1)\n",
    "c = torch.cumsum(a, 1)\n",
    "d = torch.abs(b[:, None] - c)\n",
    "print('d', d)\n",
    "beta = torch.distributions.Beta(a, d)\n",
    "sample = beta.sample()\n",
    "rem = 1 - sample\n",
    "D = torch.diag(torch.ones(4), 1)\n",
    "rem = rem @ D\n",
    "rem[:, 0] = 1\n",
    "remprod = torch.cumprod(rem, 1)\n",
    "print('remprod', remprod)\n",
    "print('sample', sample)\n",
    "final = remprod * sample\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.0000,  1.0000])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = torch.rand(15, requires_grad=True)#.view(3,5)\n",
    "o = torch.rand_like(final)\n",
    "loss = (final * o).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8055078983306885"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6724,  0.1618,  0.1117,  0.4801,  0.2422,  0.1538,  0.2405,\n",
      "         0.2617,  0.9309,  0.6892,  0.4949,  0.6704,  0.5578,  0.6466,\n",
      "         0.4510])\n"
     ]
    }
   ],
   "source": [
    "print(final.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad() missing 2 required positional arguments: 'outputs' and 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9a732ae48647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: grad() missing 2 required positional arguments: 'outputs' and 'inputs'"
     ]
    }
   ],
   "source": [
    "grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad(loss, a) #, torch.ones_like(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad = grad(loss, a)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out Gauss-Logit parametrization from here https://arxiv.org/pdf/1605.06197.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Use token from .telepythrc.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import modelsbrsample as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\n",
    "from telepyth import TelepythClient\n",
    "tp = TelepythClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "#is_cuda = False\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24300635\n",
      "Model total parameters: 24300635\n"
     ]
    }
   ],
   "source": [
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = model(cur_data, hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "            loss = raw_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180528-220743\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py:491: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 113.08 | loss  6.97 | ppl  1062.55\n",
      "| epoch   1 | 400/1106 batches | lr 20.0000 | ms/batch 106.47 | loss  6.58 | ppl   720.55\n",
      "| epoch   1 | 600/1106 batches | lr 20.0000 | ms/batch 103.84 | loss  6.48 | ppl   650.60\n",
      "| epoch   1 | 800/1106 batches | lr 20.0000 | ms/batch 108.46 | loss  6.33 | ppl   561.80\n",
      "| epoch   1 | 1000/1106 batches | lr 20.0000 | ms/batch 108.39 | loss  6.22 | ppl   500.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 127.36s | valid loss  6.02 | valid ppl   409.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   2 | 200/1106 batches | lr 20.0000 | ms/batch 103.98 | loss  6.02 | ppl   411.51\n",
      "| epoch   2 | 400/1106 batches | lr 20.0000 | ms/batch 112.89 | loss  5.85 | ppl   347.03\n",
      "| epoch   2 | 600/1106 batches | lr 20.0000 | ms/batch 106.04 | loss  5.75 | ppl   312.71\n",
      "| epoch   2 | 800/1106 batches | lr 20.0000 | ms/batch 107.69 | loss  5.70 | ppl   299.61\n",
      "| epoch   2 | 1000/1106 batches | lr 20.0000 | ms/batch 105.28 | loss  5.64 | ppl   281.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 124.58s | valid loss  5.47 | valid ppl   236.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 | 200/1106 batches | lr 20.0000 | ms/batch 106.73 | loss  5.55 | ppl   256.52\n",
      "| epoch   3 | 400/1106 batches | lr 20.0000 | ms/batch 105.90 | loss  5.42 | ppl   225.94\n",
      "| epoch   3 | 600/1106 batches | lr 20.0000 | ms/batch 106.67 | loss  5.37 | ppl   214.53\n",
      "| epoch   3 | 800/1106 batches | lr 20.0000 | ms/batch 110.01 | loss  5.36 | ppl   213.23\n",
      "| epoch   3 | 1000/1106 batches | lr 20.0000 | ms/batch 108.00 | loss  5.34 | ppl   208.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 126.38s | valid loss  5.24 | valid ppl   188.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 | 200/1106 batches | lr 20.0000 | ms/batch 109.01 | loss  5.28 | ppl   196.84\n",
      "| epoch   4 | 400/1106 batches | lr 20.0000 | ms/batch 107.31 | loss  5.17 | ppl   175.63\n",
      "| epoch   4 | 600/1106 batches | lr 20.0000 | ms/batch 108.94 | loss  5.13 | ppl   168.64\n",
      "| epoch   4 | 800/1106 batches | lr 20.0000 | ms/batch 108.85 | loss  5.15 | ppl   171.61\n",
      "| epoch   4 | 1000/1106 batches | lr 20.0000 | ms/batch 105.08 | loss  5.13 | ppl   169.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 124.94s | valid loss  5.04 | valid ppl   155.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 | 200/1106 batches | lr 20.0000 | ms/batch 108.58 | loss  5.12 | ppl   167.75\n",
      "| epoch   5 | 400/1106 batches | lr 20.0000 | ms/batch 108.93 | loss  5.01 | ppl   150.14\n",
      "| epoch   5 | 600/1106 batches | lr 20.0000 | ms/batch 109.85 | loss  4.97 | ppl   144.04\n",
      "| epoch   5 | 800/1106 batches | lr 20.0000 | ms/batch 103.47 | loss  4.99 | ppl   146.83\n",
      "| epoch   5 | 1000/1106 batches | lr 20.0000 | ms/batch 109.79 | loss  4.99 | ppl   147.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 126.39s | valid loss  4.93 | valid ppl   138.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 | 200/1106 batches | lr 20.0000 | ms/batch 107.58 | loss  4.97 | ppl   144.52\n",
      "| epoch   6 | 400/1106 batches | lr 20.0000 | ms/batch 110.97 | loss  4.87 | ppl   129.94\n",
      "| epoch   6 | 600/1106 batches | lr 20.0000 | ms/batch 111.08 | loss  4.84 | ppl   126.52\n",
      "| epoch   6 | 800/1106 batches | lr 20.0000 | ms/batch 110.01 | loss  4.86 | ppl   129.15\n",
      "| epoch   6 | 1000/1106 batches | lr 20.0000 | ms/batch 107.56 | loss  4.87 | ppl   130.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 127.33s | valid loss  4.84 | valid ppl   125.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 | 200/1106 batches | lr 20.0000 | ms/batch 109.68 | loss  4.87 | ppl   130.10\n",
      "| epoch   7 | 400/1106 batches | lr 20.0000 | ms/batch 107.68 | loss  4.75 | ppl   115.62\n",
      "| epoch   7 | 600/1106 batches | lr 20.0000 | ms/batch 116.99 | loss  4.74 | ppl   114.46\n",
      "| epoch   7 | 800/1106 batches | lr 20.0000 | ms/batch 118.84 | loss  4.76 | ppl   116.39\n",
      "| epoch   7 | 1000/1106 batches | lr 20.0000 | ms/batch 107.64 | loss  4.77 | ppl   117.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 129.89s | valid loss  4.78 | valid ppl   118.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 | 200/1106 batches | lr 20.0000 | ms/batch 106.26 | loss  4.78 | ppl   118.60\n",
      "| epoch   8 | 400/1106 batches | lr 20.0000 | ms/batch 105.08 | loss  4.65 | ppl   104.31\n",
      "| epoch   8 | 600/1106 batches | lr 20.0000 | ms/batch 108.18 | loss  4.65 | ppl   104.75\n",
      "| epoch   8 | 800/1106 batches | lr 20.0000 | ms/batch 108.80 | loss  4.66 | ppl   106.02\n",
      "| epoch   8 | 1000/1106 batches | lr 20.0000 | ms/batch 108.03 | loss  4.70 | ppl   109.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 126.22s | valid loss  4.71 | valid ppl   110.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 | 200/1106 batches | lr 20.0000 | ms/batch 110.09 | loss  4.70 | ppl   110.28\n",
      "| epoch   9 | 400/1106 batches | lr 20.0000 | ms/batch 109.19 | loss  4.59 | ppl    98.70\n",
      "| epoch   9 | 600/1106 batches | lr 20.0000 | ms/batch 111.35 | loss  4.57 | ppl    97.01\n",
      "| epoch   9 | 800/1106 batches | lr 20.0000 | ms/batch 106.65 | loss  4.61 | ppl   100.04\n",
      "| epoch   9 | 1000/1106 batches | lr 20.0000 | ms/batch 111.45 | loss  4.62 | ppl   101.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 127.41s | valid loss  4.69 | valid ppl   109.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 | 200/1106 batches | lr 20.0000 | ms/batch 105.45 | loss  4.64 | ppl   103.34\n",
      "| epoch  10 | 400/1106 batches | lr 20.0000 | ms/batch 106.02 | loss  4.51 | ppl    90.86\n",
      "| epoch  10 | 600/1106 batches | lr 20.0000 | ms/batch 106.77 | loss  4.52 | ppl    91.70\n",
      "| epoch  10 | 800/1106 batches | lr 20.0000 | ms/batch 103.57 | loss  4.54 | ppl    93.29\n",
      "| epoch  10 | 1000/1106 batches | lr 20.0000 | ms/batch 113.70 | loss  4.56 | ppl    95.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 125.84s | valid loss  4.65 | valid ppl   104.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 | 200/1106 batches | lr 20.0000 | ms/batch 111.19 | loss  4.57 | ppl    96.74\n",
      "| epoch  11 | 400/1106 batches | lr 20.0000 | ms/batch 113.38 | loss  4.45 | ppl    85.82\n",
      "| epoch  11 | 600/1106 batches | lr 20.0000 | ms/batch 111.41 | loss  4.45 | ppl    86.02\n",
      "| epoch  11 | 800/1106 batches | lr 20.0000 | ms/batch 108.94 | loss  4.48 | ppl    88.12\n",
      "| epoch  11 | 1000/1106 batches | lr 20.0000 | ms/batch 108.91 | loss  4.51 | ppl    90.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 128.52s | valid loss  4.61 | valid ppl   100.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 | 200/1106 batches | lr 20.0000 | ms/batch 110.55 | loss  4.53 | ppl    92.82\n",
      "| epoch  12 | 400/1106 batches | lr 20.0000 | ms/batch 107.94 | loss  4.40 | ppl    81.53\n",
      "| epoch  12 | 600/1106 batches | lr 20.0000 | ms/batch 106.79 | loss  4.40 | ppl    81.15\n",
      "| epoch  12 | 800/1106 batches | lr 20.0000 | ms/batch 106.37 | loss  4.44 | ppl    84.64\n",
      "| epoch  12 | 1000/1106 batches | lr 20.0000 | ms/batch 106.15 | loss  4.46 | ppl    86.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 125.34s | valid loss  4.59 | valid ppl    98.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 | 200/1106 batches | lr 20.0000 | ms/batch 106.33 | loss  4.48 | ppl    88.62\n",
      "| epoch  13 | 400/1106 batches | lr 20.0000 | ms/batch 105.92 | loss  4.37 | ppl    78.74\n",
      "| epoch  13 | 600/1106 batches | lr 20.0000 | ms/batch 108.20 | loss  4.36 | ppl    78.34\n",
      "| epoch  13 | 800/1106 batches | lr 20.0000 | ms/batch 107.53 | loss  4.39 | ppl    80.34\n",
      "| epoch  13 | 1000/1106 batches | lr 20.0000 | ms/batch 107.57 | loss  4.41 | ppl    82.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 125.47s | valid loss  4.57 | valid ppl    96.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 | 200/1106 batches | lr 20.0000 | ms/batch 112.40 | loss  4.43 | ppl    84.00\n",
      "| epoch  14 | 400/1106 batches | lr 20.0000 | ms/batch 110.05 | loss  4.31 | ppl    74.45\n",
      "| epoch  14 | 600/1106 batches | lr 20.0000 | ms/batch 108.15 | loss  4.31 | ppl    74.78\n",
      "| epoch  14 | 800/1106 batches | lr 20.0000 | ms/batch 107.12 | loss  4.35 | ppl    77.34\n",
      "| epoch  14 | 1000/1106 batches | lr 20.0000 | ms/batch 111.52 | loss  4.38 | ppl    79.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 126.65s | valid loss  4.56 | valid ppl    95.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 | 200/1106 batches | lr 20.0000 | ms/batch 108.11 | loss  4.40 | ppl    81.47\n",
      "| epoch  15 | 400/1106 batches | lr 20.0000 | ms/batch 109.75 | loss  4.28 | ppl    72.08\n",
      "| epoch  15 | 600/1106 batches | lr 20.0000 | ms/batch 108.33 | loss  4.28 | ppl    72.50\n",
      "| epoch  15 | 800/1106 batches | lr 20.0000 | ms/batch 107.52 | loss  4.30 | ppl    73.88\n",
      "| epoch  15 | 1000/1106 batches | lr 20.0000 | ms/batch 107.34 | loss  4.34 | ppl    76.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 126.52s | valid loss  4.53 | valid ppl    92.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 | 200/1106 batches | lr 20.0000 | ms/batch 109.27 | loss  4.37 | ppl    79.29\n",
      "| epoch  16 | 400/1106 batches | lr 20.0000 | ms/batch 105.55 | loss  4.24 | ppl    69.28\n",
      "| epoch  16 | 600/1106 batches | lr 20.0000 | ms/batch 106.02 | loss  4.25 | ppl    70.19\n",
      "| epoch  16 | 800/1106 batches | lr 20.0000 | ms/batch 105.81 | loss  4.27 | ppl    71.18\n",
      "| epoch  16 | 1000/1106 batches | lr 20.0000 | ms/batch 106.58 | loss  4.31 | ppl    74.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 124.93s | valid loss  4.52 | valid ppl    91.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 | 200/1106 batches | lr 20.0000 | ms/batch 110.75 | loss  4.34 | ppl    76.75\n",
      "| epoch  17 | 400/1106 batches | lr 20.0000 | ms/batch 109.24 | loss  4.20 | ppl    66.40\n",
      "| epoch  17 | 600/1106 batches | lr 20.0000 | ms/batch 108.61 | loss  4.23 | ppl    68.70\n",
      "| epoch  17 | 800/1106 batches | lr 20.0000 | ms/batch 108.90 | loss  4.26 | ppl    71.10\n",
      "| epoch  17 | 1000/1106 batches | lr 20.0000 | ms/batch 104.81 | loss  4.28 | ppl    71.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 125.74s | valid loss  4.51 | valid ppl    91.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 | 200/1106 batches | lr 20.0000 | ms/batch 110.61 | loss  4.31 | ppl    74.16\n",
      "| epoch  18 | 400/1106 batches | lr 20.0000 | ms/batch 109.74 | loss  4.18 | ppl    65.29\n",
      "| epoch  18 | 600/1106 batches | lr 20.0000 | ms/batch 108.04 | loss  4.19 | ppl    66.27\n",
      "| epoch  18 | 800/1106 batches | lr 20.0000 | ms/batch 111.12 | loss  4.21 | ppl    67.68\n",
      "| epoch  18 | 1000/1106 batches | lr 20.0000 | ms/batch 105.10 | loss  4.25 | ppl    70.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 126.93s | valid loss  4.50 | valid ppl    90.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 | 200/1106 batches | lr 20.0000 | ms/batch 111.62 | loss  4.28 | ppl    72.32\n",
      "| epoch  19 | 400/1106 batches | lr 20.0000 | ms/batch 108.73 | loss  4.15 | ppl    63.17\n",
      "| epoch  19 | 600/1106 batches | lr 20.0000 | ms/batch 111.72 | loss  4.16 | ppl    64.34\n",
      "| epoch  19 | 800/1106 batches | lr 20.0000 | ms/batch 107.15 | loss  4.19 | ppl    65.70\n",
      "| epoch  19 | 1000/1106 batches | lr 20.0000 | ms/batch 109.51 | loss  4.23 | ppl    68.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 128.54s | valid loss  4.49 | valid ppl    89.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  20 | 200/1106 batches | lr 20.0000 | ms/batch 112.58 | loss  4.26 | ppl    70.78\n",
      "| epoch  20 | 400/1106 batches | lr 20.0000 | ms/batch 111.27 | loss  4.12 | ppl    61.62\n",
      "| epoch  20 | 600/1106 batches | lr 20.0000 | ms/batch 110.81 | loss  4.14 | ppl    63.00\n",
      "| epoch  20 | 800/1106 batches | lr 20.0000 | ms/batch 108.71 | loss  4.16 | ppl    64.39\n",
      "| epoch  20 | 1000/1106 batches | lr 20.0000 | ms/batch 107.63 | loss  4.19 | ppl    66.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 128.50s | valid loss  4.48 | valid ppl    88.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  21 | 200/1106 batches | lr 20.0000 | ms/batch 113.35 | loss  4.22 | ppl    68.20\n",
      "| epoch  21 | 400/1106 batches | lr 20.0000 | ms/batch 108.30 | loss  4.10 | ppl    60.62\n",
      "| epoch  21 | 600/1106 batches | lr 20.0000 | ms/batch 107.24 | loss  4.13 | ppl    61.96\n",
      "| epoch  21 | 800/1106 batches | lr 20.0000 | ms/batch 108.16 | loss  4.15 | ppl    63.55\n",
      "| epoch  21 | 1000/1106 batches | lr 20.0000 | ms/batch 108.25 | loss  4.17 | ppl    64.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 126.90s | valid loss  4.47 | valid ppl    87.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 | 200/1106 batches | lr 20.0000 | ms/batch 111.99 | loss  4.20 | ppl    66.89\n",
      "| epoch  22 | 400/1106 batches | lr 20.0000 | ms/batch 108.52 | loss  4.08 | ppl    59.26\n",
      "| epoch  22 | 600/1106 batches | lr 20.0000 | ms/batch 110.70 | loss  4.10 | ppl    60.23\n",
      "| epoch  22 | 800/1106 batches | lr 20.0000 | ms/batch 106.25 | loss  4.12 | ppl    61.77\n",
      "| epoch  22 | 1000/1106 batches | lr 20.0000 | ms/batch 108.77 | loss  4.17 | ppl    64.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 127.49s | valid loss  4.47 | valid ppl    87.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 200/1106 batches | lr 20.0000 | ms/batch 107.49 | loss  4.18 | ppl    65.58\n",
      "| epoch  23 | 400/1106 batches | lr 20.0000 | ms/batch 113.26 | loss  4.07 | ppl    58.52\n",
      "| epoch  23 | 600/1106 batches | lr 20.0000 | ms/batch 110.25 | loss  4.07 | ppl    58.71\n",
      "| epoch  23 | 800/1106 batches | lr 20.0000 | ms/batch 110.66 | loss  4.11 | ppl    60.88\n",
      "| epoch  23 | 1000/1106 batches | lr 20.0000 | ms/batch 106.75 | loss  4.14 | ppl    62.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 127.59s | valid loss  4.47 | valid ppl    87.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 | 200/1106 batches | lr 20.0000 | ms/batch 107.40 | loss  4.16 | ppl    63.97\n",
      "| epoch  24 | 400/1106 batches | lr 20.0000 | ms/batch 111.19 | loss  4.05 | ppl    57.20\n",
      "| epoch  24 | 600/1106 batches | lr 20.0000 | ms/batch 105.47 | loss  4.06 | ppl    58.15\n",
      "| epoch  24 | 800/1106 batches | lr 20.0000 | ms/batch 108.32 | loss  4.08 | ppl    59.27\n",
      "| epoch  24 | 1000/1106 batches | lr 20.0000 | ms/batch 106.18 | loss  4.13 | ppl    62.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 127.37s | valid loss  4.44 | valid ppl    85.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  25 | 200/1106 batches | lr 20.0000 | ms/batch 107.46 | loss  4.14 | ppl    62.77\n",
      "| epoch  25 | 400/1106 batches | lr 20.0000 | ms/batch 109.87 | loss  4.03 | ppl    56.18\n",
      "| epoch  25 | 600/1106 batches | lr 20.0000 | ms/batch 109.44 | loss  4.05 | ppl    57.13\n",
      "| epoch  25 | 800/1106 batches | lr 20.0000 | ms/batch 109.62 | loss  4.07 | ppl    58.61\n",
      "| epoch  25 | 1000/1106 batches | lr 20.0000 | ms/batch 112.17 | loss  4.09 | ppl    59.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 127.32s | valid loss  4.44 | valid ppl    85.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 | 200/1106 batches | lr 20.0000 | ms/batch 108.77 | loss  4.13 | ppl    62.18\n",
      "| epoch  26 | 400/1106 batches | lr 20.0000 | ms/batch 107.55 | loss  4.02 | ppl    55.67\n",
      "| epoch  26 | 600/1106 batches | lr 20.0000 | ms/batch 112.92 | loss  4.03 | ppl    56.49\n",
      "| epoch  26 | 800/1106 batches | lr 20.0000 | ms/batch 112.12 | loss  4.06 | ppl    57.99\n",
      "| epoch  26 | 1000/1106 batches | lr 20.0000 | ms/batch 110.83 | loss  4.07 | ppl    58.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 128.43s | valid loss  4.45 | valid ppl    85.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 | 200/1106 batches | lr 20.0000 | ms/batch 117.75 | loss  4.10 | ppl    60.58\n",
      "| epoch  27 | 400/1106 batches | lr 20.0000 | ms/batch 111.30 | loss  3.99 | ppl    54.30\n",
      "| epoch  27 | 600/1106 batches | lr 20.0000 | ms/batch 107.30 | loss  4.00 | ppl    54.81\n",
      "| epoch  27 | 800/1106 batches | lr 20.0000 | ms/batch 110.11 | loss  4.02 | ppl    55.75\n",
      "| epoch  27 | 1000/1106 batches | lr 20.0000 | ms/batch 112.84 | loss  4.06 | ppl    58.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 129.03s | valid loss  4.44 | valid ppl    84.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  28 | 200/1106 batches | lr 20.0000 | ms/batch 122.67 | loss  4.09 | ppl    60.03\n",
      "| epoch  28 | 400/1106 batches | lr 20.0000 | ms/batch 132.69 | loss  3.98 | ppl    53.52\n",
      "| epoch  28 | 600/1106 batches | lr 20.0000 | ms/batch 127.33 | loss  4.00 | ppl    54.44\n",
      "| epoch  28 | 800/1106 batches | lr 20.0000 | ms/batch 117.80 | loss  4.02 | ppl    55.52\n",
      "| epoch  28 | 1000/1106 batches | lr 20.0000 | ms/batch 111.11 | loss  4.06 | ppl    58.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 140.45s | valid loss  4.44 | valid ppl    84.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 | 200/1106 batches | lr 20.0000 | ms/batch 112.19 | loss  4.08 | ppl    59.09\n",
      "| epoch  29 | 400/1106 batches | lr 20.0000 | ms/batch 109.86 | loss  3.96 | ppl    52.60\n",
      "| epoch  29 | 600/1106 batches | lr 20.0000 | ms/batch 111.39 | loss  3.97 | ppl    52.80\n",
      "| epoch  29 | 800/1106 batches | lr 20.0000 | ms/batch 110.56 | loss  4.01 | ppl    55.24\n",
      "| epoch  29 | 1000/1106 batches | lr 20.0000 | ms/batch 113.02 | loss  4.02 | ppl    55.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 129.10s | valid loss  4.44 | valid ppl    84.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 | 200/1106 batches | lr 20.0000 | ms/batch 114.98 | loss  4.06 | ppl    58.05\n",
      "| epoch  30 | 400/1106 batches | lr 20.0000 | ms/batch 123.90 | loss  3.95 | ppl    51.96\n",
      "| epoch  30 | 600/1106 batches | lr 20.0000 | ms/batch 127.19 | loss  3.96 | ppl    52.38\n",
      "| epoch  30 | 800/1106 batches | lr 20.0000 | ms/batch 133.85 | loss  3.99 | ppl    54.12\n",
      "| epoch  30 | 1000/1106 batches | lr 20.0000 | ms/batch 126.46 | loss  4.00 | ppl    54.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 147.23s | valid loss  4.43 | valid ppl    83.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  31 | 200/1106 batches | lr 20.0000 | ms/batch 127.84 | loss  4.04 | ppl    56.82\n",
      "| epoch  31 | 400/1106 batches | lr 20.0000 | ms/batch 128.87 | loss  3.93 | ppl    50.84\n",
      "| epoch  31 | 600/1106 batches | lr 20.0000 | ms/batch 129.34 | loss  3.95 | ppl    52.06\n",
      "| epoch  31 | 800/1106 batches | lr 20.0000 | ms/batch 130.80 | loss  3.97 | ppl    53.24\n",
      "| epoch  31 | 1000/1106 batches | lr 20.0000 | ms/batch 131.00 | loss  4.00 | ppl    54.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 149.12s | valid loss  4.43 | valid ppl    83.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 | 200/1106 batches | lr 20.0000 | ms/batch 128.60 | loss  4.03 | ppl    56.01\n",
      "| epoch  32 | 400/1106 batches | lr 20.0000 | ms/batch 138.57 | loss  3.93 | ppl    50.66\n",
      "| epoch  32 | 600/1106 batches | lr 20.0000 | ms/batch 122.08 | loss  3.95 | ppl    51.74\n",
      "| epoch  32 | 800/1106 batches | lr 20.0000 | ms/batch 133.13 | loss  3.97 | ppl    52.78\n",
      "| epoch  32 | 1000/1106 batches | lr 20.0000 | ms/batch 137.50 | loss  4.00 | ppl    54.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 152.97s | valid loss  4.42 | valid ppl    83.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  33 | 200/1106 batches | lr 20.0000 | ms/batch 136.86 | loss  4.02 | ppl    55.93\n",
      "| epoch  33 | 400/1106 batches | lr 20.0000 | ms/batch 129.42 | loss  3.91 | ppl    49.70\n",
      "| epoch  33 | 600/1106 batches | lr 20.0000 | ms/batch 123.77 | loss  3.91 | ppl    49.81\n",
      "| epoch  33 | 800/1106 batches | lr 20.0000 | ms/batch 119.95 | loss  3.96 | ppl    52.70\n",
      "| epoch  33 | 1000/1106 batches | lr 20.0000 | ms/batch 121.61 | loss  3.98 | ppl    53.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 146.73s | valid loss  4.42 | valid ppl    83.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 | 200/1106 batches | lr 20.0000 | ms/batch 131.80 | loss  4.02 | ppl    55.50\n",
      "| epoch  34 | 400/1106 batches | lr 20.0000 | ms/batch 136.45 | loss  3.89 | ppl    49.07\n",
      "| epoch  34 | 600/1106 batches | lr 20.0000 | ms/batch 124.03 | loss  3.91 | ppl    49.99\n",
      "| epoch  34 | 800/1106 batches | lr 20.0000 | ms/batch 128.15 | loss  3.94 | ppl    51.52\n",
      "| epoch  34 | 1000/1106 batches | lr 20.0000 | ms/batch 132.60 | loss  3.98 | ppl    53.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 153.32s | valid loss  4.41 | valid ppl    82.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  35 | 200/1106 batches | lr 20.0000 | ms/batch 132.04 | loss  4.00 | ppl    54.65\n",
      "| epoch  35 | 400/1106 batches | lr 20.0000 | ms/batch 132.80 | loss  3.88 | ppl    48.48\n",
      "| epoch  35 | 600/1106 batches | lr 20.0000 | ms/batch 124.74 | loss  3.91 | ppl    50.10\n",
      "| epoch  35 | 800/1106 batches | lr 20.0000 | ms/batch 131.23 | loss  3.93 | ppl    50.87\n",
      "| epoch  35 | 1000/1106 batches | lr 20.0000 | ms/batch 127.33 | loss  3.96 | ppl    52.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 149.76s | valid loss  4.42 | valid ppl    83.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 | 200/1106 batches | lr 20.0000 | ms/batch 129.27 | loss  3.99 | ppl    54.23\n",
      "| epoch  36 | 400/1106 batches | lr 20.0000 | ms/batch 131.41 | loss  3.87 | ppl    48.01\n",
      "| epoch  36 | 600/1106 batches | lr 20.0000 | ms/batch 132.10 | loss  3.88 | ppl    48.60\n",
      "| epoch  36 | 800/1106 batches | lr 20.0000 | ms/batch 124.05 | loss  3.92 | ppl    50.62\n",
      "| epoch  36 | 1000/1106 batches | lr 20.0000 | ms/batch 137.68 | loss  3.95 | ppl    51.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 151.97s | valid loss  4.42 | valid ppl    82.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 | 200/1106 batches | lr 20.0000 | ms/batch 129.61 | loss  3.97 | ppl    53.04\n",
      "| epoch  37 | 400/1106 batches | lr 20.0000 | ms/batch 121.56 | loss  3.86 | ppl    47.40\n",
      "| epoch  37 | 600/1106 batches | lr 20.0000 | ms/batch 126.44 | loss  3.87 | ppl    48.17\n",
      "| epoch  37 | 800/1106 batches | lr 20.0000 | ms/batch 122.65 | loss  3.90 | ppl    49.56\n",
      "| epoch  37 | 1000/1106 batches | lr 20.0000 | ms/batch 130.63 | loss  3.94 | ppl    51.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 148.69s | valid loss  4.41 | valid ppl    82.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 | 200/1106 batches | lr 20.0000 | ms/batch 126.59 | loss  3.95 | ppl    51.95\n",
      "| epoch  38 | 400/1106 batches | lr 20.0000 | ms/batch 127.23 | loss  3.86 | ppl    47.34\n",
      "| epoch  38 | 600/1106 batches | lr 20.0000 | ms/batch 130.28 | loss  3.88 | ppl    48.31\n",
      "| epoch  38 | 800/1106 batches | lr 20.0000 | ms/batch 125.27 | loss  3.90 | ppl    49.27\n",
      "| epoch  38 | 1000/1106 batches | lr 20.0000 | ms/batch 133.67 | loss  3.94 | ppl    51.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 151.54s | valid loss  4.41 | valid ppl    82.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 | 200/1106 batches | lr 20.0000 | ms/batch 129.78 | loss  3.95 | ppl    51.98\n",
      "| epoch  39 | 400/1106 batches | lr 20.0000 | ms/batch 125.46 | loss  3.84 | ppl    46.64\n",
      "| epoch  39 | 600/1106 batches | lr 20.0000 | ms/batch 128.94 | loss  3.85 | ppl    47.10\n",
      "| epoch  39 | 800/1106 batches | lr 20.0000 | ms/batch 127.12 | loss  3.88 | ppl    48.66\n",
      "| epoch  39 | 1000/1106 batches | lr 20.0000 | ms/batch 129.89 | loss  3.91 | ppl    49.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 150.37s | valid loss  4.41 | valid ppl    82.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  40 | 200/1106 batches | lr 20.0000 | ms/batch 137.92 | loss  3.94 | ppl    51.21\n",
      "| epoch  40 | 400/1106 batches | lr 20.0000 | ms/batch 137.64 | loss  3.85 | ppl    46.78\n",
      "| epoch  40 | 600/1106 batches | lr 20.0000 | ms/batch 123.21 | loss  3.85 | ppl    47.16\n",
      "| epoch  40 | 800/1106 batches | lr 20.0000 | ms/batch 133.67 | loss  3.88 | ppl    48.22\n",
      "| epoch  40 | 1000/1106 batches | lr 20.0000 | ms/batch 123.58 | loss  3.90 | ppl    49.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 153.65s | valid loss  4.41 | valid ppl    81.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  41 | 200/1106 batches | lr 20.0000 | ms/batch 130.80 | loss  3.93 | ppl    50.68\n",
      "| epoch  41 | 400/1106 batches | lr 20.0000 | ms/batch 134.39 | loss  3.83 | ppl    46.26\n",
      "| epoch  41 | 600/1106 batches | lr 20.0000 | ms/batch 121.38 | loss  3.83 | ppl    46.06\n",
      "| epoch  41 | 800/1106 batches | lr 20.0000 | ms/batch 122.52 | loss  3.88 | ppl    48.29\n",
      "| epoch  41 | 1000/1106 batches | lr 20.0000 | ms/batch 124.64 | loss  3.89 | ppl    49.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 149.05s | valid loss  4.41 | valid ppl    82.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 | 200/1106 batches | lr 20.0000 | ms/batch 132.23 | loss  3.93 | ppl    50.81\n",
      "| epoch  42 | 400/1106 batches | lr 20.0000 | ms/batch 135.97 | loss  3.84 | ppl    46.41\n",
      "| epoch  42 | 600/1106 batches | lr 20.0000 | ms/batch 131.20 | loss  3.82 | ppl    45.77\n",
      "| epoch  42 | 800/1106 batches | lr 20.0000 | ms/batch 132.32 | loss  3.86 | ppl    47.47\n",
      "| epoch  42 | 1000/1106 batches | lr 20.0000 | ms/batch 134.23 | loss  3.89 | ppl    49.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 152.77s | valid loss  4.41 | valid ppl    82.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 | 200/1106 batches | lr 20.0000 | ms/batch 130.84 | loss  3.91 | ppl    50.05\n",
      "| epoch  43 | 400/1106 batches | lr 20.0000 | ms/batch 130.98 | loss  3.81 | ppl    45.23\n",
      "| epoch  43 | 600/1106 batches | lr 20.0000 | ms/batch 116.18 | loss  3.82 | ppl    45.76\n",
      "| epoch  43 | 800/1106 batches | lr 20.0000 | ms/batch 120.12 | loss  3.85 | ppl    46.84\n",
      "| epoch  43 | 1000/1106 batches | lr 20.0000 | ms/batch 123.79 | loss  3.87 | ppl    48.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 145.77s | valid loss  4.41 | valid ppl    82.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 | 200/1106 batches | lr 20.0000 | ms/batch 118.85 | loss  3.90 | ppl    49.52\n",
      "| epoch  44 | 400/1106 batches | lr 20.0000 | ms/batch 131.48 | loss  3.80 | ppl    44.88\n",
      "| epoch  44 | 600/1106 batches | lr 20.0000 | ms/batch 126.67 | loss  3.81 | ppl    45.11\n",
      "| epoch  44 | 800/1106 batches | lr 20.0000 | ms/batch 122.23 | loss  3.82 | ppl    45.60\n",
      "| epoch  44 | 1000/1106 batches | lr 20.0000 | ms/batch 130.26 | loss  3.87 | ppl    47.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 148.57s | valid loss  4.41 | valid ppl    82.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 | 200/1106 batches | lr 20.0000 | ms/batch 131.67 | loss  3.90 | ppl    49.29\n",
      "| epoch  45 | 400/1106 batches | lr 20.0000 | ms/batch 117.82 | loss  3.79 | ppl    44.30\n",
      "| epoch  45 | 600/1106 batches | lr 20.0000 | ms/batch 123.61 | loss  3.82 | ppl    45.50\n",
      "| epoch  45 | 800/1106 batches | lr 20.0000 | ms/batch 130.67 | loss  3.82 | ppl    45.79\n",
      "| epoch  45 | 1000/1106 batches | lr 20.0000 | ms/batch 128.60 | loss  3.86 | ppl    47.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 147.01s | valid loss  4.41 | valid ppl    82.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 | 200/1106 batches | lr 20.0000 | ms/batch 135.42 | loss  3.88 | ppl    48.62\n",
      "| epoch  46 | 400/1106 batches | lr 20.0000 | ms/batch 130.97 | loss  3.79 | ppl    44.12\n",
      "| epoch  46 | 600/1106 batches | lr 20.0000 | ms/batch 129.57 | loss  3.79 | ppl    44.29\n",
      "| epoch  46 | 800/1106 batches | lr 20.0000 | ms/batch 125.26 | loss  3.83 | ppl    45.95\n",
      "| epoch  46 | 1000/1106 batches | lr 20.0000 | ms/batch 132.59 | loss  3.85 | ppl    47.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 151.90s | valid loss  4.41 | valid ppl    82.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 | 200/1106 batches | lr 20.0000 | ms/batch 135.63 | loss  3.88 | ppl    48.58\n",
      "| epoch  47 | 400/1106 batches | lr 20.0000 | ms/batch 118.88 | loss  3.76 | ppl    43.05\n",
      "| epoch  47 | 600/1106 batches | lr 20.0000 | ms/batch 140.65 | loss  3.79 | ppl    44.36\n",
      "| epoch  47 | 800/1106 batches | lr 20.0000 | ms/batch 131.92 | loss  3.82 | ppl    45.54\n",
      "| epoch  47 | 1000/1106 batches | lr 20.0000 | ms/batch 129.84 | loss  3.84 | ppl    46.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 153.17s | valid loss  4.40 | valid ppl    81.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  48 | 200/1106 batches | lr 20.0000 | ms/batch 134.69 | loss  3.87 | ppl    48.00\n",
      "| epoch  48 | 400/1106 batches | lr 20.0000 | ms/batch 126.71 | loss  3.77 | ppl    43.54\n",
      "| epoch  48 | 600/1106 batches | lr 20.0000 | ms/batch 134.19 | loss  3.78 | ppl    43.93\n",
      "| epoch  48 | 800/1106 batches | lr 20.0000 | ms/batch 131.23 | loss  3.80 | ppl    44.67\n",
      "| epoch  48 | 1000/1106 batches | lr 20.0000 | ms/batch 129.39 | loss  3.84 | ppl    46.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 152.41s | valid loss  4.40 | valid ppl    81.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  49 | 200/1106 batches | lr 20.0000 | ms/batch 129.27 | loss  3.86 | ppl    47.39\n",
      "| epoch  49 | 400/1106 batches | lr 20.0000 | ms/batch 126.30 | loss  3.76 | ppl    42.98\n",
      "| epoch  49 | 600/1106 batches | lr 20.0000 | ms/batch 123.17 | loss  3.77 | ppl    43.51\n",
      "| epoch  49 | 800/1106 batches | lr 20.0000 | ms/batch 135.56 | loss  3.79 | ppl    44.37\n",
      "| epoch  49 | 1000/1106 batches | lr 20.0000 | ms/batch 127.24 | loss  3.84 | ppl    46.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 149.65s | valid loss  4.41 | valid ppl    82.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 | 200/1106 batches | lr 20.0000 | ms/batch 131.58 | loss  3.86 | ppl    47.33\n",
      "| epoch  50 | 400/1106 batches | lr 20.0000 | ms/batch 131.19 | loss  3.75 | ppl    42.61\n",
      "| epoch  50 | 600/1106 batches | lr 20.0000 | ms/batch 131.84 | loss  3.77 | ppl    43.17\n",
      "| epoch  50 | 800/1106 batches | lr 20.0000 | ms/batch 123.79 | loss  3.79 | ppl    44.24\n",
      "| epoch  50 | 1000/1106 batches | lr 20.0000 | ms/batch 124.43 | loss  3.83 | ppl    46.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 151.04s | valid loss  4.40 | valid ppl    81.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 | 200/1106 batches | lr 20.0000 | ms/batch 119.00 | loss  3.85 | ppl    47.14\n",
      "| epoch  51 | 400/1106 batches | lr 20.0000 | ms/batch 129.13 | loss  3.74 | ppl    42.19\n",
      "| epoch  51 | 600/1106 batches | lr 20.0000 | ms/batch 132.97 | loss  3.76 | ppl    43.11\n",
      "| epoch  51 | 800/1106 batches | lr 20.0000 | ms/batch 127.05 | loss  3.78 | ppl    44.01\n",
      "| epoch  51 | 1000/1106 batches | lr 20.0000 | ms/batch 130.06 | loss  3.82 | ppl    45.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 148.64s | valid loss  4.40 | valid ppl    81.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 | 200/1106 batches | lr 20.0000 | ms/batch 131.97 | loss  3.85 | ppl    46.78\n",
      "| epoch  52 | 400/1106 batches | lr 20.0000 | ms/batch 119.01 | loss  3.74 | ppl    42.12\n",
      "| epoch  52 | 600/1106 batches | lr 20.0000 | ms/batch 120.91 | loss  3.76 | ppl    42.77\n",
      "| epoch  52 | 800/1106 batches | lr 20.0000 | ms/batch 132.63 | loss  3.79 | ppl    44.08\n",
      "| epoch  52 | 1000/1106 batches | lr 20.0000 | ms/batch 126.27 | loss  3.81 | ppl    45.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 146.26s | valid loss  4.41 | valid ppl    82.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 | 200/1106 batches | lr 20.0000 | ms/batch 119.20 | loss  3.84 | ppl    46.36\n",
      "| epoch  53 | 400/1106 batches | lr 20.0000 | ms/batch 118.47 | loss  3.74 | ppl    42.06\n",
      "| epoch  53 | 600/1106 batches | lr 20.0000 | ms/batch 129.30 | loss  3.75 | ppl    42.55\n",
      "| epoch  53 | 800/1106 batches | lr 20.0000 | ms/batch 122.89 | loss  3.76 | ppl    43.09\n",
      "| epoch  53 | 1000/1106 batches | lr 20.0000 | ms/batch 125.83 | loss  3.81 | ppl    45.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 143.62s | valid loss  4.41 | valid ppl    81.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 | 200/1106 batches | lr 20.0000 | ms/batch 123.53 | loss  3.83 | ppl    45.86\n",
      "| epoch  54 | 400/1106 batches | lr 20.0000 | ms/batch 117.68 | loss  3.73 | ppl    41.66\n",
      "| epoch  54 | 600/1106 batches | lr 20.0000 | ms/batch 122.71 | loss  3.76 | ppl    42.76\n",
      "| epoch  54 | 800/1106 batches | lr 20.0000 | ms/batch 128.61 | loss  3.77 | ppl    43.31\n",
      "| epoch  54 | 1000/1106 batches | lr 20.0000 | ms/batch 124.69 | loss  3.81 | ppl    44.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 146.31s | valid loss  4.41 | valid ppl    82.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 | 200/1106 batches | lr 20.0000 | ms/batch 137.74 | loss  3.82 | ppl    45.71\n",
      "| epoch  55 | 400/1106 batches | lr 20.0000 | ms/batch 122.61 | loss  3.72 | ppl    41.41\n",
      "| epoch  55 | 600/1106 batches | lr 20.0000 | ms/batch 125.47 | loss  3.74 | ppl    42.24\n",
      "| epoch  55 | 800/1106 batches | lr 20.0000 | ms/batch 124.39 | loss  3.76 | ppl    42.94\n",
      "| epoch  55 | 1000/1106 batches | lr 20.0000 | ms/batch 118.81 | loss  3.80 | ppl    44.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 147.60s | valid loss  4.41 | valid ppl    81.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 | 200/1106 batches | lr 20.0000 | ms/batch 135.22 | loss  3.81 | ppl    45.23\n",
      "| epoch  56 | 400/1106 batches | lr 20.0000 | ms/batch 126.40 | loss  3.71 | ppl    40.96\n",
      "| epoch  56 | 600/1106 batches | lr 20.0000 | ms/batch 129.82 | loss  3.75 | ppl    42.68\n",
      "| epoch  56 | 800/1106 batches | lr 20.0000 | ms/batch 123.74 | loss  3.75 | ppl    42.41\n",
      "| epoch  56 | 1000/1106 batches | lr 20.0000 | ms/batch 122.29 | loss  3.78 | ppl    43.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 149.41s | valid loss  4.41 | valid ppl    82.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 | 200/1106 batches | lr 20.0000 | ms/batch 132.31 | loss  3.80 | ppl    44.77\n",
      "| epoch  57 | 400/1106 batches | lr 20.0000 | ms/batch 122.99 | loss  3.71 | ppl    41.01\n",
      "| epoch  57 | 600/1106 batches | lr 20.0000 | ms/batch 124.80 | loss  3.72 | ppl    41.27\n",
      "| epoch  57 | 800/1106 batches | lr 20.0000 | ms/batch 124.02 | loss  3.73 | ppl    41.84\n",
      "| epoch  57 | 1000/1106 batches | lr 20.0000 | ms/batch 128.55 | loss  3.78 | ppl    43.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 148.98s | valid loss  4.41 | valid ppl    82.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 | 200/1106 batches | lr 20.0000 | ms/batch 137.65 | loss  3.82 | ppl    45.38\n",
      "| epoch  58 | 400/1106 batches | lr 20.0000 | ms/batch 129.96 | loss  3.71 | ppl    40.94\n",
      "| epoch  58 | 600/1106 batches | lr 20.0000 | ms/batch 116.47 | loss  3.71 | ppl    41.03\n",
      "| epoch  58 | 800/1106 batches | lr 20.0000 | ms/batch 119.21 | loss  3.74 | ppl    41.93\n",
      "| epoch  58 | 1000/1106 batches | lr 20.0000 | ms/batch 126.34 | loss  3.76 | ppl    42.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 146.76s | valid loss  4.39 | valid ppl    80.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  59 | 200/1106 batches | lr 20.0000 | ms/batch 135.12 | loss  3.80 | ppl    44.74\n",
      "| epoch  59 | 400/1106 batches | lr 20.0000 | ms/batch 123.66 | loss  3.70 | ppl    40.64\n",
      "| epoch  59 | 600/1106 batches | lr 20.0000 | ms/batch 129.98 | loss  3.71 | ppl    41.00\n",
      "| epoch  59 | 800/1106 batches | lr 20.0000 | ms/batch 128.25 | loss  3.72 | ppl    41.36\n",
      "| epoch  59 | 1000/1106 batches | lr 20.0000 | ms/batch 123.46 | loss  3.76 | ppl    42.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 148.83s | valid loss  4.40 | valid ppl    81.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 | 200/1106 batches | lr 20.0000 | ms/batch 125.57 | loss  3.79 | ppl    44.39\n",
      "| epoch  60 | 400/1106 batches | lr 20.0000 | ms/batch 119.48 | loss  3.69 | ppl    39.89\n",
      "| epoch  60 | 600/1106 batches | lr 20.0000 | ms/batch 130.49 | loss  3.70 | ppl    40.62\n",
      "| epoch  60 | 800/1106 batches | lr 20.0000 | ms/batch 136.11 | loss  3.72 | ppl    41.22\n",
      "| epoch  60 | 1000/1106 batches | lr 20.0000 | ms/batch 127.65 | loss  3.78 | ppl    43.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 150.91s | valid loss  4.42 | valid ppl    82.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 | 200/1106 batches | lr 20.0000 | ms/batch 132.94 | loss  3.79 | ppl    44.33\n",
      "| epoch  61 | 400/1106 batches | lr 20.0000 | ms/batch 125.14 | loss  3.68 | ppl    39.77\n",
      "| epoch  61 | 600/1106 batches | lr 20.0000 | ms/batch 121.65 | loss  3.69 | ppl    40.20\n",
      "| epoch  61 | 800/1106 batches | lr 20.0000 | ms/batch 127.81 | loss  3.73 | ppl    41.60\n",
      "| epoch  61 | 1000/1106 batches | lr 20.0000 | ms/batch 115.78 | loss  3.76 | ppl    42.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 144.27s | valid loss  4.40 | valid ppl    81.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 | 200/1106 batches | lr 20.0000 | ms/batch 132.30 | loss  3.79 | ppl    44.21\n",
      "| epoch  62 | 400/1106 batches | lr 20.0000 | ms/batch 129.18 | loss  3.68 | ppl    39.79\n",
      "| epoch  62 | 600/1106 batches | lr 20.0000 | ms/batch 128.45 | loss  3.70 | ppl    40.40\n",
      "| epoch  62 | 800/1106 batches | lr 20.0000 | ms/batch 128.25 | loss  3.72 | ppl    41.16\n",
      "| epoch  62 | 1000/1106 batches | lr 20.0000 | ms/batch 127.34 | loss  3.77 | ppl    43.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 149.36s | valid loss  4.41 | valid ppl    82.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 | 200/1106 batches | lr 20.0000 | ms/batch 124.97 | loss  3.78 | ppl    43.82\n",
      "| epoch  63 | 400/1106 batches | lr 20.0000 | ms/batch 127.89 | loss  3.67 | ppl    39.43\n",
      "| epoch  63 | 600/1106 batches | lr 20.0000 | ms/batch 131.33 | loss  3.69 | ppl    40.06\n",
      "| epoch  63 | 800/1106 batches | lr 20.0000 | ms/batch 124.81 | loss  3.71 | ppl    40.87\n",
      "| epoch  63 | 1000/1106 batches | lr 20.0000 | ms/batch 131.04 | loss  3.75 | ppl    42.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 149.01s | valid loss  4.41 | valid ppl    82.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 | 200/1106 batches | lr 20.0000 | ms/batch 125.18 | loss  3.78 | ppl    43.88\n",
      "| epoch  64 | 400/1106 batches | lr 20.0000 | ms/batch 127.92 | loss  3.68 | ppl    39.70\n",
      "| epoch  64 | 600/1106 batches | lr 20.0000 | ms/batch 126.30 | loss  3.69 | ppl    39.85\n",
      "| epoch  64 | 800/1106 batches | lr 20.0000 | ms/batch 127.41 | loss  3.69 | ppl    40.09\n",
      "| epoch  64 | 1000/1106 batches | lr 20.0000 | ms/batch 135.43 | loss  3.75 | ppl    42.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 150.12s | valid loss  4.40 | valid ppl    81.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 | 200/1106 batches | lr 20.0000 | ms/batch 132.99 | loss  3.77 | ppl    43.52\n",
      "| epoch  65 | 400/1106 batches | lr 20.0000 | ms/batch 125.53 | loss  3.66 | ppl    38.87\n",
      "| epoch  65 | 600/1106 batches | lr 20.0000 | ms/batch 122.56 | loss  3.69 | ppl    39.99\n",
      "| epoch  65 | 800/1106 batches | lr 20.0000 | ms/batch 129.01 | loss  3.70 | ppl    40.45\n",
      "| epoch  65 | 1000/1106 batches | lr 20.0000 | ms/batch 140.73 | loss  3.76 | ppl    42.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 151.10s | valid loss  4.41 | valid ppl    82.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 | 200/1106 batches | lr 20.0000 | ms/batch 133.86 | loss  3.76 | ppl    43.03\n",
      "| epoch  66 | 400/1106 batches | lr 20.0000 | ms/batch 127.48 | loss  3.65 | ppl    38.39\n",
      "| epoch  66 | 600/1106 batches | lr 20.0000 | ms/batch 127.79 | loss  3.68 | ppl    39.78\n",
      "| epoch  66 | 800/1106 batches | lr 20.0000 | ms/batch 131.79 | loss  3.69 | ppl    40.23\n",
      "| epoch  66 | 1000/1106 batches | lr 20.0000 | ms/batch 128.01 | loss  3.73 | ppl    41.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 148.91s | valid loss  4.40 | valid ppl    81.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 | 200/1106 batches | lr 20.0000 | ms/batch 124.48 | loss  3.76 | ppl    43.00\n",
      "| epoch  67 | 400/1106 batches | lr 20.0000 | ms/batch 132.02 | loss  3.66 | ppl    38.73\n",
      "| epoch  67 | 600/1106 batches | lr 20.0000 | ms/batch 127.07 | loss  3.66 | ppl    39.03\n",
      "| epoch  67 | 800/1106 batches | lr 20.0000 | ms/batch 140.36 | loss  3.69 | ppl    40.15\n",
      "| epoch  67 | 1000/1106 batches | lr 20.0000 | ms/batch 129.82 | loss  3.74 | ppl    42.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 152.33s | valid loss  4.42 | valid ppl    82.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 | 200/1106 batches | lr 20.0000 | ms/batch 124.65 | loss  3.76 | ppl    42.82\n",
      "| epoch  68 | 400/1106 batches | lr 20.0000 | ms/batch 129.42 | loss  3.66 | ppl    38.87\n",
      "| epoch  68 | 600/1106 batches | lr 20.0000 | ms/batch 138.19 | loss  3.67 | ppl    39.34\n",
      "| epoch  68 | 800/1106 batches | lr 20.0000 | ms/batch 130.24 | loss  3.67 | ppl    39.35\n",
      "| epoch  68 | 1000/1106 batches | lr 20.0000 | ms/batch 119.80 | loss  3.72 | ppl    41.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 148.96s | valid loss  4.41 | valid ppl    82.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 | 200/1106 batches | lr 20.0000 | ms/batch 124.31 | loss  3.75 | ppl    42.58\n",
      "| epoch  69 | 400/1106 batches | lr 20.0000 | ms/batch 127.79 | loss  3.65 | ppl    38.38\n",
      "| epoch  69 | 600/1106 batches | lr 20.0000 | ms/batch 131.67 | loss  3.66 | ppl    38.93\n",
      "| epoch  69 | 800/1106 batches | lr 20.0000 | ms/batch 132.54 | loss  3.68 | ppl    39.73\n",
      "| epoch  69 | 1000/1106 batches | lr 20.0000 | ms/batch 127.47 | loss  3.72 | ppl    41.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 149.57s | valid loss  4.41 | valid ppl    81.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 | 200/1106 batches | lr 20.0000 | ms/batch 133.63 | loss  3.74 | ppl    42.12\n",
      "| epoch  70 | 400/1106 batches | lr 20.0000 | ms/batch 133.72 | loss  3.64 | ppl    38.21\n",
      "| epoch  70 | 600/1106 batches | lr 20.0000 | ms/batch 121.76 | loss  3.66 | ppl    38.68\n",
      "| epoch  70 | 800/1106 batches | lr 20.0000 | ms/batch 126.72 | loss  3.67 | ppl    39.41\n",
      "| epoch  70 | 1000/1106 batches | lr 20.0000 | ms/batch 113.50 | loss  3.72 | ppl    41.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 147.02s | valid loss  4.41 | valid ppl    82.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 | 200/1106 batches | lr 20.0000 | ms/batch 122.05 | loss  3.73 | ppl    41.75\n",
      "| epoch  71 | 400/1106 batches | lr 20.0000 | ms/batch 116.86 | loss  3.62 | ppl    37.37\n",
      "| epoch  71 | 600/1106 batches | lr 20.0000 | ms/batch 129.15 | loss  3.65 | ppl    38.48\n",
      "| epoch  71 | 800/1106 batches | lr 20.0000 | ms/batch 119.66 | loss  3.66 | ppl    39.01\n",
      "| epoch  71 | 1000/1106 batches | lr 20.0000 | ms/batch 123.94 | loss  3.72 | ppl    41.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 145.13s | valid loss  4.41 | valid ppl    82.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 | 200/1106 batches | lr 20.0000 | ms/batch 129.25 | loss  3.74 | ppl    42.06\n",
      "| epoch  72 | 400/1106 batches | lr 20.0000 | ms/batch 125.36 | loss  3.63 | ppl    37.86\n",
      "| epoch  72 | 600/1106 batches | lr 20.0000 | ms/batch 130.72 | loss  3.65 | ppl    38.31\n",
      "| epoch  72 | 800/1106 batches | lr 20.0000 | ms/batch 112.44 | loss  3.67 | ppl    39.34\n",
      "| epoch  72 | 1000/1106 batches | lr 20.0000 | ms/batch 119.24 | loss  3.70 | ppl    40.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 144.71s | valid loss  4.40 | valid ppl    81.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 | 200/1106 batches | lr 20.0000 | ms/batch 140.54 | loss  3.72 | ppl    41.32\n",
      "| epoch  73 | 400/1106 batches | lr 20.0000 | ms/batch 125.40 | loss  3.63 | ppl    37.89\n",
      "| epoch  73 | 600/1106 batches | lr 20.0000 | ms/batch 122.05 | loss  3.65 | ppl    38.62\n",
      "| epoch  73 | 800/1106 batches | lr 20.0000 | ms/batch 130.89 | loss  3.68 | ppl    39.62\n",
      "| epoch  73 | 1000/1106 batches | lr 20.0000 | ms/batch 123.26 | loss  3.71 | ppl    40.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 149.38s | valid loss  4.41 | valid ppl    82.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 | 200/1106 batches | lr 20.0000 | ms/batch 132.47 | loss  3.72 | ppl    41.27\n",
      "| epoch  74 | 400/1106 batches | lr 20.0000 | ms/batch 124.10 | loss  3.63 | ppl    37.80\n",
      "| epoch  74 | 600/1106 batches | lr 20.0000 | ms/batch 134.00 | loss  3.65 | ppl    38.30\n",
      "| epoch  74 | 800/1106 batches | lr 20.0000 | ms/batch 127.45 | loss  3.67 | ppl    39.06\n",
      "| epoch  74 | 1000/1106 batches | lr 20.0000 | ms/batch 118.14 | loss  3.69 | ppl    40.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 147.15s | valid loss  4.41 | valid ppl    82.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 | 200/1106 batches | lr 20.0000 | ms/batch 142.01 | loss  3.71 | ppl    41.05\n",
      "| epoch  75 | 400/1106 batches | lr 20.0000 | ms/batch 123.48 | loss  3.63 | ppl    37.73\n",
      "| epoch  75 | 600/1106 batches | lr 20.0000 | ms/batch 135.11 | loss  3.63 | ppl    37.72\n",
      "| epoch  75 | 800/1106 batches | lr 20.0000 | ms/batch 134.48 | loss  3.66 | ppl    38.94\n",
      "| epoch  75 | 1000/1106 batches | lr 20.0000 | ms/batch 121.33 | loss  3.68 | ppl    39.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 152.37s | valid loss  4.42 | valid ppl    83.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 | 200/1106 batches | lr 20.0000 | ms/batch 126.03 | loss  3.72 | ppl    41.19\n",
      "| epoch  76 | 400/1106 batches | lr 20.0000 | ms/batch 132.11 | loss  3.62 | ppl    37.46\n",
      "| epoch  76 | 600/1106 batches | lr 20.0000 | ms/batch 125.02 | loss  3.63 | ppl    37.81\n",
      "| epoch  76 | 800/1106 batches | lr 20.0000 | ms/batch 136.28 | loss  3.66 | ppl    39.05\n",
      "| epoch  76 | 1000/1106 batches | lr 20.0000 | ms/batch 134.71 | loss  3.70 | ppl    40.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 152.84s | valid loss  4.42 | valid ppl    82.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 | 200/1106 batches | lr 20.0000 | ms/batch 141.02 | loss  3.72 | ppl    41.18\n",
      "| epoch  77 | 400/1106 batches | lr 20.0000 | ms/batch 123.40 | loss  3.61 | ppl    37.05\n",
      "| epoch  77 | 600/1106 batches | lr 20.0000 | ms/batch 123.70 | loss  3.63 | ppl    37.72\n",
      "| epoch  77 | 800/1106 batches | lr 20.0000 | ms/batch 115.63 | loss  3.65 | ppl    38.54\n",
      "| epoch  77 | 1000/1106 batches | lr 20.0000 | ms/batch 121.05 | loss  3.69 | ppl    40.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 145.11s | valid loss  4.41 | valid ppl    82.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 | 200/1106 batches | lr 20.0000 | ms/batch 132.16 | loss  3.71 | ppl    40.82\n",
      "| epoch  78 | 400/1106 batches | lr 20.0000 | ms/batch 128.50 | loss  3.61 | ppl    36.80\n",
      "| epoch  78 | 600/1106 batches | lr 20.0000 | ms/batch 127.38 | loss  3.63 | ppl    37.70\n",
      "| epoch  78 | 800/1106 batches | lr 20.0000 | ms/batch 121.67 | loss  3.65 | ppl    38.34\n",
      "| epoch  78 | 1000/1106 batches | lr 20.0000 | ms/batch 127.48 | loss  3.68 | ppl    39.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 146.81s | valid loss  4.42 | valid ppl    83.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 | 200/1106 batches | lr 20.0000 | ms/batch 120.06 | loss  3.70 | ppl    40.36\n",
      "| epoch  79 | 400/1106 batches | lr 20.0000 | ms/batch 129.47 | loss  3.60 | ppl    36.45\n",
      "| epoch  79 | 600/1106 batches | lr 20.0000 | ms/batch 120.38 | loss  3.63 | ppl    37.60\n",
      "| epoch  79 | 800/1106 batches | lr 20.0000 | ms/batch 119.32 | loss  3.65 | ppl    38.32\n",
      "| epoch  79 | 1000/1106 batches | lr 20.0000 | ms/batch 119.41 | loss  3.68 | ppl    39.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 140.32s | valid loss  4.41 | valid ppl    82.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 | 200/1106 batches | lr 20.0000 | ms/batch 131.38 | loss  3.70 | ppl    40.48\n",
      "| epoch  80 | 400/1106 batches | lr 20.0000 | ms/batch 133.31 | loss  3.61 | ppl    36.98\n",
      "| epoch  80 | 600/1106 batches | lr 20.0000 | ms/batch 121.81 | loss  3.61 | ppl    37.01\n",
      "| epoch  80 | 800/1106 batches | lr 20.0000 | ms/batch 124.81 | loss  3.63 | ppl    37.84\n",
      "| epoch  80 | 1000/1106 batches | lr 20.0000 | ms/batch 118.48 | loss  3.68 | ppl    39.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 146.35s | valid loss  4.40 | valid ppl    81.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 | 200/1106 batches | lr 20.0000 | ms/batch 128.07 | loss  3.70 | ppl    40.52\n",
      "| epoch  81 | 400/1106 batches | lr 20.0000 | ms/batch 122.25 | loss  3.61 | ppl    36.81\n",
      "| epoch  81 | 600/1106 batches | lr 20.0000 | ms/batch 126.76 | loss  3.60 | ppl    36.60\n",
      "| epoch  81 | 800/1106 batches | lr 20.0000 | ms/batch 126.43 | loss  3.64 | ppl    38.05\n",
      "| epoch  81 | 1000/1106 batches | lr 20.0000 | ms/batch 124.61 | loss  3.67 | ppl    39.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 146.14s | valid loss  4.41 | valid ppl    82.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 | 200/1106 batches | lr 20.0000 | ms/batch 120.01 | loss  3.70 | ppl    40.59\n",
      "| epoch  82 | 400/1106 batches | lr 20.0000 | ms/batch 130.99 | loss  3.60 | ppl    36.52\n",
      "| epoch  82 | 600/1106 batches | lr 20.0000 | ms/batch 136.28 | loss  3.61 | ppl    36.80\n",
      "| epoch  82 | 800/1106 batches | lr 20.0000 | ms/batch 114.88 | loss  3.64 | ppl    38.17\n",
      "| epoch  82 | 1000/1106 batches | lr 20.0000 | ms/batch 127.85 | loss  3.66 | ppl    38.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 146.69s | valid loss  4.40 | valid ppl    81.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 | 200/1106 batches | lr 20.0000 | ms/batch 123.24 | loss  3.69 | ppl    39.88\n",
      "| epoch  83 | 400/1106 batches | lr 20.0000 | ms/batch 129.74 | loss  3.61 | ppl    36.82\n",
      "| epoch  83 | 600/1106 batches | lr 20.0000 | ms/batch 130.38 | loss  3.59 | ppl    36.35\n",
      "| epoch  83 | 800/1106 batches | lr 20.0000 | ms/batch 122.93 | loss  3.63 | ppl    37.55\n",
      "| epoch  83 | 1000/1106 batches | lr 20.0000 | ms/batch 124.29 | loss  3.66 | ppl    38.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 147.67s | valid loss  4.41 | valid ppl    81.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 | 200/1106 batches | lr 20.0000 | ms/batch 126.38 | loss  3.69 | ppl    40.07\n",
      "| epoch  84 | 400/1106 batches | lr 20.0000 | ms/batch 117.53 | loss  3.58 | ppl    35.78\n",
      "| epoch  84 | 600/1106 batches | lr 20.0000 | ms/batch 122.94 | loss  3.60 | ppl    36.77\n",
      "| epoch  84 | 800/1106 batches | lr 20.0000 | ms/batch 127.84 | loss  3.62 | ppl    37.52\n",
      "| epoch  84 | 1000/1106 batches | lr 20.0000 | ms/batch 128.48 | loss  3.66 | ppl    38.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 146.54s | valid loss  4.41 | valid ppl    82.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 | 200/1106 batches | lr 20.0000 | ms/batch 134.43 | loss  3.69 | ppl    39.95\n",
      "| epoch  85 | 400/1106 batches | lr 20.0000 | ms/batch 127.87 | loss  3.57 | ppl    35.39\n",
      "| epoch  85 | 600/1106 batches | lr 20.0000 | ms/batch 134.06 | loss  3.62 | ppl    37.28\n",
      "| epoch  85 | 800/1106 batches | lr 20.0000 | ms/batch 126.65 | loss  3.62 | ppl    37.25\n",
      "| epoch  85 | 1000/1106 batches | lr 20.0000 | ms/batch 132.48 | loss  3.67 | ppl    39.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 153.99s | valid loss  4.41 | valid ppl    82.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 | 200/1106 batches | lr 20.0000 | ms/batch 130.58 | loss  3.68 | ppl    39.80\n",
      "| epoch  86 | 400/1106 batches | lr 20.0000 | ms/batch 121.78 | loss  3.60 | ppl    36.51\n",
      "| epoch  86 | 600/1106 batches | lr 20.0000 | ms/batch 127.16 | loss  3.62 | ppl    37.17\n",
      "| epoch  86 | 800/1106 batches | lr 20.0000 | ms/batch 130.50 | loss  3.63 | ppl    37.59\n",
      "| epoch  86 | 1000/1106 batches | lr 20.0000 | ms/batch 129.20 | loss  3.65 | ppl    38.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 149.84s | valid loss  4.41 | valid ppl    82.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 | 200/1106 batches | lr 20.0000 | ms/batch 133.84 | loss  3.68 | ppl    39.47\n",
      "| epoch  87 | 400/1106 batches | lr 20.0000 | ms/batch 135.43 | loss  3.58 | ppl    35.86\n",
      "| epoch  87 | 600/1106 batches | lr 20.0000 | ms/batch 131.50 | loss  3.60 | ppl    36.50\n",
      "| epoch  87 | 800/1106 batches | lr 20.0000 | ms/batch 124.61 | loss  3.61 | ppl    36.93\n",
      "| epoch  87 | 1000/1106 batches | lr 20.0000 | ms/batch 115.18 | loss  3.65 | ppl    38.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 148.67s | valid loss  4.42 | valid ppl    83.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 | 200/1106 batches | lr 20.0000 | ms/batch 123.54 | loss  3.67 | ppl    39.28\n",
      "| epoch  88 | 400/1106 batches | lr 20.0000 | ms/batch 130.97 | loss  3.57 | ppl    35.66\n",
      "| epoch  88 | 600/1106 batches | lr 20.0000 | ms/batch 125.29 | loss  3.58 | ppl    35.90\n",
      "| epoch  88 | 800/1106 batches | lr 20.0000 | ms/batch 123.90 | loss  3.61 | ppl    36.96\n",
      "| epoch  88 | 1000/1106 batches | lr 20.0000 | ms/batch 133.11 | loss  3.65 | ppl    38.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 150.95s | valid loss  4.41 | valid ppl    82.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 | 200/1106 batches | lr 20.0000 | ms/batch 137.16 | loss  3.68 | ppl    39.77\n",
      "| epoch  89 | 400/1106 batches | lr 20.0000 | ms/batch 123.35 | loss  3.57 | ppl    35.37\n",
      "| epoch  89 | 600/1106 batches | lr 20.0000 | ms/batch 132.57 | loss  3.59 | ppl    36.39\n",
      "| epoch  89 | 800/1106 batches | lr 20.0000 | ms/batch 128.10 | loss  3.61 | ppl    37.01\n",
      "| epoch  89 | 1000/1106 batches | lr 20.0000 | ms/batch 135.04 | loss  3.64 | ppl    37.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 152.95s | valid loss  4.41 | valid ppl    82.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 | 200/1106 batches | lr 20.0000 | ms/batch 121.21 | loss  3.68 | ppl    39.61\n",
      "| epoch  90 | 400/1106 batches | lr 20.0000 | ms/batch 113.97 | loss  3.57 | ppl    35.36\n",
      "| epoch  90 | 600/1106 batches | lr 20.0000 | ms/batch 126.88 | loss  3.60 | ppl    36.52\n",
      "| epoch  90 | 800/1106 batches | lr 20.0000 | ms/batch 126.00 | loss  3.61 | ppl    36.87\n",
      "| epoch  90 | 1000/1106 batches | lr 20.0000 | ms/batch 113.88 | loss  3.65 | ppl    38.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 140.26s | valid loss  4.42 | valid ppl    82.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 | 200/1106 batches | lr 20.0000 | ms/batch 132.77 | loss  3.66 | ppl    38.93\n",
      "| epoch  91 | 400/1106 batches | lr 20.0000 | ms/batch 122.52 | loss  3.56 | ppl    35.17\n",
      "| epoch  91 | 600/1106 batches | lr 20.0000 | ms/batch 137.29 | loss  3.58 | ppl    35.77\n",
      "| epoch  91 | 800/1106 batches | lr 20.0000 | ms/batch 128.34 | loss  3.60 | ppl    36.58\n",
      "| epoch  91 | 1000/1106 batches | lr 20.0000 | ms/batch 124.66 | loss  3.65 | ppl    38.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 150.07s | valid loss  4.42 | valid ppl    82.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 | 200/1106 batches | lr 20.0000 | ms/batch 136.27 | loss  3.66 | ppl    38.80\n",
      "| epoch  92 | 400/1106 batches | lr 20.0000 | ms/batch 125.27 | loss  3.57 | ppl    35.42\n",
      "| epoch  92 | 600/1106 batches | lr 20.0000 | ms/batch 121.82 | loss  3.60 | ppl    36.68\n",
      "| epoch  92 | 800/1106 batches | lr 20.0000 | ms/batch 126.51 | loss  3.60 | ppl    36.59\n",
      "| epoch  92 | 1000/1106 batches | lr 20.0000 | ms/batch 126.97 | loss  3.64 | ppl    38.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 148.25s | valid loss  4.41 | valid ppl    81.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 | 200/1106 batches | lr 20.0000 | ms/batch 127.13 | loss  3.67 | ppl    39.26\n",
      "| epoch  93 | 400/1106 batches | lr 20.0000 | ms/batch 120.96 | loss  3.56 | ppl    35.21\n",
      "| epoch  93 | 600/1106 batches | lr 20.0000 | ms/batch 130.33 | loss  3.58 | ppl    35.85\n",
      "| epoch  93 | 800/1106 batches | lr 20.0000 | ms/batch 129.46 | loss  3.61 | ppl    36.81\n",
      "| epoch  93 | 1000/1106 batches | lr 20.0000 | ms/batch 113.24 | loss  3.63 | ppl    37.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 146.19s | valid loss  4.42 | valid ppl    83.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 | 200/1106 batches | lr 20.0000 | ms/batch 122.78 | loss  3.67 | ppl    39.25\n",
      "| epoch  94 | 400/1106 batches | lr 20.0000 | ms/batch 128.20 | loss  3.56 | ppl    35.25\n",
      "| epoch  94 | 600/1106 batches | lr 20.0000 | ms/batch 115.13 | loss  3.58 | ppl    35.72\n",
      "| epoch  94 | 800/1106 batches | lr 20.0000 | ms/batch 133.70 | loss  3.59 | ppl    36.33\n",
      "| epoch  94 | 1000/1106 batches | lr 20.0000 | ms/batch 135.58 | loss  3.64 | ppl    37.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 148.51s | valid loss  4.41 | valid ppl    82.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 | 200/1106 batches | lr 20.0000 | ms/batch 116.12 | loss  3.66 | ppl    38.69\n",
      "| epoch  95 | 400/1106 batches | lr 20.0000 | ms/batch 126.42 | loss  3.56 | ppl    35.23\n",
      "| epoch  95 | 600/1106 batches | lr 20.0000 | ms/batch 128.19 | loss  3.57 | ppl    35.36\n",
      "| epoch  95 | 800/1106 batches | lr 20.0000 | ms/batch 131.80 | loss  3.60 | ppl    36.73\n",
      "| epoch  95 | 1000/1106 batches | lr 20.0000 | ms/batch 126.48 | loss  3.62 | ppl    37.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 145.72s | valid loss  4.41 | valid ppl    82.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 | 200/1106 batches | lr 20.0000 | ms/batch 137.61 | loss  3.65 | ppl    38.66\n",
      "| epoch  96 | 400/1106 batches | lr 20.0000 | ms/batch 121.04 | loss  3.55 | ppl    34.78\n",
      "| epoch  96 | 600/1106 batches | lr 20.0000 | ms/batch 126.92 | loss  3.57 | ppl    35.37\n",
      "| epoch  96 | 800/1106 batches | lr 20.0000 | ms/batch 129.09 | loss  3.60 | ppl    36.61\n",
      "| epoch  96 | 1000/1106 batches | lr 20.0000 | ms/batch 129.52 | loss  3.62 | ppl    37.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 149.03s | valid loss  4.41 | valid ppl    82.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 | 200/1106 batches | lr 20.0000 | ms/batch 121.73 | loss  3.65 | ppl    38.33\n",
      "| epoch  97 | 400/1106 batches | lr 20.0000 | ms/batch 123.02 | loss  3.54 | ppl    34.60\n",
      "| epoch  97 | 600/1106 batches | lr 20.0000 | ms/batch 133.22 | loss  3.57 | ppl    35.40\n",
      "| epoch  97 | 800/1106 batches | lr 20.0000 | ms/batch 128.46 | loss  3.57 | ppl    35.39\n",
      "| epoch  97 | 1000/1106 batches | lr 20.0000 | ms/batch 116.40 | loss  3.63 | ppl    37.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 147.27s | valid loss  4.40 | valid ppl    81.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 | 200/1106 batches | lr 20.0000 | ms/batch 133.52 | loss  3.65 | ppl    38.39\n",
      "| epoch  98 | 400/1106 batches | lr 20.0000 | ms/batch 128.07 | loss  3.54 | ppl    34.53\n",
      "| epoch  98 | 600/1106 batches | lr 20.0000 | ms/batch 129.42 | loss  3.56 | ppl    35.29\n",
      "| epoch  98 | 800/1106 batches | lr 20.0000 | ms/batch 123.91 | loss  3.59 | ppl    36.06\n",
      "| epoch  98 | 1000/1106 batches | lr 20.0000 | ms/batch 118.92 | loss  3.62 | ppl    37.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 148.05s | valid loss  4.41 | valid ppl    82.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 | 200/1106 batches | lr 20.0000 | ms/batch 122.01 | loss  3.65 | ppl    38.52\n",
      "| epoch  99 | 400/1106 batches | lr 20.0000 | ms/batch 127.94 | loss  3.54 | ppl    34.58\n",
      "| epoch  99 | 600/1106 batches | lr 20.0000 | ms/batch 126.55 | loss  3.57 | ppl    35.44\n",
      "| epoch  99 | 800/1106 batches | lr 20.0000 | ms/batch 123.95 | loss  3.59 | ppl    36.09\n",
      "| epoch  99 | 1000/1106 batches | lr 20.0000 | ms/batch 127.68 | loss  3.61 | ppl    36.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 146.23s | valid loss  4.41 | valid ppl    82.22\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt)\n",
    "        val_loss = evaluate(val_data, model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        if epoch % 1 == 0:\n",
    "            tp.send_text('MOSsbrsample\\nEpoch %d | loss: %.2f | ppl: %.2f' % (epoch, val_loss, math.exp(val_loss)))\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_words = 150\n",
    "generated = []\n",
    "temperature = 1.0 # highest temperature increase diversity\n",
    "log_interval = 50 # \n",
    "model.eval()\n",
    "hidden = model.init_hidden(1)\n",
    "input = Variable(torch.rand(1, 1).mul(ntokens).long().cuda(), volatile=True)\n",
    "\n",
    "sent = []\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden, return_prob=True)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    sent.append(word) \n",
    "    if i % 20 == 19:\n",
    "        generated.append(sent)\n",
    "        sent = []\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print('| Generated {}/{} words'.format(i, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
