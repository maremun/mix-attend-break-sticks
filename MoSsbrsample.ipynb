{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out Gauss-Logit parametrization from here https://arxiv.org/pdf/1605.06197.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rsample() for beta distribution is not implemented for cuda...\n",
    "it's extremely slow on cpu. like 4 seconds on batch. also need to swtich off cuda everywhere to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Use token from .telepythrc.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import modelsbrsample as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint\n",
    "from telepyth import TelepythClient\n",
    "tp = TelepythClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "is_cuda = False\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24300635\n",
      "Model total parameters: 24300635\n"
     ]
    }
   ],
   "source": [
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model.cuda()\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (lockdrop): LockedDropout()\n",
       "  (encoder): Embedding(10000, 280)\n",
       "  (rnns): ModuleList(\n",
       "    (0): WeightDrop(\n",
       "      (module): LSTM(280, 960)\n",
       "    )\n",
       "    (1): WeightDrop(\n",
       "      (module): LSTM(960, 960)\n",
       "    )\n",
       "    (2): WeightDrop(\n",
       "      (module): LSTM(960, 620)\n",
       "    )\n",
       "  )\n",
       "  (head): MoShead(\n",
       "    (lockdrop): LockedDropout()\n",
       "    (reduce): Linear(in_features=620, out_features=15, bias=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (latent): Sequential(\n",
       "      (0): Linear(in_features=620, out_features=4200, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "    (decoder): Linear(in_features=280, out_features=10000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = model(cur_data, hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "            loss = raw_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180527-195421\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 4059.76 | loss   nan | ppl      nan\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt)\n",
    "        val_loss = evaluate(val_data, model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        if epoch % 1 == 0:\n",
    "            tp.send_text('MOSsbrsample\\nEpoch %d | loss: %.2f | ppl: %.2f' % (epoch, val_loss, math.exp(val_loss)))\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_words = 150\n",
    "generated = []\n",
    "temperature = 1.0 # highest temperature increase diversity\n",
    "log_interval = 50 # \n",
    "model.eval()\n",
    "hidden = model.init_hidden(1)\n",
    "input = Variable(torch.rand(1, 1).mul(ntokens).long().cuda(), volatile=True)\n",
    "\n",
    "sent = []\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden, return_prob=True)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    sent.append(word) \n",
    "    if i % 20 == 19:\n",
    "        generated.append(sent)\n",
    "        sent = []\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print('| Generated {}/{} words'.format(i, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
