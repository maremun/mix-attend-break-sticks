{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out Gauss-Logit parametrization from here https://arxiv.org/pdf/1605.06197.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import modelsbtransform as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24300014\n",
      "Model total parameters: 24300014\n"
     ]
    }
   ],
   "source": [
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = model(cur_data.cuda(), hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "            loss = raw_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180527-115949\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py:491: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 84.18 | loss  6.98 | ppl  1071.13\n",
      "| epoch   1 | 400/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  6.54 | ppl   695.05\n",
      "| epoch   1 | 600/1106 batches | lr 20.0000 | ms/batch 85.89 | loss  6.33 | ppl   560.43\n",
      "| epoch   1 | 800/1106 batches | lr 20.0000 | ms/batch 88.65 | loss  6.20 | ppl   494.48\n",
      "| epoch   1 | 1000/1106 batches | lr 20.0000 | ms/batch 87.98 | loss  6.05 | ppl   425.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 101.62s | valid loss  5.82 | valid ppl   336.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   2 | 200/1106 batches | lr 20.0000 | ms/batch 86.27 | loss  5.85 | ppl   346.51\n",
      "| epoch   2 | 400/1106 batches | lr 20.0000 | ms/batch 86.22 | loss  5.68 | ppl   294.17\n",
      "| epoch   2 | 600/1106 batches | lr 20.0000 | ms/batch 87.72 | loss  5.58 | ppl   265.03\n",
      "| epoch   2 | 800/1106 batches | lr 20.0000 | ms/batch 87.43 | loss  5.55 | ppl   258.52\n",
      "| epoch   2 | 1000/1106 batches | lr 20.0000 | ms/batch 85.56 | loss  5.50 | ppl   243.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 101.56s | valid loss  5.38 | valid ppl   217.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 | 200/1106 batches | lr 20.0000 | ms/batch 85.77 | loss  5.41 | ppl   224.68\n",
      "| epoch   3 | 400/1106 batches | lr 20.0000 | ms/batch 84.20 | loss  5.30 | ppl   199.89\n",
      "| epoch   3 | 600/1106 batches | lr 20.0000 | ms/batch 86.98 | loss  5.24 | ppl   188.10\n",
      "| epoch   3 | 800/1106 batches | lr 20.0000 | ms/batch 87.26 | loss  5.26 | ppl   192.89\n",
      "| epoch   3 | 1000/1106 batches | lr 20.0000 | ms/batch 86.26 | loss  5.23 | ppl   187.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 100.12s | valid loss  5.13 | valid ppl   169.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 | 200/1106 batches | lr 20.0000 | ms/batch 86.85 | loss  5.21 | ppl   183.23\n",
      "| epoch   4 | 400/1106 batches | lr 20.0000 | ms/batch 86.43 | loss  5.10 | ppl   163.35\n",
      "| epoch   4 | 600/1106 batches | lr 20.0000 | ms/batch 86.23 | loss  5.05 | ppl   155.34\n",
      "| epoch   4 | 800/1106 batches | lr 20.0000 | ms/batch 85.49 | loss  5.07 | ppl   159.32\n",
      "| epoch   4 | 1000/1106 batches | lr 20.0000 | ms/batch 84.54 | loss  5.07 | ppl   158.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 100.73s | valid loss  4.99 | valid ppl   146.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 | 200/1106 batches | lr 20.0000 | ms/batch 86.93 | loss  5.05 | ppl   155.82\n",
      "| epoch   5 | 400/1106 batches | lr 20.0000 | ms/batch 87.57 | loss  4.93 | ppl   138.83\n",
      "| epoch   5 | 600/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  4.90 | ppl   134.95\n",
      "| epoch   5 | 800/1106 batches | lr 20.0000 | ms/batch 87.02 | loss  4.93 | ppl   138.12\n",
      "| epoch   5 | 1000/1106 batches | lr 20.0000 | ms/batch 86.89 | loss  4.94 | ppl   139.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 101.23s | valid loss  4.88 | valid ppl   131.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 | 200/1106 batches | lr 20.0000 | ms/batch 86.41 | loss  4.93 | ppl   138.81\n",
      "| epoch   6 | 400/1106 batches | lr 20.0000 | ms/batch 84.68 | loss  4.81 | ppl   122.79\n",
      "| epoch   6 | 600/1106 batches | lr 20.0000 | ms/batch 86.17 | loss  4.80 | ppl   121.16\n",
      "| epoch   6 | 800/1106 batches | lr 20.0000 | ms/batch 86.78 | loss  4.82 | ppl   123.44\n",
      "| epoch   6 | 1000/1106 batches | lr 20.0000 | ms/batch 84.52 | loss  4.83 | ppl   124.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 100.24s | valid loss  4.82 | valid ppl   123.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 | 200/1106 batches | lr 20.0000 | ms/batch 87.14 | loss  4.83 | ppl   125.70\n",
      "| epoch   7 | 400/1106 batches | lr 20.0000 | ms/batch 84.83 | loss  4.70 | ppl   109.98\n",
      "| epoch   7 | 600/1106 batches | lr 20.0000 | ms/batch 85.54 | loss  4.70 | ppl   109.61\n",
      "| epoch   7 | 800/1106 batches | lr 20.0000 | ms/batch 85.22 | loss  4.72 | ppl   111.91\n",
      "| epoch   7 | 1000/1106 batches | lr 20.0000 | ms/batch 86.10 | loss  4.76 | ppl   116.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 101.07s | valid loss  4.74 | valid ppl   114.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 | 200/1106 batches | lr 20.0000 | ms/batch 88.11 | loss  4.75 | ppl   115.78\n",
      "| epoch   8 | 400/1106 batches | lr 20.0000 | ms/batch 88.94 | loss  4.64 | ppl   103.18\n",
      "| epoch   8 | 600/1106 batches | lr 20.0000 | ms/batch 85.15 | loss  4.62 | ppl   101.25\n",
      "| epoch   8 | 800/1106 batches | lr 20.0000 | ms/batch 85.50 | loss  4.66 | ppl   105.18\n",
      "| epoch   8 | 1000/1106 batches | lr 20.0000 | ms/batch 87.53 | loss  4.67 | ppl   107.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 101.43s | valid loss  4.72 | valid ppl   112.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 | 200/1106 batches | lr 20.0000 | ms/batch 85.96 | loss  4.68 | ppl   107.81\n",
      "| epoch   9 | 400/1106 batches | lr 20.0000 | ms/batch 85.84 | loss  4.56 | ppl    95.45\n",
      "| epoch   9 | 600/1106 batches | lr 20.0000 | ms/batch 85.54 | loss  4.56 | ppl    95.28\n",
      "| epoch   9 | 800/1106 batches | lr 20.0000 | ms/batch 83.83 | loss  4.59 | ppl    98.06\n",
      "| epoch   9 | 1000/1106 batches | lr 20.0000 | ms/batch 88.30 | loss  4.60 | ppl    99.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 100.67s | valid loss  4.67 | valid ppl   106.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 | 200/1106 batches | lr 20.0000 | ms/batch 85.64 | loss  4.62 | ppl   101.08\n",
      "| epoch  10 | 400/1106 batches | lr 20.0000 | ms/batch 86.77 | loss  4.49 | ppl    89.33\n",
      "| epoch  10 | 600/1106 batches | lr 20.0000 | ms/batch 86.65 | loss  4.50 | ppl    89.57\n",
      "| epoch  10 | 800/1106 batches | lr 20.0000 | ms/batch 86.21 | loss  4.52 | ppl    91.84\n",
      "| epoch  10 | 1000/1106 batches | lr 20.0000 | ms/batch 85.51 | loss  4.55 | ppl    94.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 101.22s | valid loss  4.63 | valid ppl   102.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 | 200/1106 batches | lr 20.0000 | ms/batch 89.11 | loss  4.57 | ppl    96.73\n",
      "| epoch  11 | 400/1106 batches | lr 20.0000 | ms/batch 86.71 | loss  4.44 | ppl    85.02\n",
      "| epoch  11 | 600/1106 batches | lr 20.0000 | ms/batch 84.78 | loss  4.44 | ppl    84.64\n",
      "| epoch  11 | 800/1106 batches | lr 20.0000 | ms/batch 84.94 | loss  4.49 | ppl    88.75\n",
      "| epoch  11 | 1000/1106 batches | lr 20.0000 | ms/batch 85.28 | loss  4.51 | ppl    90.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 100.49s | valid loss  4.61 | valid ppl   100.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 | 200/1106 batches | lr 20.0000 | ms/batch 84.38 | loss  4.52 | ppl    92.13\n",
      "| epoch  12 | 400/1106 batches | lr 20.0000 | ms/batch 86.30 | loss  4.40 | ppl    81.78\n",
      "| epoch  12 | 600/1106 batches | lr 20.0000 | ms/batch 87.32 | loss  4.40 | ppl    81.46\n",
      "| epoch  12 | 800/1106 batches | lr 20.0000 | ms/batch 86.23 | loss  4.42 | ppl    83.48\n",
      "| epoch  12 | 1000/1106 batches | lr 20.0000 | ms/batch 87.42 | loss  4.46 | ppl    86.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 101.16s | valid loss  4.59 | valid ppl    98.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 | 200/1106 batches | lr 20.0000 | ms/batch 88.52 | loss  4.47 | ppl    87.08\n",
      "| epoch  13 | 400/1106 batches | lr 20.0000 | ms/batch 87.72 | loss  4.34 | ppl    76.87\n",
      "| epoch  13 | 600/1106 batches | lr 20.0000 | ms/batch 86.39 | loss  4.36 | ppl    77.99\n",
      "| epoch  13 | 800/1106 batches | lr 20.0000 | ms/batch 86.86 | loss  4.39 | ppl    80.43\n",
      "| epoch  13 | 1000/1106 batches | lr 20.0000 | ms/batch 86.67 | loss  4.41 | ppl    82.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 100.89s | valid loss  4.58 | valid ppl    97.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 | 200/1106 batches | lr 20.0000 | ms/batch 85.97 | loss  4.43 | ppl    84.32\n",
      "| epoch  14 | 400/1106 batches | lr 20.0000 | ms/batch 85.46 | loss  4.31 | ppl    74.79\n",
      "| epoch  14 | 600/1106 batches | lr 20.0000 | ms/batch 86.65 | loss  4.32 | ppl    75.51\n",
      "| epoch  14 | 800/1106 batches | lr 20.0000 | ms/batch 85.83 | loss  4.34 | ppl    76.79\n",
      "| epoch  14 | 1000/1106 batches | lr 20.0000 | ms/batch 88.76 | loss  4.38 | ppl    80.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 101.14s | valid loss  4.54 | valid ppl    94.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 | 200/1106 batches | lr 20.0000 | ms/batch 86.92 | loss  4.41 | ppl    82.21\n",
      "| epoch  15 | 400/1106 batches | lr 20.0000 | ms/batch 85.73 | loss  4.28 | ppl    71.98\n",
      "| epoch  15 | 600/1106 batches | lr 20.0000 | ms/batch 85.88 | loss  4.29 | ppl    72.67\n",
      "| epoch  15 | 800/1106 batches | lr 20.0000 | ms/batch 86.81 | loss  4.30 | ppl    73.92\n",
      "| epoch  15 | 1000/1106 batches | lr 20.0000 | ms/batch 85.55 | loss  4.35 | ppl    77.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 101.36s | valid loss  4.54 | valid ppl    93.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 | 200/1106 batches | lr 20.0000 | ms/batch 89.16 | loss  4.37 | ppl    79.40\n",
      "| epoch  16 | 400/1106 batches | lr 20.0000 | ms/batch 86.56 | loss  4.24 | ppl    69.15\n",
      "| epoch  16 | 600/1106 batches | lr 20.0000 | ms/batch 86.60 | loss  4.27 | ppl    71.48\n",
      "| epoch  16 | 800/1106 batches | lr 20.0000 | ms/batch 88.73 | loss  4.30 | ppl    73.66\n",
      "| epoch  16 | 1000/1106 batches | lr 20.0000 | ms/batch 88.33 | loss  4.31 | ppl    74.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 102.14s | valid loss  4.53 | valid ppl    92.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 | 200/1106 batches | lr 20.0000 | ms/batch 84.65 | loss  4.34 | ppl    76.38\n",
      "| epoch  17 | 400/1106 batches | lr 20.0000 | ms/batch 85.17 | loss  4.22 | ppl    67.71\n",
      "| epoch  17 | 600/1106 batches | lr 20.0000 | ms/batch 86.26 | loss  4.23 | ppl    68.78\n",
      "| epoch  17 | 800/1106 batches | lr 20.0000 | ms/batch 86.43 | loss  4.25 | ppl    70.12\n",
      "| epoch  17 | 1000/1106 batches | lr 20.0000 | ms/batch 86.16 | loss  4.29 | ppl    72.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 100.71s | valid loss  4.51 | valid ppl    90.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 | 200/1106 batches | lr 20.0000 | ms/batch 87.15 | loss  4.31 | ppl    74.62\n",
      "| epoch  18 | 400/1106 batches | lr 20.0000 | ms/batch 83.54 | loss  4.18 | ppl    65.61\n",
      "| epoch  18 | 600/1106 batches | lr 20.0000 | ms/batch 87.06 | loss  4.20 | ppl    66.57\n",
      "| epoch  18 | 800/1106 batches | lr 20.0000 | ms/batch 87.42 | loss  4.22 | ppl    67.94\n",
      "| epoch  18 | 1000/1106 batches | lr 20.0000 | ms/batch 85.28 | loss  4.27 | ppl    71.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 101.03s | valid loss  4.50 | valid ppl    90.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 | 200/1106 batches | lr 20.0000 | ms/batch 88.62 | loss  4.29 | ppl    73.03\n",
      "| epoch  19 | 400/1106 batches | lr 20.0000 | ms/batch 85.61 | loss  4.16 | ppl    63.93\n",
      "| epoch  19 | 600/1106 batches | lr 20.0000 | ms/batch 87.24 | loss  4.18 | ppl    65.56\n",
      "| epoch  19 | 800/1106 batches | lr 20.0000 | ms/batch 87.26 | loss  4.21 | ppl    67.02\n",
      "| epoch  19 | 1000/1106 batches | lr 20.0000 | ms/batch 86.17 | loss  4.23 | ppl    68.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 101.30s | valid loss  4.49 | valid ppl    89.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  20 | 200/1106 batches | lr 20.0000 | ms/batch 85.39 | loss  4.25 | ppl    69.89\n",
      "| epoch  20 | 400/1106 batches | lr 20.0000 | ms/batch 86.35 | loss  4.14 | ppl    63.07\n",
      "| epoch  20 | 600/1106 batches | lr 20.0000 | ms/batch 85.24 | loss  4.16 | ppl    63.78\n",
      "| epoch  20 | 800/1106 batches | lr 20.0000 | ms/batch 86.35 | loss  4.18 | ppl    65.50\n",
      "| epoch  20 | 1000/1106 batches | lr 20.0000 | ms/batch 85.91 | loss  4.20 | ppl    67.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 100.13s | valid loss  4.48 | valid ppl    88.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  21 | 200/1106 batches | lr 20.0000 | ms/batch 88.99 | loss  4.23 | ppl    69.00\n",
      "| epoch  21 | 400/1106 batches | lr 20.0000 | ms/batch 82.92 | loss  4.11 | ppl    60.82\n",
      "| epoch  21 | 600/1106 batches | lr 20.0000 | ms/batch 86.61 | loss  4.14 | ppl    62.54\n",
      "| epoch  21 | 800/1106 batches | lr 20.0000 | ms/batch 85.51 | loss  4.15 | ppl    63.73\n",
      "| epoch  21 | 1000/1106 batches | lr 20.0000 | ms/batch 86.59 | loss  4.20 | ppl    66.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 100.72s | valid loss  4.47 | valid ppl    86.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 | 200/1106 batches | lr 20.0000 | ms/batch 89.57 | loss  4.21 | ppl    67.36\n",
      "| epoch  22 | 400/1106 batches | lr 20.0000 | ms/batch 86.83 | loss  4.10 | ppl    60.46\n",
      "| epoch  22 | 600/1106 batches | lr 20.0000 | ms/batch 87.03 | loss  4.11 | ppl    61.06\n",
      "| epoch  22 | 800/1106 batches | lr 20.0000 | ms/batch 87.34 | loss  4.14 | ppl    62.65\n",
      "| epoch  22 | 1000/1106 batches | lr 20.0000 | ms/batch 84.40 | loss  4.17 | ppl    64.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 101.66s | valid loss  4.48 | valid ppl    87.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 200/1106 batches | lr 20.0000 | ms/batch 85.79 | loss  4.18 | ppl    65.69\n",
      "| epoch  23 | 400/1106 batches | lr 20.0000 | ms/batch 85.12 | loss  4.08 | ppl    58.95\n",
      "| epoch  23 | 600/1106 batches | lr 20.0000 | ms/batch 83.08 | loss  4.09 | ppl    59.90\n",
      "| epoch  23 | 800/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  4.11 | ppl    61.16\n",
      "| epoch  23 | 1000/1106 batches | lr 20.0000 | ms/batch 84.65 | loss  4.17 | ppl    64.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 100.51s | valid loss  4.46 | valid ppl    86.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  24 | 200/1106 batches | lr 20.0000 | ms/batch 87.89 | loss  4.17 | ppl    64.51\n",
      "| epoch  24 | 400/1106 batches | lr 20.0000 | ms/batch 85.41 | loss  4.06 | ppl    57.84\n",
      "| epoch  24 | 600/1106 batches | lr 20.0000 | ms/batch 86.33 | loss  4.07 | ppl    58.50\n",
      "| epoch  24 | 800/1106 batches | lr 20.0000 | ms/batch 86.85 | loss  4.10 | ppl    60.34\n",
      "| epoch  24 | 1000/1106 batches | lr 20.0000 | ms/batch 87.59 | loss  4.13 | ppl    61.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 101.28s | valid loss  4.45 | valid ppl    85.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  25 | 200/1106 batches | lr 20.0000 | ms/batch 88.13 | loss  4.15 | ppl    63.61\n",
      "| epoch  25 | 400/1106 batches | lr 20.0000 | ms/batch 86.25 | loss  4.05 | ppl    57.40\n",
      "| epoch  25 | 600/1106 batches | lr 20.0000 | ms/batch 88.04 | loss  4.06 | ppl    58.11\n",
      "| epoch  25 | 800/1106 batches | lr 20.0000 | ms/batch 87.35 | loss  4.09 | ppl    59.64\n",
      "| epoch  25 | 1000/1106 batches | lr 20.0000 | ms/batch 86.32 | loss  4.11 | ppl    60.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 101.64s | valid loss  4.45 | valid ppl    85.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 | 200/1106 batches | lr 20.0000 | ms/batch 88.31 | loss  4.13 | ppl    62.22\n",
      "| epoch  26 | 400/1106 batches | lr 20.0000 | ms/batch 87.26 | loss  4.03 | ppl    56.10\n",
      "| epoch  26 | 600/1106 batches | lr 20.0000 | ms/batch 86.92 | loss  4.03 | ppl    56.25\n",
      "| epoch  26 | 800/1106 batches | lr 20.0000 | ms/batch 89.44 | loss  4.05 | ppl    57.42\n",
      "| epoch  26 | 1000/1106 batches | lr 20.0000 | ms/batch 89.36 | loss  4.10 | ppl    60.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 101.83s | valid loss  4.44 | valid ppl    84.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  27 | 200/1106 batches | lr 20.0000 | ms/batch 86.24 | loss  4.13 | ppl    62.06\n",
      "| epoch  27 | 400/1106 batches | lr 20.0000 | ms/batch 86.47 | loss  4.01 | ppl    55.14\n",
      "| epoch  27 | 600/1106 batches | lr 20.0000 | ms/batch 86.69 | loss  4.03 | ppl    56.12\n",
      "| epoch  27 | 800/1106 batches | lr 20.0000 | ms/batch 85.12 | loss  4.04 | ppl    57.00\n",
      "| epoch  27 | 1000/1106 batches | lr 20.0000 | ms/batch 85.65 | loss  4.09 | ppl    59.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 100.79s | valid loss  4.44 | valid ppl    85.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 | 200/1106 batches | lr 20.0000 | ms/batch 88.80 | loss  4.11 | ppl    60.76\n",
      "| epoch  28 | 400/1106 batches | lr 20.0000 | ms/batch 85.20 | loss  4.00 | ppl    54.33\n",
      "| epoch  28 | 600/1106 batches | lr 20.0000 | ms/batch 87.24 | loss  4.00 | ppl    54.49\n",
      "| epoch  28 | 800/1106 batches | lr 20.0000 | ms/batch 88.10 | loss  4.04 | ppl    56.75\n",
      "| epoch  28 | 1000/1106 batches | lr 20.0000 | ms/batch 85.43 | loss  4.05 | ppl    57.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 101.15s | valid loss  4.43 | valid ppl    84.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  29 | 200/1106 batches | lr 20.0000 | ms/batch 88.81 | loss  4.09 | ppl    59.59\n",
      "| epoch  29 | 400/1106 batches | lr 20.0000 | ms/batch 88.43 | loss  3.98 | ppl    53.58\n",
      "| epoch  29 | 600/1106 batches | lr 20.0000 | ms/batch 86.62 | loss  3.99 | ppl    54.31\n",
      "| epoch  29 | 800/1106 batches | lr 20.0000 | ms/batch 88.13 | loss  4.02 | ppl    55.62\n",
      "| epoch  29 | 1000/1106 batches | lr 20.0000 | ms/batch 84.65 | loss  4.03 | ppl    56.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 101.08s | valid loss  4.44 | valid ppl    84.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 | 200/1106 batches | lr 20.0000 | ms/batch 86.44 | loss  4.06 | ppl    58.22\n",
      "| epoch  30 | 400/1106 batches | lr 20.0000 | ms/batch 87.22 | loss  3.95 | ppl    52.02\n",
      "| epoch  30 | 600/1106 batches | lr 20.0000 | ms/batch 88.18 | loss  3.98 | ppl    53.42\n",
      "| epoch  30 | 800/1106 batches | lr 20.0000 | ms/batch 84.40 | loss  4.00 | ppl    54.73\n",
      "| epoch  30 | 1000/1106 batches | lr 20.0000 | ms/batch 83.89 | loss  4.03 | ppl    56.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 100.57s | valid loss  4.43 | valid ppl    84.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 | 200/1106 batches | lr 20.0000 | ms/batch 87.50 | loss  4.05 | ppl    57.50\n",
      "| epoch  31 | 400/1106 batches | lr 20.0000 | ms/batch 86.93 | loss  3.96 | ppl    52.24\n",
      "| epoch  31 | 600/1106 batches | lr 20.0000 | ms/batch 86.52 | loss  3.97 | ppl    53.10\n",
      "| epoch  31 | 800/1106 batches | lr 20.0000 | ms/batch 84.63 | loss  4.00 | ppl    54.44\n",
      "| epoch  31 | 1000/1106 batches | lr 20.0000 | ms/batch 86.35 | loss  4.03 | ppl    56.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 100.98s | valid loss  4.43 | valid ppl    84.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  32 | 200/1106 batches | lr 20.0000 | ms/batch 87.18 | loss  4.05 | ppl    57.27\n",
      "| epoch  32 | 400/1106 batches | lr 20.0000 | ms/batch 85.16 | loss  3.93 | ppl    50.78\n",
      "| epoch  32 | 600/1106 batches | lr 20.0000 | ms/batch 88.41 | loss  3.94 | ppl    51.49\n",
      "| epoch  32 | 800/1106 batches | lr 20.0000 | ms/batch 86.73 | loss  3.99 | ppl    53.86\n",
      "| epoch  32 | 1000/1106 batches | lr 20.0000 | ms/batch 86.03 | loss  4.01 | ppl    55.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 101.10s | valid loss  4.42 | valid ppl    83.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  33 | 200/1106 batches | lr 20.0000 | ms/batch 87.38 | loss  4.03 | ppl    56.37\n",
      "| epoch  33 | 400/1106 batches | lr 20.0000 | ms/batch 85.98 | loss  3.92 | ppl    50.58\n",
      "| epoch  33 | 600/1106 batches | lr 20.0000 | ms/batch 85.46 | loss  3.94 | ppl    51.32\n",
      "| epoch  33 | 800/1106 batches | lr 20.0000 | ms/batch 84.45 | loss  3.97 | ppl    53.08\n",
      "| epoch  33 | 1000/1106 batches | lr 20.0000 | ms/batch 86.33 | loss  4.00 | ppl    54.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 101.04s | valid loss  4.41 | valid ppl    82.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  34 | 200/1106 batches | lr 20.0000 | ms/batch 86.53 | loss  4.03 | ppl    56.05\n",
      "| epoch  34 | 400/1106 batches | lr 20.0000 | ms/batch 88.11 | loss  3.91 | ppl    49.87\n",
      "| epoch  34 | 600/1106 batches | lr 20.0000 | ms/batch 87.19 | loss  3.94 | ppl    51.32\n",
      "| epoch  34 | 800/1106 batches | lr 20.0000 | ms/batch 85.49 | loss  3.96 | ppl    52.48\n",
      "| epoch  34 | 1000/1106 batches | lr 20.0000 | ms/batch 86.67 | loss  3.98 | ppl    53.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 101.08s | valid loss  4.44 | valid ppl    84.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 | 200/1106 batches | lr 20.0000 | ms/batch 86.75 | loss  4.02 | ppl    55.49\n",
      "| epoch  35 | 400/1106 batches | lr 20.0000 | ms/batch 85.65 | loss  3.90 | ppl    49.25\n",
      "| epoch  35 | 600/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  3.91 | ppl    49.91\n",
      "| epoch  35 | 800/1106 batches | lr 20.0000 | ms/batch 86.78 | loss  3.95 | ppl    51.70\n",
      "| epoch  35 | 1000/1106 batches | lr 20.0000 | ms/batch 87.41 | loss  3.98 | ppl    53.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 101.09s | valid loss  4.42 | valid ppl    82.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 | 200/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  4.00 | ppl    54.63\n",
      "| epoch  36 | 400/1106 batches | lr 20.0000 | ms/batch 83.05 | loss  3.88 | ppl    48.50\n",
      "| epoch  36 | 600/1106 batches | lr 20.0000 | ms/batch 86.02 | loss  3.91 | ppl    49.71\n",
      "| epoch  36 | 800/1106 batches | lr 20.0000 | ms/batch 86.66 | loss  3.92 | ppl    50.65\n",
      "| epoch  36 | 1000/1106 batches | lr 20.0000 | ms/batch 84.89 | loss  3.97 | ppl    52.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 100.32s | valid loss  4.41 | valid ppl    82.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  37 | 200/1106 batches | lr 20.0000 | ms/batch 85.82 | loss  3.98 | ppl    53.33\n",
      "| epoch  37 | 400/1106 batches | lr 20.0000 | ms/batch 82.08 | loss  3.89 | ppl    48.72\n",
      "| epoch  37 | 600/1106 batches | lr 20.0000 | ms/batch 83.85 | loss  3.91 | ppl    49.79\n",
      "| epoch  37 | 800/1106 batches | lr 20.0000 | ms/batch 86.06 | loss  3.92 | ppl    50.29\n",
      "| epoch  37 | 1000/1106 batches | lr 20.0000 | ms/batch 85.49 | loss  3.96 | ppl    52.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 99.86s | valid loss  4.41 | valid ppl    82.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 | 200/1106 batches | lr 20.0000 | ms/batch 86.49 | loss  3.97 | ppl    53.18\n",
      "| epoch  38 | 400/1106 batches | lr 20.0000 | ms/batch 87.16 | loss  3.87 | ppl    47.71\n",
      "| epoch  38 | 600/1106 batches | lr 20.0000 | ms/batch 83.56 | loss  3.88 | ppl    48.55\n",
      "| epoch  38 | 800/1106 batches | lr 20.0000 | ms/batch 86.34 | loss  3.91 | ppl    49.92\n",
      "| epoch  38 | 1000/1106 batches | lr 20.0000 | ms/batch 82.27 | loss  3.94 | ppl    51.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 99.54s | valid loss  4.40 | valid ppl    81.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  39 | 200/1106 batches | lr 20.0000 | ms/batch 85.78 | loss  3.95 | ppl    52.17\n",
      "| epoch  39 | 400/1106 batches | lr 20.0000 | ms/batch 84.67 | loss  3.87 | ppl    47.90\n",
      "| epoch  39 | 600/1106 batches | lr 20.0000 | ms/batch 83.58 | loss  3.88 | ppl    48.27\n",
      "| epoch  39 | 800/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  3.90 | ppl    49.45\n",
      "| epoch  39 | 1000/1106 batches | lr 20.0000 | ms/batch 87.22 | loss  3.93 | ppl    51.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 100.32s | valid loss  4.40 | valid ppl    81.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  40 | 200/1106 batches | lr 20.0000 | ms/batch 85.92 | loss  3.95 | ppl    52.04\n",
      "| epoch  40 | 400/1106 batches | lr 20.0000 | ms/batch 85.10 | loss  3.86 | ppl    47.48\n",
      "| epoch  40 | 600/1106 batches | lr 20.0000 | ms/batch 84.54 | loss  3.86 | ppl    47.48\n",
      "| epoch  40 | 800/1106 batches | lr 20.0000 | ms/batch 83.54 | loss  3.90 | ppl    49.45\n",
      "| epoch  40 | 1000/1106 batches | lr 20.0000 | ms/batch 85.98 | loss  3.93 | ppl    50.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 99.47s | valid loss  4.40 | valid ppl    81.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  41 | 200/1106 batches | lr 20.0000 | ms/batch 85.67 | loss  3.94 | ppl    51.64\n",
      "| epoch  41 | 400/1106 batches | lr 20.0000 | ms/batch 84.61 | loss  3.86 | ppl    47.40\n",
      "| epoch  41 | 600/1106 batches | lr 20.0000 | ms/batch 84.24 | loss  3.85 | ppl    46.78\n",
      "| epoch  41 | 800/1106 batches | lr 20.0000 | ms/batch 84.67 | loss  3.89 | ppl    48.74\n",
      "| epoch  41 | 1000/1106 batches | lr 20.0000 | ms/batch 85.83 | loss  3.92 | ppl    50.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 99.43s | valid loss  4.40 | valid ppl    81.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 | 200/1106 batches | lr 20.0000 | ms/batch 84.90 | loss  3.93 | ppl    51.14\n",
      "| epoch  42 | 400/1106 batches | lr 20.0000 | ms/batch 87.14 | loss  3.83 | ppl    46.17\n",
      "| epoch  42 | 600/1106 batches | lr 20.0000 | ms/batch 84.82 | loss  3.84 | ppl    46.69\n",
      "| epoch  42 | 800/1106 batches | lr 20.0000 | ms/batch 87.77 | loss  3.87 | ppl    48.18\n",
      "| epoch  42 | 1000/1106 batches | lr 20.0000 | ms/batch 86.97 | loss  3.89 | ppl    49.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 100.16s | valid loss  4.40 | valid ppl    81.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 | 200/1106 batches | lr 20.0000 | ms/batch 84.57 | loss  3.92 | ppl    50.62\n",
      "| epoch  43 | 400/1106 batches | lr 20.0000 | ms/batch 85.46 | loss  3.82 | ppl    45.72\n",
      "| epoch  43 | 600/1106 batches | lr 20.0000 | ms/batch 85.09 | loss  3.84 | ppl    46.38\n",
      "| epoch  43 | 800/1106 batches | lr 20.0000 | ms/batch 83.89 | loss  3.84 | ppl    46.62\n",
      "| epoch  43 | 1000/1106 batches | lr 20.0000 | ms/batch 84.53 | loss  3.89 | ppl    48.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 99.22s | valid loss  4.40 | valid ppl    81.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 | 200/1106 batches | lr 20.0000 | ms/batch 85.71 | loss  3.91 | ppl    50.07\n",
      "| epoch  44 | 400/1106 batches | lr 20.0000 | ms/batch 86.34 | loss  3.82 | ppl    45.48\n",
      "| epoch  44 | 600/1106 batches | lr 20.0000 | ms/batch 85.00 | loss  3.84 | ppl    46.69\n",
      "| epoch  44 | 800/1106 batches | lr 20.0000 | ms/batch 84.62 | loss  3.85 | ppl    47.01\n",
      "| epoch  44 | 1000/1106 batches | lr 20.0000 | ms/batch 87.32 | loss  3.89 | ppl    48.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 100.89s | valid loss  4.39 | valid ppl    80.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  45 | 200/1106 batches | lr 20.0000 | ms/batch 87.59 | loss  3.91 | ppl    49.71\n",
      "| epoch  45 | 400/1106 batches | lr 20.0000 | ms/batch 84.96 | loss  3.81 | ppl    44.98\n",
      "| epoch  45 | 600/1106 batches | lr 20.0000 | ms/batch 85.84 | loss  3.81 | ppl    45.37\n",
      "| epoch  45 | 800/1106 batches | lr 20.0000 | ms/batch 85.90 | loss  3.85 | ppl    47.13\n",
      "| epoch  45 | 1000/1106 batches | lr 20.0000 | ms/batch 84.92 | loss  3.87 | ppl    48.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 100.41s | valid loss  4.41 | valid ppl    82.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 | 200/1106 batches | lr 20.0000 | ms/batch 86.34 | loss  3.90 | ppl    49.61\n",
      "| epoch  46 | 400/1106 batches | lr 20.0000 | ms/batch 86.36 | loss  3.79 | ppl    44.39\n",
      "| epoch  46 | 600/1106 batches | lr 20.0000 | ms/batch 86.61 | loss  3.82 | ppl    45.52\n",
      "| epoch  46 | 800/1106 batches | lr 20.0000 | ms/batch 86.30 | loss  3.84 | ppl    46.57\n",
      "| epoch  46 | 1000/1106 batches | lr 20.0000 | ms/batch 84.45 | loss  3.86 | ppl    47.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 100.16s | valid loss  4.40 | valid ppl    81.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 | 200/1106 batches | lr 20.0000 | ms/batch 87.11 | loss  3.89 | ppl    48.83\n",
      "| epoch  47 | 400/1106 batches | lr 20.0000 | ms/batch 85.93 | loss  3.80 | ppl    44.73\n",
      "| epoch  47 | 600/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  3.80 | ppl    44.82\n",
      "| epoch  47 | 800/1106 batches | lr 20.0000 | ms/batch 84.61 | loss  3.82 | ppl    45.78\n",
      "| epoch  47 | 1000/1106 batches | lr 20.0000 | ms/batch 84.13 | loss  3.86 | ppl    47.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 100.32s | valid loss  4.40 | valid ppl    81.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 | 200/1106 batches | lr 20.0000 | ms/batch 85.20 | loss  3.88 | ppl    48.66\n",
      "| epoch  48 | 400/1106 batches | lr 20.0000 | ms/batch 86.21 | loss  3.79 | ppl    44.12\n",
      "| epoch  48 | 600/1106 batches | lr 20.0000 | ms/batch 83.21 | loss  3.80 | ppl    44.62\n",
      "| epoch  48 | 800/1106 batches | lr 20.0000 | ms/batch 85.13 | loss  3.81 | ppl    45.34\n",
      "| epoch  48 | 1000/1106 batches | lr 20.0000 | ms/batch 84.92 | loss  3.87 | ppl    47.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 99.82s | valid loss  4.41 | valid ppl    82.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 | 200/1106 batches | lr 20.0000 | ms/batch 86.91 | loss  3.88 | ppl    48.22\n",
      "| epoch  49 | 400/1106 batches | lr 20.0000 | ms/batch 87.78 | loss  3.78 | ppl    43.66\n",
      "| epoch  49 | 600/1106 batches | lr 20.0000 | ms/batch 83.69 | loss  3.79 | ppl    44.15\n",
      "| epoch  49 | 800/1106 batches | lr 20.0000 | ms/batch 84.78 | loss  3.81 | ppl    45.27\n",
      "| epoch  49 | 1000/1106 batches | lr 20.0000 | ms/batch 83.66 | loss  3.85 | ppl    47.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 100.48s | valid loss  4.40 | valid ppl    81.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 | 200/1106 batches | lr 20.0000 | ms/batch 85.21 | loss  3.87 | ppl    48.05\n",
      "| epoch  50 | 400/1106 batches | lr 20.0000 | ms/batch 85.91 | loss  3.77 | ppl    43.18\n",
      "| epoch  50 | 600/1106 batches | lr 20.0000 | ms/batch 86.15 | loss  3.78 | ppl    44.03\n",
      "| epoch  50 | 800/1106 batches | lr 20.0000 | ms/batch 84.76 | loss  3.81 | ppl    45.27\n",
      "| epoch  50 | 1000/1106 batches | lr 20.0000 | ms/batch 85.97 | loss  3.85 | ppl    46.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 100.54s | valid loss  4.40 | valid ppl    81.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 | 200/1106 batches | lr 20.0000 | ms/batch 86.96 | loss  3.87 | ppl    47.76\n",
      "| epoch  51 | 400/1106 batches | lr 20.0000 | ms/batch 85.45 | loss  3.76 | ppl    43.11\n",
      "| epoch  51 | 600/1106 batches | lr 20.0000 | ms/batch 84.51 | loss  3.78 | ppl    43.79\n",
      "| epoch  51 | 800/1106 batches | lr 20.0000 | ms/batch 85.33 | loss  3.81 | ppl    45.10\n",
      "| epoch  51 | 1000/1106 batches | lr 20.0000 | ms/batch 87.50 | loss  3.84 | ppl    46.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 100.35s | valid loss  4.39 | valid ppl    80.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  52 | 200/1106 batches | lr 20.0000 | ms/batch 82.46 | loss  3.85 | ppl    47.20\n",
      "| epoch  52 | 400/1106 batches | lr 20.0000 | ms/batch 84.78 | loss  3.76 | ppl    42.78\n",
      "| epoch  52 | 600/1106 batches | lr 20.0000 | ms/batch 88.28 | loss  3.77 | ppl    43.40\n",
      "| epoch  52 | 800/1106 batches | lr 20.0000 | ms/batch 84.07 | loss  3.79 | ppl    44.14\n",
      "| epoch  52 | 1000/1106 batches | lr 20.0000 | ms/batch 86.03 | loss  3.83 | ppl    46.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 99.69s | valid loss  4.39 | valid ppl    80.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  53 | 200/1106 batches | lr 20.0000 | ms/batch 85.60 | loss  3.84 | ppl    46.75\n",
      "| epoch  53 | 400/1106 batches | lr 20.0000 | ms/batch 86.55 | loss  3.75 | ppl    42.62\n",
      "| epoch  53 | 600/1106 batches | lr 20.0000 | ms/batch 84.32 | loss  3.77 | ppl    43.57\n",
      "| epoch  53 | 800/1106 batches | lr 20.0000 | ms/batch 83.36 | loss  3.79 | ppl    44.15\n",
      "| epoch  53 | 1000/1106 batches | lr 20.0000 | ms/batch 85.68 | loss  3.83 | ppl    46.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 100.16s | valid loss  4.41 | valid ppl    82.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 | 200/1106 batches | lr 20.0000 | ms/batch 87.00 | loss  3.85 | ppl    46.88\n",
      "| epoch  54 | 400/1106 batches | lr 20.0000 | ms/batch 84.34 | loss  3.75 | ppl    42.51\n",
      "| epoch  54 | 600/1106 batches | lr 20.0000 | ms/batch 84.91 | loss  3.77 | ppl    43.19\n",
      "| epoch  54 | 800/1106 batches | lr 20.0000 | ms/batch 84.80 | loss  3.78 | ppl    43.87\n",
      "| epoch  54 | 1000/1106 batches | lr 20.0000 | ms/batch 86.66 | loss  3.82 | ppl    45.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 100.42s | valid loss  4.39 | valid ppl    81.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 | 200/1106 batches | lr 20.0000 | ms/batch 85.86 | loss  3.83 | ppl    46.15\n",
      "| epoch  55 | 400/1106 batches | lr 20.0000 | ms/batch 83.68 | loss  3.73 | ppl    41.86\n",
      "| epoch  55 | 600/1106 batches | lr 20.0000 | ms/batch 85.91 | loss  3.77 | ppl    43.37\n",
      "| epoch  55 | 800/1106 batches | lr 20.0000 | ms/batch 85.81 | loss  3.77 | ppl    43.31\n",
      "| epoch  55 | 1000/1106 batches | lr 20.0000 | ms/batch 86.75 | loss  3.81 | ppl    45.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 100.30s | valid loss  4.39 | valid ppl    80.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  56 | 200/1106 batches | lr 20.0000 | ms/batch 86.12 | loss  3.83 | ppl    45.97\n",
      "| epoch  56 | 400/1106 batches | lr 20.0000 | ms/batch 86.16 | loss  3.74 | ppl    41.96\n",
      "| epoch  56 | 600/1106 batches | lr 20.0000 | ms/batch 85.67 | loss  3.74 | ppl    42.25\n",
      "| epoch  56 | 800/1106 batches | lr 20.0000 | ms/batch 85.89 | loss  3.75 | ppl    42.68\n",
      "| epoch  56 | 1000/1106 batches | lr 20.0000 | ms/batch 85.66 | loss  3.81 | ppl    44.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 100.90s | valid loss  4.39 | valid ppl    80.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  57 | 200/1106 batches | lr 20.0000 | ms/batch 84.72 | loss  3.83 | ppl    46.22\n",
      "| epoch  57 | 400/1106 batches | lr 20.0000 | ms/batch 86.46 | loss  3.73 | ppl    41.80\n",
      "| epoch  57 | 600/1106 batches | lr 20.0000 | ms/batch 87.69 | loss  3.74 | ppl    42.17\n",
      "| epoch  57 | 800/1106 batches | lr 20.0000 | ms/batch 88.70 | loss  3.76 | ppl    43.01\n",
      "| epoch  57 | 1000/1106 batches | lr 20.0000 | ms/batch 85.77 | loss  3.79 | ppl    44.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 100.95s | valid loss  4.40 | valid ppl    81.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 | 200/1106 batches | lr 20.0000 | ms/batch 86.18 | loss  3.83 | ppl    45.92\n",
      "| epoch  58 | 400/1106 batches | lr 20.0000 | ms/batch 84.28 | loss  3.72 | ppl    41.32\n",
      "| epoch  58 | 600/1106 batches | lr 20.0000 | ms/batch 85.93 | loss  3.74 | ppl    42.08\n",
      "| epoch  58 | 800/1106 batches | lr 20.0000 | ms/batch 87.84 | loss  3.75 | ppl    42.33\n",
      "| epoch  58 | 1000/1106 batches | lr 20.0000 | ms/batch 85.94 | loss  3.78 | ppl    43.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 100.12s | valid loss  4.38 | valid ppl    80.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  59 | 200/1106 batches | lr 20.0000 | ms/batch 86.09 | loss  3.81 | ppl    45.31\n",
      "| epoch  59 | 400/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  3.72 | ppl    41.08\n",
      "| epoch  59 | 600/1106 batches | lr 20.0000 | ms/batch 84.01 | loss  3.73 | ppl    41.49\n",
      "| epoch  59 | 800/1106 batches | lr 20.0000 | ms/batch 85.43 | loss  3.74 | ppl    41.98\n",
      "| epoch  59 | 1000/1106 batches | lr 20.0000 | ms/batch 85.05 | loss  3.80 | ppl    44.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 100.92s | valid loss  4.41 | valid ppl    82.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 | 200/1106 batches | lr 20.0000 | ms/batch 86.79 | loss  3.81 | ppl    45.29\n",
      "| epoch  60 | 400/1106 batches | lr 20.0000 | ms/batch 86.19 | loss  3.71 | ppl    40.76\n",
      "| epoch  60 | 600/1106 batches | lr 20.0000 | ms/batch 85.38 | loss  3.71 | ppl    41.00\n",
      "| epoch  60 | 800/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  3.75 | ppl    42.33\n",
      "| epoch  60 | 1000/1106 batches | lr 20.0000 | ms/batch 85.51 | loss  3.79 | ppl    44.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 100.23s | valid loss  4.39 | valid ppl    80.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 | 200/1106 batches | lr 20.0000 | ms/batch 86.36 | loss  3.81 | ppl    45.33\n",
      "| epoch  61 | 400/1106 batches | lr 20.0000 | ms/batch 86.31 | loss  3.70 | ppl    40.59\n",
      "| epoch  61 | 600/1106 batches | lr 20.0000 | ms/batch 87.47 | loss  3.72 | ppl    41.32\n",
      "| epoch  61 | 800/1106 batches | lr 20.0000 | ms/batch 86.00 | loss  3.74 | ppl    41.99\n",
      "| epoch  61 | 1000/1106 batches | lr 20.0000 | ms/batch 86.61 | loss  3.79 | ppl    44.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 101.15s | valid loss  4.40 | valid ppl    81.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 | 200/1106 batches | lr 20.0000 | ms/batch 86.78 | loss  3.80 | ppl    44.59\n",
      "| epoch  62 | 400/1106 batches | lr 20.0000 | ms/batch 84.61 | loss  3.70 | ppl    40.26\n",
      "| epoch  62 | 600/1106 batches | lr 20.0000 | ms/batch 86.24 | loss  3.71 | ppl    41.02\n",
      "| epoch  62 | 800/1106 batches | lr 20.0000 | ms/batch 85.33 | loss  3.74 | ppl    41.90\n",
      "| epoch  62 | 1000/1106 batches | lr 20.0000 | ms/batch 84.91 | loss  3.77 | ppl    43.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 100.13s | valid loss  4.38 | valid ppl    80.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  63 | 200/1106 batches | lr 20.0000 | ms/batch 88.10 | loss  3.79 | ppl    44.47\n",
      "| epoch  63 | 400/1106 batches | lr 20.0000 | ms/batch 85.46 | loss  3.70 | ppl    40.35\n",
      "| epoch  63 | 600/1106 batches | lr 20.0000 | ms/batch 86.74 | loss  3.70 | ppl    40.61\n",
      "| epoch  63 | 800/1106 batches | lr 20.0000 | ms/batch 84.76 | loss  3.72 | ppl    41.11\n",
      "| epoch  63 | 1000/1106 batches | lr 20.0000 | ms/batch 84.21 | loss  3.77 | ppl    43.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 100.66s | valid loss  4.39 | valid ppl    80.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 | 200/1106 batches | lr 20.0000 | ms/batch 88.46 | loss  3.79 | ppl    44.35\n",
      "| epoch  64 | 400/1106 batches | lr 20.0000 | ms/batch 85.64 | loss  3.68 | ppl    39.71\n",
      "| epoch  64 | 600/1106 batches | lr 20.0000 | ms/batch 84.82 | loss  3.71 | ppl    40.97\n",
      "| epoch  64 | 800/1106 batches | lr 20.0000 | ms/batch 85.18 | loss  3.72 | ppl    41.25\n",
      "| epoch  64 | 1000/1106 batches | lr 20.0000 | ms/batch 87.26 | loss  3.78 | ppl    43.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 100.96s | valid loss  4.40 | valid ppl    81.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 | 200/1106 batches | lr 20.0000 | ms/batch 84.71 | loss  3.78 | ppl    43.93\n",
      "| epoch  65 | 400/1106 batches | lr 20.0000 | ms/batch 86.71 | loss  3.67 | ppl    39.16\n",
      "| epoch  65 | 600/1106 batches | lr 20.0000 | ms/batch 87.12 | loss  3.71 | ppl    40.70\n",
      "| epoch  65 | 800/1106 batches | lr 20.0000 | ms/batch 85.94 | loss  3.71 | ppl    40.87\n",
      "| epoch  65 | 1000/1106 batches | lr 20.0000 | ms/batch 85.41 | loss  3.75 | ppl    42.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 100.33s | valid loss  4.39 | valid ppl    80.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 | 200/1106 batches | lr 20.0000 | ms/batch 86.44 | loss  3.78 | ppl    43.75\n",
      "| epoch  66 | 400/1106 batches | lr 20.0000 | ms/batch 85.35 | loss  3.68 | ppl    39.61\n",
      "| epoch  66 | 600/1106 batches | lr 20.0000 | ms/batch 86.15 | loss  3.69 | ppl    39.91\n",
      "| epoch  66 | 800/1106 batches | lr 20.0000 | ms/batch 84.83 | loss  3.71 | ppl    40.92\n",
      "| epoch  66 | 1000/1106 batches | lr 20.0000 | ms/batch 84.32 | loss  3.76 | ppl    43.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 99.53s | valid loss  4.40 | valid ppl    81.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 | 200/1106 batches | lr 20.0000 | ms/batch 83.79 | loss  3.78 | ppl    43.68\n",
      "| epoch  67 | 400/1106 batches | lr 20.0000 | ms/batch 86.57 | loss  3.69 | ppl    39.89\n",
      "| epoch  67 | 600/1106 batches | lr 20.0000 | ms/batch 87.34 | loss  3.69 | ppl    40.11\n",
      "| epoch  67 | 800/1106 batches | lr 20.0000 | ms/batch 84.16 | loss  3.70 | ppl    40.27\n",
      "| epoch  67 | 1000/1106 batches | lr 20.0000 | ms/batch 84.05 | loss  3.75 | ppl    42.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 99.98s | valid loss  4.39 | valid ppl    80.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 | 200/1106 batches | lr 20.0000 | ms/batch 84.56 | loss  3.77 | ppl    43.35\n",
      "| epoch  68 | 400/1106 batches | lr 20.0000 | ms/batch 86.43 | loss  3.67 | ppl    39.24\n",
      "| epoch  68 | 600/1106 batches | lr 20.0000 | ms/batch 84.96 | loss  3.68 | ppl    39.81\n",
      "| epoch  68 | 800/1106 batches | lr 20.0000 | ms/batch 85.29 | loss  3.70 | ppl    40.52\n",
      "| epoch  68 | 1000/1106 batches | lr 20.0000 | ms/batch 84.74 | loss  3.74 | ppl    42.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 100.14s | valid loss  4.39 | valid ppl    80.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 | 200/1106 batches | lr 20.0000 | ms/batch 86.09 | loss  3.76 | ppl    42.97\n",
      "| epoch  69 | 400/1106 batches | lr 20.0000 | ms/batch 85.68 | loss  3.66 | ppl    38.92\n",
      "| epoch  69 | 600/1106 batches | lr 20.0000 | ms/batch 85.38 | loss  3.67 | ppl    39.32\n",
      "| epoch  69 | 800/1106 batches | lr 20.0000 | ms/batch 85.72 | loss  3.70 | ppl    40.29\n",
      "| epoch  69 | 1000/1106 batches | lr 20.0000 | ms/batch 84.80 | loss  3.74 | ppl    41.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 100.16s | valid loss  4.40 | valid ppl    81.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 | 200/1106 batches | lr 20.0000 | ms/batch 83.48 | loss  3.75 | ppl    42.59\n",
      "| epoch  70 | 400/1106 batches | lr 20.0000 | ms/batch 84.88 | loss  3.65 | ppl    38.33\n",
      "| epoch  70 | 600/1106 batches | lr 20.0000 | ms/batch 84.64 | loss  3.68 | ppl    39.56\n",
      "| epoch  70 | 800/1106 batches | lr 20.0000 | ms/batch 86.79 | loss  3.69 | ppl    39.86\n",
      "| epoch  70 | 1000/1106 batches | lr 20.0000 | ms/batch 85.77 | loss  3.75 | ppl    42.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 100.35s | valid loss  4.39 | valid ppl    80.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 | 200/1106 batches | lr 20.0000 | ms/batch 86.24 | loss  3.76 | ppl    42.88\n",
      "| epoch  71 | 400/1106 batches | lr 20.0000 | ms/batch 85.07 | loss  3.66 | ppl    38.68\n",
      "| epoch  71 | 600/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  3.67 | ppl    39.12\n",
      "| epoch  71 | 800/1106 batches | lr 20.0000 | ms/batch 85.59 | loss  3.70 | ppl    40.31\n",
      "| epoch  71 | 1000/1106 batches | lr 20.0000 | ms/batch 83.68 | loss  3.71 | ppl    41.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 99.86s | valid loss  4.40 | valid ppl    81.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 | 200/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  3.75 | ppl    42.33\n",
      "| epoch  72 | 400/1106 batches | lr 20.0000 | ms/batch 85.57 | loss  3.66 | ppl    38.77\n",
      "| epoch  72 | 600/1106 batches | lr 20.0000 | ms/batch 85.82 | loss  3.67 | ppl    39.35\n",
      "| epoch  72 | 800/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  3.70 | ppl    40.33\n",
      "| epoch  72 | 1000/1106 batches | lr 20.0000 | ms/batch 84.99 | loss  3.72 | ppl    41.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 99.70s | valid loss  4.39 | valid ppl    80.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 | 200/1106 batches | lr 20.0000 | ms/batch 85.41 | loss  3.74 | ppl    42.21\n",
      "| epoch  73 | 400/1106 batches | lr 20.0000 | ms/batch 81.82 | loss  3.65 | ppl    38.60\n",
      "| epoch  73 | 600/1106 batches | lr 20.0000 | ms/batch 87.13 | loss  3.67 | ppl    39.23\n",
      "| epoch  73 | 800/1106 batches | lr 20.0000 | ms/batch 85.05 | loss  3.68 | ppl    39.71\n",
      "| epoch  73 | 1000/1106 batches | lr 20.0000 | ms/batch 87.06 | loss  3.72 | ppl    41.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 100.31s | valid loss  4.39 | valid ppl    80.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 | 200/1106 batches | lr 20.0000 | ms/batch 86.70 | loss  3.73 | ppl    41.80\n",
      "| epoch  74 | 400/1106 batches | lr 20.0000 | ms/batch 85.37 | loss  3.65 | ppl    38.46\n",
      "| epoch  74 | 600/1106 batches | lr 20.0000 | ms/batch 86.79 | loss  3.66 | ppl    38.73\n",
      "| epoch  74 | 800/1106 batches | lr 20.0000 | ms/batch 85.06 | loss  3.68 | ppl    39.47\n",
      "| epoch  74 | 1000/1106 batches | lr 20.0000 | ms/batch 84.48 | loss  3.70 | ppl    40.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 100.52s | valid loss  4.41 | valid ppl    82.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 | 200/1106 batches | lr 20.0000 | ms/batch 86.57 | loss  3.73 | ppl    41.87\n",
      "| epoch  75 | 400/1106 batches | lr 20.0000 | ms/batch 86.03 | loss  3.64 | ppl    37.91\n",
      "| epoch  75 | 600/1106 batches | lr 20.0000 | ms/batch 86.03 | loss  3.65 | ppl    38.41\n",
      "| epoch  75 | 800/1106 batches | lr 20.0000 | ms/batch 84.02 | loss  3.68 | ppl    39.76\n",
      "| epoch  75 | 1000/1106 batches | lr 20.0000 | ms/batch 87.33 | loss  3.72 | ppl    41.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 100.35s | valid loss  4.41 | valid ppl    81.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 | 200/1106 batches | lr 20.0000 | ms/batch 84.63 | loss  3.73 | ppl    41.85\n",
      "| epoch  76 | 400/1106 batches | lr 20.0000 | ms/batch 85.38 | loss  3.63 | ppl    37.70\n",
      "| epoch  76 | 600/1106 batches | lr 20.0000 | ms/batch 85.12 | loss  3.65 | ppl    38.40\n",
      "| epoch  76 | 800/1106 batches | lr 20.0000 | ms/batch 86.16 | loss  3.67 | ppl    39.20\n",
      "| epoch  76 | 1000/1106 batches | lr 20.0000 | ms/batch 84.52 | loss  3.71 | ppl    41.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 99.89s | valid loss  4.39 | valid ppl    80.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 | 200/1106 batches | lr 20.0000 | ms/batch 88.18 | loss  3.73 | ppl    41.49\n",
      "| epoch  77 | 400/1106 batches | lr 20.0000 | ms/batch 86.16 | loss  3.62 | ppl    37.35\n",
      "| epoch  77 | 600/1106 batches | lr 20.0000 | ms/batch 87.55 | loss  3.65 | ppl    38.65\n",
      "| epoch  77 | 800/1106 batches | lr 20.0000 | ms/batch 85.98 | loss  3.67 | ppl    39.18\n",
      "| epoch  77 | 1000/1106 batches | lr 20.0000 | ms/batch 85.40 | loss  3.70 | ppl    40.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 100.42s | valid loss  4.39 | valid ppl    80.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 | 200/1106 batches | lr 20.0000 | ms/batch 87.60 | loss  3.72 | ppl    41.35\n",
      "| epoch  78 | 400/1106 batches | lr 20.0000 | ms/batch 85.41 | loss  3.62 | ppl    37.33\n",
      "| epoch  78 | 600/1106 batches | lr 20.0000 | ms/batch 84.31 | loss  3.65 | ppl    38.61\n",
      "| epoch  78 | 800/1106 batches | lr 20.0000 | ms/batch 86.29 | loss  3.66 | ppl    38.84\n",
      "| epoch  78 | 1000/1106 batches | lr 20.0000 | ms/batch 85.86 | loss  3.70 | ppl    40.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 100.11s | valid loss  4.39 | valid ppl    80.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 | 200/1106 batches | lr 20.0000 | ms/batch 86.47 | loss  3.72 | ppl    41.38\n",
      "| epoch  79 | 400/1106 batches | lr 20.0000 | ms/batch 83.76 | loss  3.63 | ppl    37.65\n",
      "| epoch  79 | 600/1106 batches | lr 20.0000 | ms/batch 84.84 | loss  3.64 | ppl    37.92\n",
      "| epoch  79 | 800/1106 batches | lr 20.0000 | ms/batch 84.49 | loss  3.65 | ppl    38.50\n",
      "| epoch  79 | 1000/1106 batches | lr 20.0000 | ms/batch 86.53 | loss  3.71 | ppl    40.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 100.03s | valid loss  4.39 | valid ppl    80.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 | 200/1106 batches | lr 20.0000 | ms/batch 87.43 | loss  3.72 | ppl    41.32\n",
      "| epoch  80 | 400/1106 batches | lr 20.0000 | ms/batch 86.10 | loss  3.63 | ppl    37.59\n",
      "| epoch  80 | 600/1106 batches | lr 20.0000 | ms/batch 86.56 | loss  3.62 | ppl    37.25\n",
      "| epoch  80 | 800/1106 batches | lr 20.0000 | ms/batch 87.72 | loss  3.66 | ppl    39.03\n",
      "| epoch  80 | 1000/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  3.69 | ppl    40.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 100.35s | valid loss  4.40 | valid ppl    81.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 | 200/1106 batches | lr 20.0000 | ms/batch 84.44 | loss  3.72 | ppl    41.10\n",
      "| epoch  81 | 400/1106 batches | lr 20.0000 | ms/batch 85.55 | loss  3.62 | ppl    37.38\n",
      "| epoch  81 | 600/1106 batches | lr 20.0000 | ms/batch 85.97 | loss  3.62 | ppl    37.50\n",
      "| epoch  81 | 800/1106 batches | lr 20.0000 | ms/batch 86.06 | loss  3.66 | ppl    38.90\n",
      "| epoch  81 | 1000/1106 batches | lr 20.0000 | ms/batch 83.54 | loss  3.69 | ppl    39.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 99.29s | valid loss  4.38 | valid ppl    80.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  82 | 200/1106 batches | lr 20.0000 | ms/batch 85.67 | loss  3.71 | ppl    40.74\n",
      "| epoch  82 | 400/1106 batches | lr 20.0000 | ms/batch 85.88 | loss  3.63 | ppl    37.73\n",
      "| epoch  82 | 600/1106 batches | lr 20.0000 | ms/batch 85.84 | loss  3.62 | ppl    37.22\n",
      "| epoch  82 | 800/1106 batches | lr 20.0000 | ms/batch 85.40 | loss  3.64 | ppl    38.25\n",
      "| epoch  82 | 1000/1106 batches | lr 20.0000 | ms/batch 84.68 | loss  3.69 | ppl    39.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 99.81s | valid loss  4.38 | valid ppl    80.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 | 200/1106 batches | lr 20.0000 | ms/batch 85.07 | loss  3.71 | ppl    41.03\n",
      "| epoch  83 | 400/1106 batches | lr 20.0000 | ms/batch 84.39 | loss  3.60 | ppl    36.49\n",
      "| epoch  83 | 600/1106 batches | lr 20.0000 | ms/batch 84.62 | loss  3.62 | ppl    37.33\n",
      "| epoch  83 | 800/1106 batches | lr 20.0000 | ms/batch 85.82 | loss  3.65 | ppl    38.40\n",
      "| epoch  83 | 1000/1106 batches | lr 20.0000 | ms/batch 84.43 | loss  3.68 | ppl    39.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 99.39s | valid loss  4.39 | valid ppl    80.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 | 200/1106 batches | lr 20.0000 | ms/batch 85.31 | loss  3.71 | ppl    40.92\n",
      "| epoch  84 | 400/1106 batches | lr 20.0000 | ms/batch 82.52 | loss  3.59 | ppl    36.32\n",
      "| epoch  84 | 600/1106 batches | lr 20.0000 | ms/batch 82.95 | loss  3.63 | ppl    37.90\n",
      "| epoch  84 | 800/1106 batches | lr 20.0000 | ms/batch 83.33 | loss  3.64 | ppl    38.07\n",
      "| epoch  84 | 1000/1106 batches | lr 20.0000 | ms/batch 85.52 | loss  3.69 | ppl    40.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 98.84s | valid loss  4.39 | valid ppl    80.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 | 200/1106 batches | lr 20.0000 | ms/batch 85.10 | loss  3.70 | ppl    40.65\n",
      "| epoch  85 | 400/1106 batches | lr 20.0000 | ms/batch 84.99 | loss  3.62 | ppl    37.24\n",
      "| epoch  85 | 600/1106 batches | lr 20.0000 | ms/batch 84.47 | loss  3.63 | ppl    37.74\n",
      "| epoch  85 | 800/1106 batches | lr 20.0000 | ms/batch 84.37 | loss  3.64 | ppl    38.12\n",
      "| epoch  85 | 1000/1106 batches | lr 20.0000 | ms/batch 84.45 | loss  3.68 | ppl    39.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 98.95s | valid loss  4.38 | valid ppl    79.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  86 | 200/1106 batches | lr 20.0000 | ms/batch 85.49 | loss  3.69 | ppl    40.23\n",
      "| epoch  86 | 400/1106 batches | lr 20.0000 | ms/batch 84.37 | loss  3.60 | ppl    36.51\n",
      "| epoch  86 | 600/1106 batches | lr 20.0000 | ms/batch 85.37 | loss  3.62 | ppl    37.17\n",
      "| epoch  86 | 800/1106 batches | lr 20.0000 | ms/batch 83.31 | loss  3.63 | ppl    37.63\n",
      "| epoch  86 | 1000/1106 batches | lr 20.0000 | ms/batch 83.86 | loss  3.67 | ppl    39.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 98.81s | valid loss  4.38 | valid ppl    79.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 | 200/1106 batches | lr 20.0000 | ms/batch 84.37 | loss  3.69 | ppl    40.12\n",
      "| epoch  87 | 400/1106 batches | lr 20.0000 | ms/batch 84.30 | loss  3.60 | ppl    36.42\n",
      "| epoch  87 | 600/1106 batches | lr 20.0000 | ms/batch 84.80 | loss  3.60 | ppl    36.56\n",
      "| epoch  87 | 800/1106 batches | lr 20.0000 | ms/batch 85.63 | loss  3.63 | ppl    37.71\n",
      "| epoch  87 | 1000/1106 batches | lr 20.0000 | ms/batch 84.13 | loss  3.67 | ppl    39.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 99.29s | valid loss  4.38 | valid ppl    79.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 | 200/1106 batches | lr 20.0000 | ms/batch 86.04 | loss  3.70 | ppl    40.56\n",
      "| epoch  88 | 400/1106 batches | lr 20.0000 | ms/batch 85.09 | loss  3.59 | ppl    36.35\n",
      "| epoch  88 | 600/1106 batches | lr 20.0000 | ms/batch 83.66 | loss  3.61 | ppl    36.96\n",
      "| epoch  88 | 800/1106 batches | lr 20.0000 | ms/batch 84.46 | loss  3.63 | ppl    37.55\n",
      "| epoch  88 | 1000/1106 batches | lr 20.0000 | ms/batch 85.44 | loss  3.66 | ppl    38.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 99.30s | valid loss  4.38 | valid ppl    79.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 | 200/1106 batches | lr 20.0000 | ms/batch 84.64 | loss  3.70 | ppl    40.35\n",
      "| epoch  89 | 400/1106 batches | lr 20.0000 | ms/batch 87.47 | loss  3.59 | ppl    36.20\n",
      "| epoch  89 | 600/1106 batches | lr 20.0000 | ms/batch 84.50 | loss  3.62 | ppl    37.23\n",
      "| epoch  89 | 800/1106 batches | lr 20.0000 | ms/batch 84.00 | loss  3.63 | ppl    37.54\n",
      "| epoch  89 | 1000/1106 batches | lr 20.0000 | ms/batch 84.87 | loss  3.67 | ppl    39.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 99.91s | valid loss  4.38 | valid ppl    80.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 | 200/1106 batches | lr 20.0000 | ms/batch 85.52 | loss  3.69 | ppl    39.90\n",
      "| epoch  90 | 400/1106 batches | lr 20.0000 | ms/batch 85.56 | loss  3.58 | ppl    36.02\n",
      "| epoch  90 | 600/1106 batches | lr 20.0000 | ms/batch 85.19 | loss  3.59 | ppl    36.35\n",
      "| epoch  90 | 800/1106 batches | lr 20.0000 | ms/batch 87.98 | loss  3.62 | ppl    37.30\n",
      "| epoch  90 | 1000/1106 batches | lr 20.0000 | ms/batch 86.43 | loss  3.67 | ppl    39.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 101.54s | valid loss  4.38 | valid ppl    79.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 | 200/1106 batches | lr 20.0000 | ms/batch 89.63 | loss  3.68 | ppl    39.51\n",
      "| epoch  91 | 400/1106 batches | lr 20.0000 | ms/batch 86.04 | loss  3.59 | ppl    36.17\n",
      "| epoch  91 | 600/1106 batches | lr 20.0000 | ms/batch 84.89 | loss  3.62 | ppl    37.40\n",
      "| epoch  91 | 800/1106 batches | lr 20.0000 | ms/batch 86.39 | loss  3.62 | ppl    37.31\n",
      "| epoch  91 | 1000/1106 batches | lr 20.0000 | ms/batch 86.91 | loss  3.67 | ppl    39.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time: 102.39s | valid loss  4.38 | valid ppl    80.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 | 200/1106 batches | lr 20.0000 | ms/batch 87.14 | loss  3.68 | ppl    39.79\n",
      "| epoch  92 | 400/1106 batches | lr 20.0000 | ms/batch 86.21 | loss  3.58 | ppl    35.95\n",
      "| epoch  92 | 600/1106 batches | lr 20.0000 | ms/batch 87.59 | loss  3.60 | ppl    36.64\n",
      "| epoch  92 | 800/1106 batches | lr 20.0000 | ms/batch 86.52 | loss  3.63 | ppl    37.55\n",
      "| epoch  92 | 1000/1106 batches | lr 20.0000 | ms/batch 85.47 | loss  3.66 | ppl    38.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time: 101.10s | valid loss  4.39 | valid ppl    80.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 | 200/1106 batches | lr 20.0000 | ms/batch 84.77 | loss  3.69 | ppl    39.90\n",
      "| epoch  93 | 400/1106 batches | lr 20.0000 | ms/batch 86.02 | loss  3.58 | ppl    36.01\n",
      "| epoch  93 | 600/1106 batches | lr 20.0000 | ms/batch 86.06 | loss  3.60 | ppl    36.60\n",
      "| epoch  93 | 800/1106 batches | lr 20.0000 | ms/batch 85.11 | loss  3.61 | ppl    36.80\n",
      "| epoch  93 | 1000/1106 batches | lr 20.0000 | ms/batch 85.73 | loss  3.66 | ppl    38.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time: 100.52s | valid loss  4.38 | valid ppl    80.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 | 200/1106 batches | lr 20.0000 | ms/batch 88.22 | loss  3.68 | ppl    39.64\n",
      "| epoch  94 | 400/1106 batches | lr 20.0000 | ms/batch 86.30 | loss  3.58 | ppl    35.95\n",
      "| epoch  94 | 600/1106 batches | lr 20.0000 | ms/batch 87.68 | loss  3.59 | ppl    36.21\n",
      "| epoch  94 | 800/1106 batches | lr 20.0000 | ms/batch 88.39 | loss  3.63 | ppl    37.62\n",
      "| epoch  94 | 1000/1106 batches | lr 20.0000 | ms/batch 86.10 | loss  3.65 | ppl    38.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time: 102.04s | valid loss  4.39 | valid ppl    80.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 | 200/1106 batches | lr 20.0000 | ms/batch 87.57 | loss  3.67 | ppl    39.15\n",
      "| epoch  95 | 400/1106 batches | lr 20.0000 | ms/batch 88.23 | loss  3.57 | ppl    35.50\n",
      "| epoch  95 | 600/1106 batches | lr 20.0000 | ms/batch 85.84 | loss  3.59 | ppl    36.25\n",
      "| epoch  95 | 800/1106 batches | lr 20.0000 | ms/batch 86.79 | loss  3.62 | ppl    37.40\n",
      "| epoch  95 | 1000/1106 batches | lr 20.0000 | ms/batch 86.24 | loss  3.64 | ppl    38.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 100.93s | valid loss  4.39 | valid ppl    80.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 | 200/1106 batches | lr 20.0000 | ms/batch 85.11 | loss  3.67 | ppl    39.28\n",
      "| epoch  96 | 400/1106 batches | lr 20.0000 | ms/batch 84.81 | loss  3.56 | ppl    35.26\n",
      "| epoch  96 | 600/1106 batches | lr 20.0000 | ms/batch 84.60 | loss  3.59 | ppl    36.17\n",
      "| epoch  96 | 800/1106 batches | lr 20.0000 | ms/batch 85.72 | loss  3.58 | ppl    35.98\n",
      "| epoch  96 | 1000/1106 batches | lr 20.0000 | ms/batch 84.70 | loss  3.65 | ppl    38.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time: 99.69s | valid loss  4.39 | valid ppl    80.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 | 200/1106 batches | lr 20.0000 | ms/batch 86.95 | loss  3.68 | ppl    39.47\n",
      "| epoch  97 | 400/1106 batches | lr 20.0000 | ms/batch 87.00 | loss  3.56 | ppl    35.16\n",
      "| epoch  97 | 600/1106 batches | lr 20.0000 | ms/batch 85.95 | loss  3.59 | ppl    36.13\n",
      "| epoch  97 | 800/1106 batches | lr 20.0000 | ms/batch 85.48 | loss  3.61 | ppl    36.84\n",
      "| epoch  97 | 1000/1106 batches | lr 20.0000 | ms/batch 85.60 | loss  3.63 | ppl    37.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time: 100.87s | valid loss  4.38 | valid ppl    79.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  98 | 200/1106 batches | lr 20.0000 | ms/batch 86.31 | loss  3.67 | ppl    39.44\n",
      "| epoch  98 | 400/1106 batches | lr 20.0000 | ms/batch 86.20 | loss  3.56 | ppl    35.07\n",
      "| epoch  98 | 600/1106 batches | lr 20.0000 | ms/batch 85.94 | loss  3.59 | ppl    36.18\n",
      "| epoch  98 | 800/1106 batches | lr 20.0000 | ms/batch 86.86 | loss  3.60 | ppl    36.73\n",
      "| epoch  98 | 1000/1106 batches | lr 20.0000 | ms/batch 83.89 | loss  3.63 | ppl    37.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time: 100.32s | valid loss  4.38 | valid ppl    80.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 | 200/1106 batches | lr 20.0000 | ms/batch 85.44 | loss  3.67 | ppl    39.14\n",
      "| epoch  99 | 400/1106 batches | lr 20.0000 | ms/batch 85.60 | loss  3.55 | ppl    34.90\n",
      "| epoch  99 | 600/1106 batches | lr 20.0000 | ms/batch 86.05 | loss  3.58 | ppl    35.91\n",
      "| epoch  99 | 800/1106 batches | lr 20.0000 | ms/batch 85.32 | loss  3.60 | ppl    36.62\n",
      "| epoch  99 | 1000/1106 batches | lr 20.0000 | ms/batch 85.31 | loss  3.64 | ppl    37.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time: 99.81s | valid loss  4.39 | valid ppl    80.34\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt)\n",
    "        val_loss = evaluate(val_data, model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "num_words = 150\n",
    "generated = []\n",
    "temperature = 1.0 # highest temperature increase diversity\n",
    "log_interval = 50 # \n",
    "model.eval()\n",
    "hidden = model.init_hidden(1)\n",
    "input = Variable(torch.rand(1, 1).mul(ntokens).long().cuda(), volatile=True)\n",
    "\n",
    "sent = []\n",
    "for i in range(num_words):\n",
    "    output, hidden = model(input, hidden, return_prob=True)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    input.data.fill_(word_idx)\n",
    "    word = corpus.dictionary.idx2word[word_idx]\n",
    "    sent.append(word) \n",
    "    if i % 20 == 19:\n",
    "        generated.append(sent)\n",
    "        sent = []\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        print('| Generated {}/{} words'.format(i, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
