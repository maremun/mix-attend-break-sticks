{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised and fixed code from https://github.com/JayParks/transformer (MT) for LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripped the code from the JayParks repo for MT Transformer. Introduced a few updates and changes for speed, but it's still frustratingly slow. Possible improvement - speed it up.\n",
    "\n",
    "Another issue - hyperparameter search for language modelling (number of heads, number of self-attention layers, etc). Does not work well from the box. This might be of help https://arxiv.org/pdf/1804.00247.pdf.\n",
    "\n",
    "Also consider parallelizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Clean up\n",
    "* Add MoS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sequence length batching\n",
    "\n",
    "This version of Transformer LM usesrandom sequence length batching.\n",
    "\n",
    "**NB** Make sure the src code does not assuem the existence of PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from showprogress import showprogress\n",
    "\n",
    "import torch\n",
    "torch.cuda.device(0)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as pad\n",
    "from torch.nn.utils import clip_grad_norm_ as clip\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import const\n",
    "from data import *\n",
    "from transformermos import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "voc_size = 10000\n",
    "batch_size = 12\n",
    "seq_len = 74\n",
    "n_experts = 10\n",
    "a = torch.ones([batch_size, seq_len, d_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = torch.nn.Linear(d_model, n_experts*d_model)\n",
    "l2 = torch.nn.Linear(d_model, voc_size)\n",
    "l3 = torch.nn.Linear(d_model, n_experts, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8880, 10000])\n",
      "torch.Size([888, 10])\n",
      "torch.Size([888, 10000])\n"
     ]
    }
   ],
   "source": [
    "output = a\n",
    "latent = l1(output)  # h  [batch_size x seq_len x n_experts * d_model]\n",
    "logit = l2(latent.view(-1, d_model))  # HW [batch_size * seq_len * n_experts x d_model]\n",
    "print(logit.shape)\n",
    "prior_logit = l3(output).contiguous().view(-1, n_experts)\n",
    "prior = torch.nn.functional.softmax(prior_logit, dim=1)  # pi\n",
    "print(prior.shape)\n",
    "prob = torch.nn.functional.softmax(logit.view(-1, voc_size), dim=1).view(-1, n_experts, voc_size)  # exp(hw) / sum(exp(hw))\n",
    "prob = (prob * prior.unsqueeze(2).expand_as(prob)).sum(1)\n",
    "print(prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "#     if log_:\n",
    "#         with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "#             f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptb_datapath_train = 'data/penn/train.txt'\n",
    "# ptb_datapath_valid = 'data/penn/valid.txt'\n",
    "# ptb_datapath_test = 'data/penn/test.txt'\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "# ptb_train = DataSet(ptb_datapath_train, batch_size, display_freq=0, max_len=90, trunc_len=90)\n",
    "# ptb_valid = DataSet(ptb_datapath_valid, batch_size, display_freq=0, max_len=90, trunc_len=90)\n",
    "# ptb_test = DataSet(ptb_datapath_test, batch_size, display_freq=0, max_len=90, trunc_len=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptb_train.build_dict()\n",
    "# ptb_valid.change_dict(ptb_train.dictionary)\n",
    "# ptb_test.change_dict(ptb_train.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([77465, 12])\n",
      "torch.Size([7376, 10])\n",
      "torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "############ Optional: get data by tokens ###############\n",
    "corpus = Corpus('data/penn')\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "batch_size = 12\n",
    "train_data = batchify(corpus.train, batch_size, )\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, test_batch_size)\n",
    "\n",
    "#### how to take a batch ####\n",
    "# the data is already splitten into batch_size(now we need to decide about seq length)\n",
    "# batch_num = 2\n",
    "# batch = get_batch(train_data, batch_num, seq_len=35)\n",
    "\n",
    "\n",
    "#### TODO (if needed) ###\n",
    "# 1) repackage hiddens for learning by tokens\n",
    "# 2) learn not every step (depends on 1st point)\n",
    "# 3) add grad clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(corpus.dictionary) #corpus.dictionary.total # ptb_train.num_vocb\n",
    "n_tokens = voc_size\n",
    "emb_dim = 512\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "n_layers = 2\n",
    "n_heads = 4\n",
    "d_ff = 2048\n",
    "max_tgt_seq_len = 90\n",
    "dropout = 0.1\n",
    "weighted_model = False\n",
    "share_proj_weight = True\n",
    "lr = 1e-6\n",
    "n_epochs = 1000\n",
    "clip_grad = 5\n",
    "warmup_steps = 2000\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMTransformer(n_layers, d_k, d_v, emb_dim, d_ff,\n",
    "                      n_heads, max_tgt_seq_len, voc_size,\n",
    "                      dropout, weighted_model, share_proj_weight)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=const.PAD)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "#opt = optim.Adam(model.trainable_params(), lr=lr)\n",
    "# lr_lambda = lambda epoch: 0.99 ** epoch\n",
    "#lrsched = StepLR(opt, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt0 = 70\n",
    "max_seq_len_delta = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_len is strange parameter\n",
    "def evaluate(data_source, model, ntokens, seq_len):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batch = 0\n",
    "    for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "        data, targets = get_batch(data_source, i, seq_len=seq_len)\n",
    "        seq_len = data.shape[1]\n",
    "        lengths = torch.ones(data.shape[0], device=device, dtype=torch.long) * seq_len\n",
    "\n",
    "        log_prob, self_attn = model(data, lengths)\n",
    "        loss = criterion(log_prob, targets.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch += 1\n",
    "    return total_loss / batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.trainable_params(),betas=(0.9, 0.98), eps=1e-09, lr=lr, weight_decay=1e-5)\n",
    "i=0\n",
    "best_val_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch 1, learning rate 0.000001 \n",
      "| epoch   0 | 200/1106 batches | lr 0.0001 | ms/batch 40.20 | loss 12.26 | ppl 211825.15\n",
      "| epoch   0 | 400/1106 batches | lr 0.0002 | ms/batch 39.63 | loss 10.30 | ppl 29599.82\n",
      "| epoch   0 | 600/1106 batches | lr 0.0003 | ms/batch 39.16 | loss  9.06 | ppl  8587.34\n",
      "| epoch   0 | 800/1106 batches | lr 0.0004 | ms/batch 40.22 | loss  8.95 | ppl  7737.95\n",
      "| epoch   0 | 1000/1106 batches | lr 0.0005 | ms/batch 39.71 | loss  8.75 | ppl  6329.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 47.99s | valid loss  8.11 | valid ppl  3324.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 2, learning rate 0.000565 \n",
      "| epoch   1 | 200/1106 batches | lr 0.0001 | ms/batch 39.83 | loss  7.89 | ppl  2680.51\n",
      "| epoch   1 | 400/1106 batches | lr 0.0002 | ms/batch 39.62 | loss  7.73 | ppl  2275.75\n",
      "| epoch   1 | 600/1106 batches | lr 0.0003 | ms/batch 40.57 | loss  5.81 | ppl   334.65\n",
      "| epoch   1 | 800/1106 batches | lr 0.0004 | ms/batch 40.54 | loss  6.06 | ppl   428.64\n",
      "| epoch   1 | 1000/1106 batches | lr 0.0005 | ms/batch 39.35 | loss  5.76 | ppl   316.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 47.71s | valid loss  5.80 | valid ppl   330.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 3, learning rate 0.000565 \n",
      "| epoch   2 | 200/1106 batches | lr 0.0001 | ms/batch 40.18 | loss  5.53 | ppl   251.24\n",
      "| epoch   2 | 400/1106 batches | lr 0.0002 | ms/batch 38.96 | loss  5.67 | ppl   289.21\n",
      "| epoch   2 | 600/1106 batches | lr 0.0003 | ms/batch 39.89 | loss  5.28 | ppl   197.27\n",
      "| epoch   2 | 800/1106 batches | lr 0.0004 | ms/batch 38.38 | loss  5.11 | ppl   165.86\n",
      "| epoch   2 | 1000/1106 batches | lr 0.0005 | ms/batch 39.60 | loss  5.49 | ppl   242.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 47.67s | valid loss  5.67 | valid ppl   289.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 4, learning rate 0.000572 \n",
      "| epoch   3 | 200/1106 batches | lr 0.0001 | ms/batch 40.02 | loss  5.47 | ppl   237.67\n",
      "| epoch   3 | 400/1106 batches | lr 0.0002 | ms/batch 40.33 | loss  5.49 | ppl   242.63\n",
      "| epoch   3 | 600/1106 batches | lr 0.0003 | ms/batch 38.69 | loss  5.22 | ppl   184.82\n",
      "| epoch   3 | 800/1106 batches | lr 0.0004 | ms/batch 39.99 | loss  5.31 | ppl   202.91\n",
      "| epoch   3 | 1000/1106 batches | lr 0.0005 | ms/batch 40.56 | loss  5.18 | ppl   177.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 47.72s | valid loss  5.54 | valid ppl   254.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 5, learning rate 0.000566 \n",
      "| epoch   4 | 200/1106 batches | lr 0.0001 | ms/batch 40.13 | loss  5.40 | ppl   221.47\n",
      "| epoch   4 | 400/1106 batches | lr 0.0002 | ms/batch 39.15 | loss  5.24 | ppl   188.47\n",
      "| epoch   4 | 600/1106 batches | lr 0.0003 | ms/batch 39.37 | loss  5.02 | ppl   152.17\n",
      "| epoch   4 | 800/1106 batches | lr 0.0004 | ms/batch 39.93 | loss  5.05 | ppl   155.60\n",
      "| epoch   4 | 1000/1106 batches | lr 0.0005 | ms/batch 39.25 | loss  4.89 | ppl   132.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 47.39s | valid loss  5.45 | valid ppl   232.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 6, learning rate 0.000566 \n",
      "| epoch   5 | 200/1106 batches | lr 0.0001 | ms/batch 39.98 | loss  5.35 | ppl   211.55\n",
      "| epoch   5 | 400/1106 batches | lr 0.0002 | ms/batch 39.03 | loss  4.76 | ppl   116.30\n",
      "| epoch   5 | 600/1106 batches | lr 0.0003 | ms/batch 40.74 | loss  4.71 | ppl   111.03\n",
      "| epoch   5 | 800/1106 batches | lr 0.0004 | ms/batch 39.48 | loss  5.42 | ppl   225.90\n",
      "| epoch   5 | 1000/1106 batches | lr 0.0005 | ms/batch 40.00 | loss  4.77 | ppl   117.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 47.59s | valid loss  5.35 | valid ppl   210.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 7, learning rate 0.000566 \n",
      "| epoch   6 | 200/1106 batches | lr 0.0001 | ms/batch 40.08 | loss  5.03 | ppl   152.97\n",
      "| epoch   6 | 400/1106 batches | lr 0.0002 | ms/batch 40.79 | loss  5.43 | ppl   227.53\n",
      "| epoch   6 | 600/1106 batches | lr 0.0003 | ms/batch 39.87 | loss  4.97 | ppl   143.87\n",
      "| epoch   6 | 800/1106 batches | lr 0.0004 | ms/batch 39.98 | loss  5.14 | ppl   170.22\n",
      "| epoch   6 | 1000/1106 batches | lr 0.0005 | ms/batch 39.67 | loss  5.02 | ppl   151.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 47.65s | valid loss  5.34 | valid ppl   208.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "Start epoch 8, learning rate 0.000562 \n",
      "| epoch   7 | 200/1106 batches | lr 0.0001 | ms/batch 39.43 | loss  5.22 | ppl   185.31\n",
      "| epoch   7 | 400/1106 batches | lr 0.0002 | ms/batch 42.04 | loss  4.66 | ppl   105.86\n",
      "| epoch   7 | 600/1106 batches | lr 0.0003 | ms/batch 39.50 | loss  4.83 | ppl   125.40\n",
      "| epoch   7 | 800/1106 batches | lr 0.0004 | ms/batch 40.05 | loss  4.99 | ppl   146.87\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        print('Start epoch %d, learning rate %f '%(epoch + 1, opt.state_dict()['param_groups'][0]['lr']))\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        batch, i = 0, 0\n",
    "        while i < train_data.size(0) - 2:\n",
    "            bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "            # Prevent excessively small or negative sequence lengths\n",
    "            seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "            # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "            seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "            data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "            seq_len = data.shape[1]\n",
    "            lengths = torch.ones(data.shape[0], device=device, dtype=torch.long) * seq_len\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output, self_attn = model.forward(data, lengths)\n",
    "#             break\n",
    "#         break\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            batch += 1\n",
    "            i += seq_len\n",
    "            \n",
    "            new_lr = np.power(emb_dim, -0.5) * np.min([\n",
    "                np.power((batch), -0.5),\n",
    "                np.power(warmup_steps, -1.5) * (batch)])\n",
    "            for param_group in opt.param_groups:\n",
    "                param_group['lr'] = new_lr\n",
    "\n",
    "            if batch % log_interval == 0 and batch > 0:\n",
    "                cur_loss = loss.item()\n",
    "                elapsed = time.time() - start_time\n",
    "                logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                        'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt0, opt.param_groups[0]['lr'],\n",
    "                    elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "        val_loss = evaluate(val_data, model, voc_size, bptt0)\n",
    "        logging('-' * 89)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        logging('-' * 89)\n",
    "\n",
    "        best_val_loss.append(val_loss)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89)\n",
    "    logging('Exiting from training early')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
