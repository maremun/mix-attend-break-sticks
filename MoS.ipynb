{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* MoS (just straight update from https://github.com/zihangdai/mos which is PyTorch 0.2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import model as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24300620\n",
      "Model total parameters: 24300620\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15\n",
    "\n",
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "if is_cuda:\n",
    "    parallel_model = model.cuda()\n",
    "else:\n",
    "    parallel_model = model\n",
    "\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5)))\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = model(cur_data.cuda(), hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "\n",
    "            loss = raw_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180514-144913\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/katrutsa/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py:491: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 117.75 | loss  7.00 | ppl  1096.81\n",
      "| epoch   1 | 400/1106 batches | lr 20.0000 | ms/batch 117.22 | loss  6.53 | ppl   685.71\n",
      "| epoch   1 | 600/1106 batches | lr 20.0000 | ms/batch 115.43 | loss  6.30 | ppl   546.30\n",
      "| epoch   1 | 800/1106 batches | lr 20.0000 | ms/batch 115.43 | loss  6.18 | ppl   483.19\n",
      "| epoch   1 | 1000/1106 batches | lr 20.0000 | ms/batch 117.71 | loss  6.03 | ppl   414.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 137.49s | valid loss  5.81 | valid ppl   332.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   2 | 200/1106 batches | lr 20.0000 | ms/batch 116.45 | loss  5.83 | ppl   342.03\n",
      "| epoch   2 | 400/1106 batches | lr 20.0000 | ms/batch 118.11 | loss  5.67 | ppl   290.42\n",
      "| epoch   2 | 600/1106 batches | lr 20.0000 | ms/batch 116.03 | loss  5.57 | ppl   263.68\n",
      "| epoch   2 | 800/1106 batches | lr 20.0000 | ms/batch 118.48 | loss  5.55 | ppl   257.72\n",
      "| epoch   2 | 1000/1106 batches | lr 20.0000 | ms/batch 117.42 | loss  5.50 | ppl   245.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 137.84s | valid loss  5.35 | valid ppl   209.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 | 200/1106 batches | lr 20.0000 | ms/batch 114.19 | loss  5.43 | ppl   228.76\n",
      "| epoch   3 | 400/1106 batches | lr 20.0000 | ms/batch 117.02 | loss  5.31 | ppl   203.36\n",
      "| epoch   3 | 600/1106 batches | lr 20.0000 | ms/batch 117.35 | loss  5.27 | ppl   193.58\n",
      "| epoch   3 | 800/1106 batches | lr 20.0000 | ms/batch 118.10 | loss  5.27 | ppl   195.02\n",
      "| epoch   3 | 1000/1106 batches | lr 20.0000 | ms/batch 117.26 | loss  5.25 | ppl   190.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 138.13s | valid loss  5.15 | valid ppl   172.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 | 200/1106 batches | lr 20.0000 | ms/batch 118.63 | loss  5.20 | ppl   181.82\n",
      "| epoch   4 | 400/1106 batches | lr 20.0000 | ms/batch 116.47 | loss  5.09 | ppl   162.19\n",
      "| epoch   4 | 600/1106 batches | lr 20.0000 | ms/batch 119.07 | loss  5.05 | ppl   156.51\n",
      "| epoch   4 | 800/1106 batches | lr 20.0000 | ms/batch 118.86 | loss  5.08 | ppl   160.95\n",
      "| epoch   4 | 1000/1106 batches | lr 20.0000 | ms/batch 118.42 | loss  5.07 | ppl   158.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 138.39s | valid loss  5.00 | valid ppl   148.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 | 200/1106 batches | lr 20.0000 | ms/batch 119.08 | loss  5.06 | ppl   158.29\n",
      "| epoch   5 | 400/1106 batches | lr 20.0000 | ms/batch 117.80 | loss  4.95 | ppl   141.39\n",
      "| epoch   5 | 600/1106 batches | lr 20.0000 | ms/batch 117.56 | loss  4.91 | ppl   136.16\n",
      "| epoch   5 | 800/1106 batches | lr 20.0000 | ms/batch 116.22 | loss  4.94 | ppl   140.05\n",
      "| epoch   5 | 1000/1106 batches | lr 20.0000 | ms/batch 116.57 | loss  4.95 | ppl   140.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 138.58s | valid loss  4.89 | valid ppl   133.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 | 200/1106 batches | lr 20.0000 | ms/batch 118.63 | loss  4.93 | ppl   138.63\n",
      "| epoch   6 | 400/1106 batches | lr 20.0000 | ms/batch 119.54 | loss  4.82 | ppl   123.95\n",
      "| epoch   6 | 600/1106 batches | lr 20.0000 | ms/batch 117.86 | loss  4.80 | ppl   121.24\n",
      "| epoch   6 | 800/1106 batches | lr 20.0000 | ms/batch 117.78 | loss  4.82 | ppl   124.44\n",
      "| epoch   6 | 1000/1106 batches | lr 20.0000 | ms/batch 117.78 | loss  4.83 | ppl   125.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 138.75s | valid loss  4.81 | valid ppl   122.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 | 200/1106 batches | lr 20.0000 | ms/batch 118.80 | loss  4.84 | ppl   126.11\n",
      "| epoch   7 | 400/1106 batches | lr 20.0000 | ms/batch 117.58 | loss  4.71 | ppl   111.22\n",
      "| epoch   7 | 600/1106 batches | lr 20.0000 | ms/batch 118.22 | loss  4.71 | ppl   110.82\n",
      "| epoch   7 | 800/1106 batches | lr 20.0000 | ms/batch 120.01 | loss  4.73 | ppl   112.99\n",
      "| epoch   7 | 1000/1106 batches | lr 20.0000 | ms/batch 116.46 | loss  4.74 | ppl   114.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 139.09s | valid loss  4.75 | valid ppl   115.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 | 200/1106 batches | lr 20.0000 | ms/batch 117.97 | loss  4.75 | ppl   115.75\n",
      "| epoch   8 | 400/1106 batches | lr 20.0000 | ms/batch 115.69 | loss  4.62 | ppl   101.53\n",
      "| epoch   8 | 600/1106 batches | lr 20.0000 | ms/batch 117.53 | loss  4.62 | ppl   101.66\n",
      "| epoch   8 | 800/1106 batches | lr 20.0000 | ms/batch 117.79 | loss  4.64 | ppl   103.58\n",
      "| epoch   8 | 1000/1106 batches | lr 20.0000 | ms/batch 117.89 | loss  4.68 | ppl   107.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 138.86s | valid loss  4.69 | valid ppl   108.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 | 200/1106 batches | lr 20.0000 | ms/batch 118.60 | loss  4.68 | ppl   107.80\n",
      "| epoch   9 | 400/1106 batches | lr 20.0000 | ms/batch 118.54 | loss  4.57 | ppl    96.07\n",
      "| epoch   9 | 600/1106 batches | lr 20.0000 | ms/batch 117.90 | loss  4.55 | ppl    94.91\n",
      "| epoch   9 | 800/1106 batches | lr 20.0000 | ms/batch 117.41 | loss  4.58 | ppl    97.96\n",
      "| epoch   9 | 1000/1106 batches | lr 20.0000 | ms/batch 117.91 | loss  4.60 | ppl    99.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 138.73s | valid loss  4.66 | valid ppl   105.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 | 200/1106 batches | lr 20.0000 | ms/batch 117.83 | loss  4.62 | ppl   101.12\n",
      "| epoch  10 | 400/1106 batches | lr 20.0000 | ms/batch 117.79 | loss  4.49 | ppl    89.33\n",
      "| epoch  10 | 600/1106 batches | lr 20.0000 | ms/batch 117.34 | loss  4.50 | ppl    89.79\n",
      "| epoch  10 | 800/1106 batches | lr 20.0000 | ms/batch 115.76 | loss  4.52 | ppl    91.78\n",
      "| epoch  10 | 1000/1106 batches | lr 20.0000 | ms/batch 120.19 | loss  4.54 | ppl    93.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 138.92s | valid loss  4.62 | valid ppl   101.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 | 200/1106 batches | lr 20.0000 | ms/batch 116.96 | loss  4.55 | ppl    94.81\n",
      "| epoch  11 | 400/1106 batches | lr 20.0000 | ms/batch 118.84 | loss  4.44 | ppl    84.40\n",
      "| epoch  11 | 600/1106 batches | lr 20.0000 | ms/batch 118.54 | loss  4.44 | ppl    84.73\n",
      "| epoch  11 | 800/1106 batches | lr 20.0000 | ms/batch 117.88 | loss  4.46 | ppl    86.87\n",
      "| epoch  11 | 1000/1106 batches | lr 20.0000 | ms/batch 117.32 | loss  4.49 | ppl    89.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 139.05s | valid loss  4.59 | valid ppl    98.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 | 200/1106 batches | lr 20.0000 | ms/batch 121.01 | loss  4.52 | ppl    91.45\n",
      "| epoch  12 | 400/1106 batches | lr 20.0000 | ms/batch 119.04 | loss  4.39 | ppl    80.49\n",
      "| epoch  12 | 600/1106 batches | lr 20.0000 | ms/batch 117.68 | loss  4.39 | ppl    80.44\n",
      "| epoch  12 | 800/1106 batches | lr 20.0000 | ms/batch 117.05 | loss  4.42 | ppl    83.50\n",
      "| epoch  12 | 1000/1106 batches | lr 20.0000 | ms/batch 117.91 | loss  4.45 | ppl    85.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 138.94s | valid loss  4.57 | valid ppl    96.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 | 200/1106 batches | lr 20.0000 | ms/batch 116.08 | loss  4.47 | ppl    87.30\n",
      "| epoch  13 | 400/1106 batches | lr 20.0000 | ms/batch 116.85 | loss  4.36 | ppl    78.00\n",
      "| epoch  13 | 600/1106 batches | lr 20.0000 | ms/batch 117.49 | loss  4.36 | ppl    77.93\n",
      "| epoch  13 | 800/1106 batches | lr 20.0000 | ms/batch 119.27 | loss  4.37 | ppl    79.38\n",
      "| epoch  13 | 1000/1106 batches | lr 20.0000 | ms/batch 119.33 | loss  4.41 | ppl    81.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 138.89s | valid loss  4.55 | valid ppl    94.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 | 200/1106 batches | lr 20.0000 | ms/batch 120.55 | loss  4.42 | ppl    82.99\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(parallel_model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(parallel_model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt) \n",
    "        \n",
    "        val_loss = evaluate(val_data, parallel_model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(parallel_model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, parallel_model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [pytorch]",
   "language": "python",
   "name": "Python [pytorch]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
