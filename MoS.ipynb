{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import model as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24300620\n",
      "Model total parameters: 24300620\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15\n",
    "\n",
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = model(cur_data.cuda(), hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "\n",
    "            loss = raw_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180524-062133\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py:491: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 88.35 | loss  6.98 | ppl  1078.04\n",
      "| epoch   1 | 400/1106 batches | lr 20.0000 | ms/batch 87.71 | loss  6.48 | ppl   650.80\n",
      "| epoch   1 | 600/1106 batches | lr 20.0000 | ms/batch 87.83 | loss  6.24 | ppl   511.66\n",
      "| epoch   1 | 800/1106 batches | lr 20.0000 | ms/batch 85.48 | loss  6.11 | ppl   449.99\n",
      "| epoch   1 | 1000/1106 batches | lr 20.0000 | ms/batch 87.90 | loss  5.97 | ppl   390.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 102.67s | valid loss  5.75 | valid ppl   315.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   2 | 200/1106 batches | lr 20.0000 | ms/batch 86.57 | loss  5.81 | ppl   332.66\n",
      "| epoch   2 | 400/1106 batches | lr 20.0000 | ms/batch 88.90 | loss  5.66 | ppl   287.57\n",
      "| epoch   2 | 600/1106 batches | lr 20.0000 | ms/batch 86.81 | loss  5.57 | ppl   261.23\n",
      "| epoch   2 | 800/1106 batches | lr 20.0000 | ms/batch 88.00 | loss  5.54 | ppl   254.76\n",
      "| epoch   2 | 1000/1106 batches | lr 20.0000 | ms/batch 87.21 | loss  5.49 | ppl   243.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 102.75s | valid loss  5.33 | valid ppl   206.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 | 200/1106 batches | lr 20.0000 | ms/batch 84.67 | loss  5.42 | ppl   226.86\n",
      "| epoch   3 | 400/1106 batches | lr 20.0000 | ms/batch 86.99 | loss  5.31 | ppl   201.59\n",
      "| epoch   3 | 600/1106 batches | lr 20.0000 | ms/batch 87.68 | loss  5.25 | ppl   190.78\n",
      "| epoch   3 | 800/1106 batches | lr 20.0000 | ms/batch 87.55 | loss  5.25 | ppl   191.47\n",
      "| epoch   3 | 1000/1106 batches | lr 20.0000 | ms/batch 85.97 | loss  5.24 | ppl   188.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 102.02s | valid loss  5.13 | valid ppl   169.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 | 200/1106 batches | lr 20.0000 | ms/batch 87.75 | loss  5.19 | ppl   179.45\n",
      "| epoch   4 | 400/1106 batches | lr 20.0000 | ms/batch 86.27 | loss  5.08 | ppl   160.55\n",
      "| epoch   4 | 600/1106 batches | lr 20.0000 | ms/batch 87.59 | loss  5.04 | ppl   154.13\n",
      "| epoch   4 | 800/1106 batches | lr 20.0000 | ms/batch 87.78 | loss  5.07 | ppl   158.40\n",
      "| epoch   4 | 1000/1106 batches | lr 20.0000 | ms/batch 87.80 | loss  5.05 | ppl   156.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 102.02s | valid loss  4.99 | valid ppl   146.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 | 200/1106 batches | lr 20.0000 | ms/batch 88.28 | loss  5.05 | ppl   156.06\n",
      "| epoch   5 | 400/1106 batches | lr 20.0000 | ms/batch 87.78 | loss  4.94 | ppl   139.75\n",
      "| epoch   5 | 600/1106 batches | lr 20.0000 | ms/batch 87.43 | loss  4.90 | ppl   134.14\n",
      "| epoch   5 | 800/1106 batches | lr 20.0000 | ms/batch 85.84 | loss  4.92 | ppl   137.33\n",
      "| epoch   5 | 1000/1106 batches | lr 20.0000 | ms/batch 86.26 | loss  4.93 | ppl   138.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 102.67s | valid loss  4.89 | valid ppl   132.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 | 200/1106 batches | lr 20.0000 | ms/batch 88.85 | loss  4.92 | ppl   136.77\n",
      "| epoch   6 | 400/1106 batches | lr 20.0000 | ms/batch 89.75 | loss  4.81 | ppl   122.63\n",
      "| epoch   6 | 600/1106 batches | lr 20.0000 | ms/batch 87.48 | loss  4.78 | ppl   119.43\n",
      "| epoch   6 | 800/1106 batches | lr 20.0000 | ms/batch 86.86 | loss  4.81 | ppl   122.57\n",
      "| epoch   6 | 1000/1106 batches | lr 20.0000 | ms/batch 87.40 | loss  4.82 | ppl   123.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 102.95s | valid loss  4.80 | valid ppl   121.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 | 200/1106 batches | lr 20.0000 | ms/batch 88.45 | loss  4.82 | ppl   124.37\n",
      "| epoch   7 | 400/1106 batches | lr 20.0000 | ms/batch 86.71 | loss  4.70 | ppl   110.23\n",
      "| epoch   7 | 600/1106 batches | lr 20.0000 | ms/batch 86.79 | loss  4.70 | ppl   109.48\n",
      "| epoch   7 | 800/1106 batches | lr 20.0000 | ms/batch 87.68 | loss  4.72 | ppl   111.93\n",
      "| epoch   7 | 1000/1106 batches | lr 20.0000 | ms/batch 87.23 | loss  4.72 | ppl   112.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 102.72s | valid loss  4.74 | valid ppl   114.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 | 200/1106 batches | lr 20.0000 | ms/batch 87.52 | loss  4.74 | ppl   114.17\n",
      "| epoch   8 | 400/1106 batches | lr 20.0000 | ms/batch 85.85 | loss  4.61 | ppl   100.24\n",
      "| epoch   8 | 600/1106 batches | lr 20.0000 | ms/batch 86.44 | loss  4.61 | ppl   100.36\n",
      "| epoch   8 | 800/1106 batches | lr 20.0000 | ms/batch 87.10 | loss  4.63 | ppl   102.43\n",
      "| epoch   8 | 1000/1106 batches | lr 20.0000 | ms/batch 87.28 | loss  4.67 | ppl   106.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 102.50s | valid loss  4.68 | valid ppl   107.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 | 200/1106 batches | lr 20.0000 | ms/batch 89.71 | loss  4.67 | ppl   106.49\n",
      "| epoch   9 | 400/1106 batches | lr 20.0000 | ms/batch 90.90 | loss  4.56 | ppl    95.49\n",
      "| epoch   9 | 600/1106 batches | lr 20.0000 | ms/batch 86.94 | loss  4.54 | ppl    93.35\n",
      "| epoch   9 | 800/1106 batches | lr 20.0000 | ms/batch 87.49 | loss  4.57 | ppl    96.96\n",
      "| epoch   9 | 1000/1106 batches | lr 20.0000 | ms/batch 87.25 | loss  4.59 | ppl    98.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 103.85s | valid loss  4.66 | valid ppl   105.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 | 200/1106 batches | lr 20.0000 | ms/batch 88.03 | loss  4.61 | ppl   100.22\n",
      "| epoch  10 | 400/1106 batches | lr 20.0000 | ms/batch 89.27 | loss  4.48 | ppl    88.40\n",
      "| epoch  10 | 600/1106 batches | lr 20.0000 | ms/batch 86.64 | loss  4.48 | ppl    88.58\n",
      "| epoch  10 | 800/1106 batches | lr 20.0000 | ms/batch 84.75 | loss  4.51 | ppl    91.26\n",
      "| epoch  10 | 1000/1106 batches | lr 20.0000 | ms/batch 89.71 | loss  4.53 | ppl    92.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 103.23s | valid loss  4.62 | valid ppl   101.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 | 200/1106 batches | lr 20.0000 | ms/batch 87.29 | loss  4.55 | ppl    94.40\n",
      "| epoch  11 | 400/1106 batches | lr 20.0000 | ms/batch 90.27 | loss  4.43 | ppl    83.78\n",
      "| epoch  11 | 600/1106 batches | lr 20.0000 | ms/batch 87.51 | loss  4.43 | ppl    83.97\n",
      "| epoch  11 | 800/1106 batches | lr 20.0000 | ms/batch 86.76 | loss  4.46 | ppl    86.29\n",
      "| epoch  11 | 1000/1106 batches | lr 20.0000 | ms/batch 87.03 | loss  4.48 | ppl    88.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 103.23s | valid loss  4.59 | valid ppl    98.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 | 200/1106 batches | lr 20.0000 | ms/batch 89.99 | loss  4.51 | ppl    90.65\n",
      "| epoch  12 | 400/1106 batches | lr 20.0000 | ms/batch 89.44 | loss  4.38 | ppl    79.97\n",
      "| epoch  12 | 600/1106 batches | lr 20.0000 | ms/batch 87.31 | loss  4.38 | ppl    79.89\n",
      "| epoch  12 | 800/1106 batches | lr 20.0000 | ms/batch 87.53 | loss  4.42 | ppl    83.28\n",
      "| epoch  12 | 1000/1106 batches | lr 20.0000 | ms/batch 86.97 | loss  4.44 | ppl    85.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 103.33s | valid loss  4.56 | valid ppl    95.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 | 200/1106 batches | lr 20.0000 | ms/batch 86.39 | loss  4.46 | ppl    86.63\n",
      "| epoch  13 | 400/1106 batches | lr 20.0000 | ms/batch 88.06 | loss  4.35 | ppl    77.66\n",
      "| epoch  13 | 600/1106 batches | lr 20.0000 | ms/batch 87.22 | loss  4.34 | ppl    76.83\n",
      "| epoch  13 | 800/1106 batches | lr 20.0000 | ms/batch 87.95 | loss  4.37 | ppl    79.17\n",
      "| epoch  13 | 1000/1106 batches | lr 20.0000 | ms/batch 88.67 | loss  4.40 | ppl    81.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 103.08s | valid loss  4.54 | valid ppl    93.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 | 200/1106 batches | lr 20.0000 | ms/batch 90.11 | loss  4.41 | ppl    82.32\n",
      "| epoch  14 | 400/1106 batches | lr 20.0000 | ms/batch 89.09 | loss  4.29 | ppl    73.26\n",
      "| epoch  14 | 600/1106 batches | lr 20.0000 | ms/batch 86.79 | loss  4.30 | ppl    74.04\n",
      "| epoch  14 | 800/1106 batches | lr 20.0000 | ms/batch 90.12 | loss  4.33 | ppl    76.16\n",
      "| epoch  14 | 1000/1106 batches | lr 20.0000 | ms/batch 88.02 | loss  4.36 | ppl    78.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 103.06s | valid loss  4.53 | valid ppl    92.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 | 200/1106 batches | lr 20.0000 | ms/batch 88.04 | loss  4.38 | ppl    79.83\n",
      "| epoch  15 | 400/1106 batches | lr 20.0000 | ms/batch 88.38 | loss  4.26 | ppl    71.03\n",
      "| epoch  15 | 600/1106 batches | lr 20.0000 | ms/batch 88.38 | loss  4.27 | ppl    71.82\n",
      "| epoch  15 | 800/1106 batches | lr 20.0000 | ms/batch 88.66 | loss  4.29 | ppl    72.89\n",
      "| epoch  15 | 1000/1106 batches | lr 20.0000 | ms/batch 87.39 | loss  4.33 | ppl    76.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 103.49s | valid loss  4.51 | valid ppl    91.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 | 200/1106 batches | lr 20.0000 | ms/batch 88.22 | loss  4.36 | ppl    77.98\n",
      "| epoch  16 | 400/1106 batches | lr 20.0000 | ms/batch 86.93 | loss  4.23 | ppl    68.77\n",
      "| epoch  16 | 600/1106 batches | lr 20.0000 | ms/batch 87.82 | loss  4.24 | ppl    69.35\n",
      "| epoch  16 | 800/1106 batches | lr 20.0000 | ms/batch 87.08 | loss  4.25 | ppl    70.36\n",
      "| epoch  16 | 1000/1106 batches | lr 20.0000 | ms/batch 90.90 | loss  4.30 | ppl    73.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 103.85s | valid loss  4.50 | valid ppl    90.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 | 200/1106 batches | lr 20.0000 | ms/batch 90.72 | loss  4.33 | ppl    76.16\n",
      "| epoch  17 | 400/1106 batches | lr 20.0000 | ms/batch 88.75 | loss  4.19 | ppl    66.17\n",
      "| epoch  17 | 600/1106 batches | lr 20.0000 | ms/batch 88.89 | loss  4.21 | ppl    67.60\n",
      "| epoch  17 | 800/1106 batches | lr 20.0000 | ms/batch 90.63 | loss  4.26 | ppl    70.48\n",
      "| epoch  17 | 1000/1106 batches | lr 20.0000 | ms/batch 88.83 | loss  4.27 | ppl    71.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 104.72s | valid loss  4.49 | valid ppl    89.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 | 200/1106 batches | lr 20.0000 | ms/batch 89.89 | loss  4.29 | ppl    72.82\n",
      "| epoch  18 | 400/1106 batches | lr 20.0000 | ms/batch 87.74 | loss  4.17 | ppl    65.02\n",
      "| epoch  18 | 600/1106 batches | lr 20.0000 | ms/batch 87.44 | loss  4.18 | ppl    65.55\n",
      "| epoch  18 | 800/1106 batches | lr 20.0000 | ms/batch 90.56 | loss  4.21 | ppl    67.11\n",
      "| epoch  18 | 1000/1106 batches | lr 20.0000 | ms/batch 88.46 | loss  4.24 | ppl    69.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 104.32s | valid loss  4.48 | valid ppl    87.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 | 200/1106 batches | lr 20.0000 | ms/batch 90.40 | loss  4.27 | ppl    71.31\n",
      "| epoch  19 | 400/1106 batches | lr 20.0000 | ms/batch 86.42 | loss  4.15 | ppl    63.27\n",
      "| epoch  19 | 600/1106 batches | lr 20.0000 | ms/batch 88.71 | loss  4.16 | ppl    63.98\n",
      "| epoch  19 | 800/1106 batches | lr 20.0000 | ms/batch 88.36 | loss  4.18 | ppl    65.04\n",
      "| epoch  19 | 1000/1106 batches | lr 20.0000 | ms/batch 86.78 | loss  4.22 | ppl    68.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 103.52s | valid loss  4.47 | valid ppl    87.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  20 | 200/1106 batches | lr 20.0000 | ms/batch 89.40 | loss  4.25 | ppl    69.95\n",
      "| epoch  20 | 400/1106 batches | lr 20.0000 | ms/batch 86.94 | loss  4.12 | ppl    61.38\n",
      "| epoch  20 | 600/1106 batches | lr 20.0000 | ms/batch 86.67 | loss  4.14 | ppl    62.49\n",
      "| epoch  20 | 800/1106 batches | lr 20.0000 | ms/batch 87.69 | loss  4.17 | ppl    64.44\n",
      "| epoch  20 | 1000/1106 batches | lr 20.0000 | ms/batch 87.95 | loss  4.19 | ppl    65.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 102.70s | valid loss  4.46 | valid ppl    86.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  21 | 200/1106 batches | lr 20.0000 | ms/batch 87.37 | loss  4.21 | ppl    67.28\n",
      "| epoch  21 | 400/1106 batches | lr 20.0000 | ms/batch 88.63 | loss  4.11 | ppl    60.76\n",
      "| epoch  21 | 600/1106 batches | lr 20.0000 | ms/batch 88.07 | loss  4.12 | ppl    61.51\n",
      "| epoch  21 | 800/1106 batches | lr 20.0000 | ms/batch 87.99 | loss  4.15 | ppl    63.14\n",
      "| epoch  21 | 1000/1106 batches | lr 20.0000 | ms/batch 88.10 | loss  4.16 | ppl    64.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 103.07s | valid loss  4.44 | valid ppl    85.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 | 200/1106 batches | lr 20.0000 | ms/batch 90.45 | loss  4.19 | ppl    66.19\n",
      "| epoch  22 | 400/1106 batches | lr 20.0000 | ms/batch 85.94 | loss  4.07 | ppl    58.64\n",
      "| epoch  22 | 600/1106 batches | lr 20.0000 | ms/batch 87.59 | loss  4.09 | ppl    59.99\n",
      "| epoch  22 | 800/1106 batches | lr 20.0000 | ms/batch 87.18 | loss  4.12 | ppl    61.64\n",
      "| epoch  22 | 1000/1106 batches | lr 20.0000 | ms/batch 88.95 | loss  4.16 | ppl    64.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 103.30s | valid loss  4.45 | valid ppl    85.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 200/1106 batches | lr 20.0000 | ms/batch 90.17 | loss  4.17 | ppl    64.82\n",
      "| epoch  23 | 400/1106 batches | lr 20.0000 | ms/batch 89.46 | loss  4.06 | ppl    58.05\n",
      "| epoch  23 | 600/1106 batches | lr 20.0000 | ms/batch 89.33 | loss  4.07 | ppl    58.70\n",
      "| epoch  23 | 800/1106 batches | lr 20.0000 | ms/batch 91.63 | loss  4.11 | ppl    60.80\n",
      "| epoch  23 | 1000/1106 batches | lr 20.0000 | ms/batch 88.08 | loss  4.13 | ppl    62.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 105.32s | valid loss  4.44 | valid ppl    85.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 | 200/1106 batches | lr 20.0000 | ms/batch 90.14 | loss  4.15 | ppl    63.64\n",
      "| epoch  24 | 400/1106 batches | lr 20.0000 | ms/batch 89.61 | loss  4.05 | ppl    57.28\n",
      "| epoch  24 | 600/1106 batches | lr 20.0000 | ms/batch 87.05 | loss  4.06 | ppl    57.93\n",
      "| epoch  24 | 800/1106 batches | lr 20.0000 | ms/batch 88.77 | loss  4.08 | ppl    59.24\n",
      "| epoch  24 | 1000/1106 batches | lr 20.0000 | ms/batch 90.08 | loss  4.13 | ppl    61.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 105.84s | valid loss  4.43 | valid ppl    83.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  25 | 200/1106 batches | lr 20.0000 | ms/batch 91.43 | loss  4.13 | ppl    62.26\n",
      "| epoch  25 | 400/1106 batches | lr 20.0000 | ms/batch 88.11 | loss  4.03 | ppl    56.12\n",
      "| epoch  25 | 600/1106 batches | lr 20.0000 | ms/batch 88.86 | loss  4.04 | ppl    56.82\n",
      "| epoch  25 | 800/1106 batches | lr 20.0000 | ms/batch 89.91 | loss  4.07 | ppl    58.58\n",
      "| epoch  25 | 1000/1106 batches | lr 20.0000 | ms/batch 90.36 | loss  4.09 | ppl    59.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 105.27s | valid loss  4.42 | valid ppl    83.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 | 200/1106 batches | lr 20.0000 | ms/batch 88.53 | loss  4.12 | ppl    61.42\n",
      "| epoch  26 | 400/1106 batches | lr 20.0000 | ms/batch 90.12 | loss  4.02 | ppl    55.54\n",
      "| epoch  26 | 600/1106 batches | lr 20.0000 | ms/batch 90.38 | loss  4.03 | ppl    56.28\n",
      "| epoch  26 | 800/1106 batches | lr 20.0000 | ms/batch 90.17 | loss  4.05 | ppl    57.63\n",
      "| epoch  26 | 1000/1106 batches | lr 20.0000 | ms/batch 90.67 | loss  4.07 | ppl    58.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 105.06s | valid loss  4.43 | valid ppl    83.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 | 200/1106 batches | lr 20.0000 | ms/batch 91.68 | loss  4.10 | ppl    60.14\n",
      "| epoch  27 | 400/1106 batches | lr 20.0000 | ms/batch 90.69 | loss  4.00 | ppl    54.49\n",
      "| epoch  27 | 600/1106 batches | lr 20.0000 | ms/batch 89.58 | loss  4.00 | ppl    54.76\n",
      "| epoch  27 | 800/1106 batches | lr 20.0000 | ms/batch 88.33 | loss  4.02 | ppl    55.90\n",
      "| epoch  27 | 1000/1106 batches | lr 20.0000 | ms/batch 92.59 | loss  4.06 | ppl    58.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 104.81s | valid loss  4.41 | valid ppl    82.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  28 | 200/1106 batches | lr 20.0000 | ms/batch 90.23 | loss  4.09 | ppl    59.77\n",
      "| epoch  28 | 400/1106 batches | lr 20.0000 | ms/batch 89.72 | loss  3.98 | ppl    53.70\n",
      "| epoch  28 | 600/1106 batches | lr 20.0000 | ms/batch 89.23 | loss  3.99 | ppl    54.21\n",
      "| epoch  28 | 800/1106 batches | lr 20.0000 | ms/batch 88.38 | loss  4.02 | ppl    55.58\n",
      "| epoch  28 | 1000/1106 batches | lr 20.0000 | ms/batch 89.43 | loss  4.06 | ppl    58.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 105.21s | valid loss  4.42 | valid ppl    82.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 | 200/1106 batches | lr 20.0000 | ms/batch 91.52 | loss  4.07 | ppl    58.64\n",
      "| epoch  29 | 400/1106 batches | lr 20.0000 | ms/batch 90.21 | loss  3.96 | ppl    52.63\n",
      "| epoch  29 | 600/1106 batches | lr 20.0000 | ms/batch 90.52 | loss  3.97 | ppl    52.94\n",
      "| epoch  29 | 800/1106 batches | lr 20.0000 | ms/batch 89.22 | loss  4.01 | ppl    55.19\n",
      "| epoch  29 | 1000/1106 batches | lr 20.0000 | ms/batch 88.62 | loss  4.02 | ppl    55.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 105.01s | valid loss  4.42 | valid ppl    82.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 | 200/1106 batches | lr 20.0000 | ms/batch 92.81 | loss  4.05 | ppl    57.40\n",
      "| epoch  30 | 400/1106 batches | lr 20.0000 | ms/batch 91.24 | loss  3.96 | ppl    52.25\n",
      "| epoch  30 | 600/1106 batches | lr 20.0000 | ms/batch 89.73 | loss  3.96 | ppl    52.34\n",
      "| epoch  30 | 800/1106 batches | lr 20.0000 | ms/batch 89.86 | loss  3.99 | ppl    54.17\n",
      "| epoch  30 | 1000/1106 batches | lr 20.0000 | ms/batch 88.37 | loss  4.01 | ppl    54.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 105.24s | valid loss  4.42 | valid ppl    82.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 | 200/1106 batches | lr 20.0000 | ms/batch 87.34 | loss  4.03 | ppl    56.39\n",
      "| epoch  31 | 400/1106 batches | lr 20.0000 | ms/batch 90.77 | loss  3.93 | ppl    50.99\n",
      "| epoch  31 | 600/1106 batches | lr 20.0000 | ms/batch 91.97 | loss  3.95 | ppl    52.00\n",
      "| epoch  31 | 800/1106 batches | lr 20.0000 | ms/batch 86.49 | loss  3.98 | ppl    53.52\n",
      "| epoch  31 | 1000/1106 batches | lr 20.0000 | ms/batch 88.17 | loss  4.00 | ppl    54.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 104.31s | valid loss  4.41 | valid ppl    82.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  32 | 200/1106 batches | lr 20.0000 | ms/batch 90.90 | loss  4.02 | ppl    55.66\n",
      "| epoch  32 | 400/1106 batches | lr 20.0000 | ms/batch 89.16 | loss  3.93 | ppl    51.03\n",
      "| epoch  32 | 600/1106 batches | lr 20.0000 | ms/batch 87.90 | loss  3.94 | ppl    51.56\n",
      "| epoch  32 | 800/1106 batches | lr 20.0000 | ms/batch 86.19 | loss  3.97 | ppl    52.82\n",
      "| epoch  32 | 1000/1106 batches | lr 20.0000 | ms/batch 87.33 | loss  4.00 | ppl    54.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 103.66s | valid loss  4.41 | valid ppl    82.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 | 200/1106 batches | lr 20.0000 | ms/batch 89.99 | loss  4.02 | ppl    55.63\n",
      "| epoch  33 | 400/1106 batches | lr 20.0000 | ms/batch 87.50 | loss  3.90 | ppl    49.47\n",
      "| epoch  33 | 600/1106 batches | lr 20.0000 | ms/batch 91.43 | loss  3.91 | ppl    49.97\n",
      "| epoch  33 | 800/1106 batches | lr 20.0000 | ms/batch 90.20 | loss  3.96 | ppl    52.54\n",
      "| epoch  33 | 1000/1106 batches | lr 20.0000 | ms/batch 89.57 | loss  3.98 | ppl    53.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 104.85s | valid loss  4.40 | valid ppl    81.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  34 | 200/1106 batches | lr 20.0000 | ms/batch 90.33 | loss  4.01 | ppl    55.00\n",
      "| epoch  34 | 400/1106 batches | lr 20.0000 | ms/batch 88.12 | loss  3.90 | ppl    49.18\n",
      "| epoch  34 | 600/1106 batches | lr 20.0000 | ms/batch 89.25 | loss  3.91 | ppl    50.13\n",
      "| epoch  34 | 800/1106 batches | lr 20.0000 | ms/batch 87.16 | loss  3.94 | ppl    51.65\n",
      "| epoch  34 | 1000/1106 batches | lr 20.0000 | ms/batch 89.22 | loss  3.98 | ppl    53.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 104.78s | valid loss  4.40 | valid ppl    81.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  35 | 200/1106 batches | lr 20.0000 | ms/batch 90.26 | loss  4.00 | ppl    54.49\n",
      "| epoch  35 | 400/1106 batches | lr 20.0000 | ms/batch 92.05 | loss  3.89 | ppl    48.70\n",
      "| epoch  35 | 600/1106 batches | lr 20.0000 | ms/batch 90.59 | loss  3.91 | ppl    50.03\n",
      "| epoch  35 | 800/1106 batches | lr 20.0000 | ms/batch 89.27 | loss  3.93 | ppl    50.88\n",
      "| epoch  35 | 1000/1106 batches | lr 20.0000 | ms/batch 89.16 | loss  3.95 | ppl    52.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 105.26s | valid loss  4.40 | valid ppl    81.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 | 200/1106 batches | lr 20.0000 | ms/batch 91.90 | loss  3.99 | ppl    53.85\n",
      "| epoch  36 | 400/1106 batches | lr 20.0000 | ms/batch 87.83 | loss  3.88 | ppl    48.23\n",
      "| epoch  36 | 600/1106 batches | lr 20.0000 | ms/batch 89.12 | loss  3.88 | ppl    48.56\n",
      "| epoch  36 | 800/1106 batches | lr 20.0000 | ms/batch 89.28 | loss  3.92 | ppl    50.34\n",
      "| epoch  36 | 1000/1106 batches | lr 20.0000 | ms/batch 89.56 | loss  3.95 | ppl    51.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 104.98s | valid loss  4.39 | valid ppl    81.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  37 | 200/1106 batches | lr 20.0000 | ms/batch 89.77 | loss  3.97 | ppl    53.14\n",
      "| epoch  37 | 400/1106 batches | lr 20.0000 | ms/batch 86.95 | loss  3.86 | ppl    47.43\n",
      "| epoch  37 | 600/1106 batches | lr 20.0000 | ms/batch 90.61 | loss  3.87 | ppl    48.16\n",
      "| epoch  37 | 800/1106 batches | lr 20.0000 | ms/batch 90.03 | loss  3.90 | ppl    49.43\n",
      "| epoch  37 | 1000/1106 batches | lr 20.0000 | ms/batch 87.96 | loss  3.94 | ppl    51.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 105.16s | valid loss  4.39 | valid ppl    80.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  38 | 200/1106 batches | lr 20.0000 | ms/batch 90.47 | loss  3.95 | ppl    51.97\n",
      "| epoch  38 | 400/1106 batches | lr 20.0000 | ms/batch 85.78 | loss  3.86 | ppl    47.48\n",
      "| epoch  38 | 600/1106 batches | lr 20.0000 | ms/batch 88.06 | loss  3.88 | ppl    48.31\n",
      "| epoch  38 | 800/1106 batches | lr 20.0000 | ms/batch 89.57 | loss  3.89 | ppl    49.00\n",
      "| epoch  38 | 1000/1106 batches | lr 20.0000 | ms/batch 88.64 | loss  3.94 | ppl    51.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 104.54s | valid loss  4.39 | valid ppl    80.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 | 200/1106 batches | lr 20.0000 | ms/batch 90.24 | loss  3.96 | ppl    52.28\n",
      "| epoch  39 | 400/1106 batches | lr 20.0000 | ms/batch 90.24 | loss  3.85 | ppl    46.97\n",
      "| epoch  39 | 600/1106 batches | lr 20.0000 | ms/batch 87.97 | loss  3.85 | ppl    47.07\n",
      "| epoch  39 | 800/1106 batches | lr 20.0000 | ms/batch 91.80 | loss  3.89 | ppl    48.79\n",
      "| epoch  39 | 1000/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  3.91 | ppl    49.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 105.21s | valid loss  4.38 | valid ppl    80.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  40 | 200/1106 batches | lr 20.0000 | ms/batch 90.36 | loss  3.93 | ppl    51.02\n",
      "| epoch  40 | 400/1106 batches | lr 20.0000 | ms/batch 87.60 | loss  3.85 | ppl    46.99\n",
      "| epoch  40 | 600/1106 batches | lr 20.0000 | ms/batch 87.09 | loss  3.85 | ppl    46.96\n",
      "| epoch  40 | 800/1106 batches | lr 20.0000 | ms/batch 90.28 | loss  3.88 | ppl    48.23\n",
      "| epoch  40 | 1000/1106 batches | lr 20.0000 | ms/batch 91.08 | loss  3.91 | ppl    49.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 105.20s | valid loss  4.38 | valid ppl    80.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  41 | 200/1106 batches | lr 20.0000 | ms/batch 89.99 | loss  3.92 | ppl    50.31\n",
      "| epoch  41 | 400/1106 batches | lr 20.0000 | ms/batch 90.21 | loss  3.84 | ppl    46.46\n",
      "| epoch  41 | 600/1106 batches | lr 20.0000 | ms/batch 89.55 | loss  3.83 | ppl    46.08\n",
      "| epoch  41 | 800/1106 batches | lr 20.0000 | ms/batch 89.02 | loss  3.88 | ppl    48.35\n",
      "| epoch  41 | 1000/1106 batches | lr 20.0000 | ms/batch 90.30 | loss  3.90 | ppl    49.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 104.87s | valid loss  4.39 | valid ppl    80.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 | 200/1106 batches | lr 20.0000 | ms/batch 88.81 | loss  3.92 | ppl    50.56\n",
      "| epoch  42 | 400/1106 batches | lr 20.0000 | ms/batch 88.47 | loss  3.84 | ppl    46.66\n",
      "| epoch  42 | 600/1106 batches | lr 20.0000 | ms/batch 88.79 | loss  3.82 | ppl    45.73\n",
      "| epoch  42 | 800/1106 batches | lr 20.0000 | ms/batch 88.71 | loss  3.87 | ppl    47.73\n",
      "| epoch  42 | 1000/1106 batches | lr 20.0000 | ms/batch 91.71 | loss  3.89 | ppl    49.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 104.73s | valid loss  4.38 | valid ppl    80.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 | 200/1106 batches | lr 20.0000 | ms/batch 88.05 | loss  3.90 | ppl    49.64\n",
      "| epoch  43 | 400/1106 batches | lr 20.0000 | ms/batch 91.06 | loss  3.81 | ppl    45.35\n",
      "| epoch  43 | 600/1106 batches | lr 20.0000 | ms/batch 90.64 | loss  3.82 | ppl    45.70\n",
      "| epoch  43 | 800/1106 batches | lr 20.0000 | ms/batch 90.37 | loss  3.85 | ppl    47.03\n",
      "| epoch  43 | 1000/1106 batches | lr 20.0000 | ms/batch 91.86 | loss  3.87 | ppl    47.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 104.90s | valid loss  4.38 | valid ppl    80.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 | 200/1106 batches | lr 20.0000 | ms/batch 86.73 | loss  3.91 | ppl    49.65\n",
      "| epoch  44 | 400/1106 batches | lr 20.0000 | ms/batch 89.69 | loss  3.81 | ppl    45.13\n",
      "| epoch  44 | 600/1106 batches | lr 20.0000 | ms/batch 89.22 | loss  3.81 | ppl    45.23\n",
      "| epoch  44 | 800/1106 batches | lr 20.0000 | ms/batch 89.68 | loss  3.82 | ppl    45.79\n",
      "| epoch  44 | 1000/1106 batches | lr 20.0000 | ms/batch 89.90 | loss  3.87 | ppl    47.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 104.63s | valid loss  4.39 | valid ppl    80.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 | 200/1106 batches | lr 20.0000 | ms/batch 89.18 | loss  3.89 | ppl    49.01\n",
      "| epoch  45 | 400/1106 batches | lr 20.0000 | ms/batch 88.83 | loss  3.79 | ppl    44.30\n",
      "| epoch  45 | 600/1106 batches | lr 20.0000 | ms/batch 89.43 | loss  3.82 | ppl    45.62\n",
      "| epoch  45 | 800/1106 batches | lr 20.0000 | ms/batch 87.67 | loss  3.83 | ppl    45.90\n",
      "| epoch  45 | 1000/1106 batches | lr 20.0000 | ms/batch 91.41 | loss  3.86 | ppl    47.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 105.44s | valid loss  4.38 | valid ppl    79.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  46 | 200/1106 batches | lr 20.0000 | ms/batch 89.81 | loss  3.88 | ppl    48.54\n",
      "| epoch  46 | 400/1106 batches | lr 20.0000 | ms/batch 88.15 | loss  3.79 | ppl    44.39\n",
      "| epoch  46 | 600/1106 batches | lr 20.0000 | ms/batch 90.58 | loss  3.79 | ppl    44.35\n",
      "| epoch  46 | 800/1106 batches | lr 20.0000 | ms/batch 90.54 | loss  3.83 | ppl    46.10\n",
      "| epoch  46 | 1000/1106 batches | lr 20.0000 | ms/batch 88.79 | loss  3.85 | ppl    47.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 104.95s | valid loss  4.38 | valid ppl    79.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  47 | 200/1106 batches | lr 20.0000 | ms/batch 90.08 | loss  3.88 | ppl    48.64\n",
      "| epoch  47 | 400/1106 batches | lr 20.0000 | ms/batch 88.54 | loss  3.77 | ppl    43.57\n",
      "| epoch  47 | 600/1106 batches | lr 20.0000 | ms/batch 91.11 | loss  3.80 | ppl    44.54\n",
      "| epoch  47 | 800/1106 batches | lr 20.0000 | ms/batch 90.38 | loss  3.82 | ppl    45.60\n",
      "| epoch  47 | 1000/1106 batches | lr 20.0000 | ms/batch 90.68 | loss  3.84 | ppl    46.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 105.45s | valid loss  4.38 | valid ppl    79.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  48 | 200/1106 batches | lr 20.0000 | ms/batch 89.49 | loss  3.87 | ppl    48.01\n",
      "| epoch  48 | 400/1106 batches | lr 20.0000 | ms/batch 91.16 | loss  3.79 | ppl    44.07\n",
      "| epoch  48 | 600/1106 batches | lr 20.0000 | ms/batch 88.13 | loss  3.77 | ppl    43.58\n",
      "| epoch  48 | 800/1106 batches | lr 20.0000 | ms/batch 86.56 | loss  3.80 | ppl    44.72\n",
      "| epoch  48 | 1000/1106 batches | lr 20.0000 | ms/batch 88.95 | loss  3.84 | ppl    46.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 104.31s | valid loss  4.38 | valid ppl    79.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  49 | 200/1106 batches | lr 20.0000 | ms/batch 88.68 | loss  3.87 | ppl    47.74\n",
      "| epoch  49 | 400/1106 batches | lr 20.0000 | ms/batch 89.08 | loss  3.77 | ppl    43.25\n",
      "| epoch  49 | 600/1106 batches | lr 20.0000 | ms/batch 86.47 | loss  3.78 | ppl    43.60\n",
      "| epoch  49 | 800/1106 batches | lr 20.0000 | ms/batch 88.14 | loss  3.79 | ppl    44.36\n",
      "| epoch  49 | 1000/1106 batches | lr 20.0000 | ms/batch 90.28 | loss  3.85 | ppl    46.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 104.84s | valid loss  4.40 | valid ppl    81.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 | 200/1106 batches | lr 20.0000 | ms/batch 89.38 | loss  3.85 | ppl    46.98\n",
      "| epoch  50 | 400/1106 batches | lr 20.0000 | ms/batch 87.71 | loss  3.76 | ppl    42.91\n",
      "| epoch  50 | 600/1106 batches | lr 20.0000 | ms/batch 88.40 | loss  3.77 | ppl    43.28\n",
      "| epoch  50 | 800/1106 batches | lr 20.0000 | ms/batch 88.68 | loss  3.79 | ppl    44.32\n",
      "| epoch  50 | 1000/1106 batches | lr 20.0000 | ms/batch 88.79 | loss  3.84 | ppl    46.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 104.62s | valid loss  4.38 | valid ppl    79.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 | 200/1106 batches | lr 20.0000 | ms/batch 88.61 | loss  3.85 | ppl    47.08\n",
      "| epoch  51 | 400/1106 batches | lr 20.0000 | ms/batch 89.27 | loss  3.75 | ppl    42.31\n",
      "| epoch  51 | 600/1106 batches | lr 20.0000 | ms/batch 88.10 | loss  3.76 | ppl    43.02\n",
      "| epoch  51 | 800/1106 batches | lr 20.0000 | ms/batch 89.17 | loss  3.79 | ppl    44.09\n",
      "| epoch  51 | 1000/1106 batches | lr 20.0000 | ms/batch 88.94 | loss  3.82 | ppl    45.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time: 104.70s | valid loss  4.38 | valid ppl    79.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 | 200/1106 batches | lr 20.0000 | ms/batch 91.28 | loss  3.85 | ppl    47.06\n",
      "| epoch  52 | 400/1106 batches | lr 20.0000 | ms/batch 88.26 | loss  3.74 | ppl    42.16\n",
      "| epoch  52 | 600/1106 batches | lr 20.0000 | ms/batch 87.74 | loss  3.76 | ppl    42.87\n",
      "| epoch  52 | 800/1106 batches | lr 20.0000 | ms/batch 88.49 | loss  3.79 | ppl    44.40\n",
      "| epoch  52 | 1000/1106 batches | lr 20.0000 | ms/batch 89.40 | loss  3.82 | ppl    45.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time: 104.42s | valid loss  4.38 | valid ppl    79.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 | 200/1106 batches | lr 20.0000 | ms/batch 88.03 | loss  3.84 | ppl    46.57\n",
      "| epoch  53 | 400/1106 batches | lr 20.0000 | ms/batch 89.03 | loss  3.74 | ppl    42.18\n",
      "| epoch  53 | 600/1106 batches | lr 20.0000 | ms/batch 89.85 | loss  3.75 | ppl    42.46\n",
      "| epoch  53 | 800/1106 batches | lr 20.0000 | ms/batch 88.16 | loss  3.77 | ppl    43.21\n",
      "| epoch  53 | 1000/1106 batches | lr 20.0000 | ms/batch 90.51 | loss  3.81 | ppl    45.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time: 104.92s | valid loss  4.37 | valid ppl    79.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  54 | 200/1106 batches | lr 20.0000 | ms/batch 90.00 | loss  3.82 | ppl    45.72\n",
      "| epoch  54 | 400/1106 batches | lr 20.0000 | ms/batch 88.69 | loss  3.74 | ppl    41.97\n",
      "| epoch  54 | 600/1106 batches | lr 20.0000 | ms/batch 87.90 | loss  3.75 | ppl    42.50\n",
      "| epoch  54 | 800/1106 batches | lr 20.0000 | ms/batch 86.64 | loss  3.77 | ppl    43.28\n",
      "| epoch  54 | 1000/1106 batches | lr 20.0000 | ms/batch 89.97 | loss  3.81 | ppl    45.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time: 104.74s | valid loss  4.38 | valid ppl    79.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 | 200/1106 batches | lr 20.0000 | ms/batch 90.65 | loss  3.82 | ppl    45.69\n",
      "| epoch  55 | 400/1106 batches | lr 20.0000 | ms/batch 86.67 | loss  3.73 | ppl    41.60\n",
      "| epoch  55 | 600/1106 batches | lr 20.0000 | ms/batch 87.84 | loss  3.74 | ppl    42.24\n",
      "| epoch  55 | 800/1106 batches | lr 20.0000 | ms/batch 89.13 | loss  3.76 | ppl    43.06\n",
      "| epoch  55 | 1000/1106 batches | lr 20.0000 | ms/batch 89.66 | loss  3.80 | ppl    44.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 104.65s | valid loss  4.38 | valid ppl    79.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 | 200/1106 batches | lr 20.0000 | ms/batch 91.15 | loss  3.81 | ppl    45.29\n",
      "| epoch  56 | 400/1106 batches | lr 20.0000 | ms/batch 87.97 | loss  3.72 | ppl    41.22\n",
      "| epoch  56 | 600/1106 batches | lr 20.0000 | ms/batch 90.45 | loss  3.75 | ppl    42.46\n",
      "| epoch  56 | 800/1106 batches | lr 20.0000 | ms/batch 89.12 | loss  3.75 | ppl    42.48\n",
      "| epoch  56 | 1000/1106 batches | lr 20.0000 | ms/batch 88.48 | loss  3.79 | ppl    44.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time: 105.14s | valid loss  4.37 | valid ppl    79.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 | 200/1106 batches | lr 20.0000 | ms/batch 89.63 | loss  3.81 | ppl    45.18\n",
      "| epoch  57 | 400/1106 batches | lr 20.0000 | ms/batch 90.79 | loss  3.72 | ppl    41.33\n",
      "| epoch  57 | 600/1106 batches | lr 20.0000 | ms/batch 88.19 | loss  3.73 | ppl    41.73\n",
      "| epoch  57 | 800/1106 batches | lr 20.0000 | ms/batch 88.25 | loss  3.74 | ppl    42.14\n",
      "| epoch  57 | 1000/1106 batches | lr 20.0000 | ms/batch 89.33 | loss  3.78 | ppl    43.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time: 104.90s | valid loss  4.38 | valid ppl    80.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 | 200/1106 batches | lr 20.0000 | ms/batch 90.18 | loss  3.81 | ppl    45.33\n",
      "| epoch  58 | 400/1106 batches | lr 20.0000 | ms/batch 89.47 | loss  3.71 | ppl    40.89\n",
      "| epoch  58 | 600/1106 batches | lr 20.0000 | ms/batch 88.90 | loss  3.72 | ppl    41.32\n",
      "| epoch  58 | 800/1106 batches | lr 20.0000 | ms/batch 91.49 | loss  3.74 | ppl    42.09\n",
      "| epoch  58 | 1000/1106 batches | lr 20.0000 | ms/batch 89.34 | loss  3.77 | ppl    43.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time: 105.43s | valid loss  4.38 | valid ppl    79.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 | 200/1106 batches | lr 20.0000 | ms/batch 90.26 | loss  3.81 | ppl    45.00\n",
      "| epoch  59 | 400/1106 batches | lr 20.0000 | ms/batch 88.18 | loss  3.71 | ppl    40.82\n",
      "| epoch  59 | 600/1106 batches | lr 20.0000 | ms/batch 88.73 | loss  3.72 | ppl    41.34\n",
      "| epoch  59 | 800/1106 batches | lr 20.0000 | ms/batch 90.88 | loss  3.73 | ppl    41.75\n",
      "| epoch  59 | 1000/1106 batches | lr 20.0000 | ms/batch 92.50 | loss  3.76 | ppl    42.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time: 105.03s | valid loss  4.39 | valid ppl    80.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 | 200/1106 batches | lr 20.0000 | ms/batch 89.10 | loss  3.80 | ppl    44.62\n",
      "| epoch  60 | 400/1106 batches | lr 20.0000 | ms/batch 89.63 | loss  3.69 | ppl    40.21\n",
      "| epoch  60 | 600/1106 batches | lr 20.0000 | ms/batch 85.96 | loss  3.70 | ppl    40.62\n",
      "| epoch  60 | 800/1106 batches | lr 20.0000 | ms/batch 89.64 | loss  3.72 | ppl    41.25\n",
      "| epoch  60 | 1000/1106 batches | lr 20.0000 | ms/batch 85.76 | loss  3.78 | ppl    43.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 104.19s | valid loss  4.39 | valid ppl    80.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 | 200/1106 batches | lr 20.0000 | ms/batch 94.52 | loss  3.79 | ppl    44.21\n",
      "| epoch  61 | 400/1106 batches | lr 20.0000 | ms/batch 89.57 | loss  3.69 | ppl    39.86\n",
      "| epoch  61 | 600/1106 batches | lr 20.0000 | ms/batch 88.94 | loss  3.69 | ppl    40.22\n",
      "| epoch  61 | 800/1106 batches | lr 20.0000 | ms/batch 88.23 | loss  3.73 | ppl    41.71\n",
      "| epoch  61 | 1000/1106 batches | lr 20.0000 | ms/batch 87.95 | loss  3.76 | ppl    43.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time: 105.09s | valid loss  4.38 | valid ppl    79.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 | 200/1106 batches | lr 20.0000 | ms/batch 89.99 | loss  3.79 | ppl    44.26\n",
      "| epoch  62 | 400/1106 batches | lr 20.0000 | ms/batch 89.70 | loss  3.69 | ppl    40.02\n",
      "| epoch  62 | 600/1106 batches | lr 20.0000 | ms/batch 89.36 | loss  3.70 | ppl    40.34\n",
      "| epoch  62 | 800/1106 batches | lr 20.0000 | ms/batch 88.24 | loss  3.72 | ppl    41.25\n",
      "| epoch  62 | 1000/1106 batches | lr 20.0000 | ms/batch 87.72 | loss  3.77 | ppl    43.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time: 104.59s | valid loss  4.38 | valid ppl    79.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 | 200/1106 batches | lr 20.0000 | ms/batch 88.57 | loss  3.78 | ppl    43.90\n",
      "| epoch  63 | 400/1106 batches | lr 20.0000 | ms/batch 88.04 | loss  3.68 | ppl    39.63\n",
      "| epoch  63 | 600/1106 batches | lr 20.0000 | ms/batch 89.40 | loss  3.69 | ppl    40.22\n",
      "| epoch  63 | 800/1106 batches | lr 20.0000 | ms/batch 87.55 | loss  3.72 | ppl    41.20\n",
      "| epoch  63 | 1000/1106 batches | lr 20.0000 | ms/batch 89.08 | loss  3.75 | ppl    42.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time: 104.20s | valid loss  4.38 | valid ppl    79.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 | 200/1106 batches | lr 20.0000 | ms/batch 90.35 | loss  3.78 | ppl    43.64\n",
      "| epoch  64 | 400/1106 batches | lr 20.0000 | ms/batch 88.74 | loss  3.69 | ppl    40.05\n",
      "| epoch  64 | 600/1106 batches | lr 20.0000 | ms/batch 88.43 | loss  3.68 | ppl    39.81\n",
      "| epoch  64 | 800/1106 batches | lr 20.0000 | ms/batch 87.99 | loss  3.71 | ppl    40.73\n",
      "| epoch  64 | 1000/1106 batches | lr 20.0000 | ms/batch 86.92 | loss  3.76 | ppl    42.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time: 104.02s | valid loss  4.37 | valid ppl    79.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  65 | 200/1106 batches | lr 20.0000 | ms/batch 90.24 | loss  3.78 | ppl    43.61\n",
      "| epoch  65 | 400/1106 batches | lr 20.0000 | ms/batch 87.84 | loss  3.66 | ppl    39.00\n",
      "| epoch  65 | 600/1106 batches | lr 20.0000 | ms/batch 88.69 | loss  3.69 | ppl    40.23\n",
      "| epoch  65 | 800/1106 batches | lr 20.0000 | ms/batch 85.99 | loss  3.70 | ppl    40.57\n",
      "| epoch  65 | 1000/1106 batches | lr 20.0000 | ms/batch 87.71 | loss  3.76 | ppl    42.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 103.56s | valid loss  4.38 | valid ppl    79.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 | 200/1106 batches | lr 20.0000 | ms/batch 87.41 | loss  3.76 | ppl    43.06\n",
      "| epoch  66 | 400/1106 batches | lr 20.0000 | ms/batch 92.00 | loss  3.65 | ppl    38.43\n",
      "| epoch  66 | 600/1106 batches | lr 20.0000 | ms/batch 90.61 | loss  3.69 | ppl    39.96\n",
      "| epoch  66 | 800/1106 batches | lr 20.0000 | ms/batch 86.56 | loss  3.70 | ppl    40.41\n",
      "| epoch  66 | 1000/1106 batches | lr 20.0000 | ms/batch 88.27 | loss  3.73 | ppl    41.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time: 104.19s | valid loss  4.38 | valid ppl    79.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 | 200/1106 batches | lr 20.0000 | ms/batch 91.49 | loss  3.76 | ppl    42.85\n",
      "| epoch  67 | 400/1106 batches | lr 20.0000 | ms/batch 89.22 | loss  3.66 | ppl    38.80\n",
      "| epoch  67 | 600/1106 batches | lr 20.0000 | ms/batch 88.95 | loss  3.66 | ppl    38.94\n",
      "| epoch  67 | 800/1106 batches | lr 20.0000 | ms/batch 88.33 | loss  3.70 | ppl    40.30\n",
      "| epoch  67 | 1000/1106 batches | lr 20.0000 | ms/batch 89.05 | loss  3.74 | ppl    42.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time: 104.49s | valid loss  4.38 | valid ppl    79.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 | 200/1106 batches | lr 20.0000 | ms/batch 85.93 | loss  3.76 | ppl    42.96\n",
      "| epoch  68 | 400/1106 batches | lr 20.0000 | ms/batch 88.34 | loss  3.67 | ppl    39.17\n",
      "| epoch  68 | 600/1106 batches | lr 20.0000 | ms/batch 89.72 | loss  3.68 | ppl    39.72\n",
      "| epoch  68 | 800/1106 batches | lr 20.0000 | ms/batch 87.87 | loss  3.67 | ppl    39.39\n",
      "| epoch  68 | 1000/1106 batches | lr 20.0000 | ms/batch 86.89 | loss  3.73 | ppl    41.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time: 103.31s | valid loss  4.37 | valid ppl    79.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 | 200/1106 batches | lr 20.0000 | ms/batch 88.13 | loss  3.75 | ppl    42.60\n",
      "| epoch  69 | 400/1106 batches | lr 20.0000 | ms/batch 88.31 | loss  3.66 | ppl    38.85\n",
      "| epoch  69 | 600/1106 batches | lr 20.0000 | ms/batch 88.35 | loss  3.66 | ppl    38.91\n",
      "| epoch  69 | 800/1106 batches | lr 20.0000 | ms/batch 88.02 | loss  3.68 | ppl    39.74\n",
      "| epoch  69 | 1000/1106 batches | lr 20.0000 | ms/batch 87.74 | loss  3.72 | ppl    41.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time: 103.73s | valid loss  4.37 | valid ppl    79.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  70 | 200/1106 batches | lr 20.0000 | ms/batch 89.44 | loss  3.74 | ppl    42.24\n",
      "| epoch  70 | 400/1106 batches | lr 20.0000 | ms/batch 90.69 | loss  3.65 | ppl    38.61\n",
      "| epoch  70 | 600/1106 batches | lr 20.0000 | ms/batch 88.99 | loss  3.65 | ppl    38.60\n",
      "| epoch  70 | 800/1106 batches | lr 20.0000 | ms/batch 88.28 | loss  3.68 | ppl    39.67\n",
      "| epoch  70 | 1000/1106 batches | lr 20.0000 | ms/batch 87.21 | loss  3.72 | ppl    41.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 104.46s | valid loss  4.37 | valid ppl    79.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 | 200/1106 batches | lr 20.0000 | ms/batch 87.80 | loss  3.74 | ppl    41.97\n",
      "| epoch  71 | 400/1106 batches | lr 20.0000 | ms/batch 87.93 | loss  3.63 | ppl    37.79\n",
      "| epoch  71 | 600/1106 batches | lr 20.0000 | ms/batch 88.43 | loss  3.66 | ppl    38.77\n",
      "| epoch  71 | 800/1106 batches | lr 20.0000 | ms/batch 88.55 | loss  3.66 | ppl    39.04\n",
      "| epoch  71 | 1000/1106 batches | lr 20.0000 | ms/batch 87.18 | loss  3.73 | ppl    41.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time: 104.20s | valid loss  4.37 | valid ppl    79.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 | 200/1106 batches | lr 20.0000 | ms/batch 88.37 | loss  3.74 | ppl    42.12\n",
      "| epoch  72 | 400/1106 batches | lr 20.0000 | ms/batch 89.36 | loss  3.64 | ppl    38.04\n",
      "| epoch  72 | 600/1106 batches | lr 20.0000 | ms/batch 88.27 | loss  3.65 | ppl    38.36\n",
      "| epoch  72 | 800/1106 batches | lr 20.0000 | ms/batch 86.92 | loss  3.68 | ppl    39.55\n",
      "| epoch  72 | 1000/1106 batches | lr 20.0000 | ms/batch 86.93 | loss  3.69 | ppl    40.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time: 103.27s | valid loss  4.37 | valid ppl    78.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  73 | 200/1106 batches | lr 20.0000 | ms/batch 88.31 | loss  3.72 | ppl    41.30\n",
      "| epoch  73 | 400/1106 batches | lr 20.0000 | ms/batch 88.69 | loss  3.65 | ppl    38.34\n",
      "| epoch  73 | 600/1106 batches | lr 20.0000 | ms/batch 90.10 | loss  3.66 | ppl    38.71\n",
      "| epoch  73 | 800/1106 batches | lr 20.0000 | ms/batch 89.63 | loss  3.68 | ppl    39.71\n",
      "| epoch  73 | 1000/1106 batches | lr 20.0000 | ms/batch 88.29 | loss  3.71 | ppl    40.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time: 104.13s | valid loss  4.38 | valid ppl    80.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 | 200/1106 batches | lr 20.0000 | ms/batch 90.60 | loss  3.73 | ppl    41.51\n",
      "| epoch  74 | 400/1106 batches | lr 20.0000 | ms/batch 87.92 | loss  3.63 | ppl    37.87\n",
      "| epoch  74 | 600/1106 batches | lr 20.0000 | ms/batch 89.49 | loss  3.65 | ppl    38.33\n",
      "| epoch  74 | 800/1106 batches | lr 20.0000 | ms/batch 87.39 | loss  3.67 | ppl    39.19\n",
      "| epoch  74 | 1000/1106 batches | lr 20.0000 | ms/batch 88.53 | loss  3.70 | ppl    40.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time: 104.17s | valid loss  4.38 | valid ppl    79.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 | 200/1106 batches | lr 20.0000 | ms/batch 88.93 | loss  3.71 | ppl    40.94\n",
      "| epoch  75 | 400/1106 batches | lr 20.0000 | ms/batch 88.36 | loss  3.64 | ppl    38.07\n",
      "| epoch  75 | 600/1106 batches | lr 20.0000 | ms/batch 89.79 | loss  3.63 | ppl    37.74\n",
      "| epoch  75 | 800/1106 batches | lr 20.0000 | ms/batch 89.63 | loss  3.66 | ppl    38.89\n",
      "| epoch  75 | 1000/1106 batches | lr 20.0000 | ms/batch 87.80 | loss  3.69 | ppl    40.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 104.65s | valid loss  4.37 | valid ppl    79.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 | 200/1106 batches | lr 20.0000 | ms/batch 90.25 | loss  3.72 | ppl    41.25\n",
      "| epoch  76 | 400/1106 batches | lr 20.0000 | ms/batch 88.97 | loss  3.63 | ppl    37.67\n",
      "| epoch  76 | 600/1106 batches | lr 20.0000 | ms/batch 90.10 | loss  3.63 | ppl    37.71\n",
      "| epoch  76 | 800/1106 batches | lr 20.0000 | ms/batch 88.04 | loss  3.67 | ppl    39.18\n",
      "| epoch  76 | 1000/1106 batches | lr 20.0000 | ms/batch 89.96 | loss  3.70 | ppl    40.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time: 104.73s | valid loss  4.37 | valid ppl    79.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 | 200/1106 batches | lr 20.0000 | ms/batch 88.56 | loss  3.72 | ppl    41.27\n",
      "| epoch  77 | 400/1106 batches | lr 20.0000 | ms/batch 88.69 | loss  3.62 | ppl    37.16\n",
      "| epoch  77 | 600/1106 batches | lr 20.0000 | ms/batch 89.28 | loss  3.63 | ppl    37.82\n",
      "| epoch  77 | 800/1106 batches | lr 20.0000 | ms/batch 90.01 | loss  3.66 | ppl    38.73\n",
      "| epoch  77 | 1000/1106 batches | lr 20.0000 | ms/batch 87.63 | loss  3.70 | ppl    40.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time: 104.49s | valid loss  4.38 | valid ppl    80.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 | 200/1106 batches | lr 20.0000 | ms/batch 91.94 | loss  3.71 | ppl    40.98\n",
      "| epoch  78 | 400/1106 batches | lr 20.0000 | ms/batch 90.48 | loss  3.60 | ppl    36.72\n",
      "| epoch  78 | 600/1106 batches | lr 20.0000 | ms/batch 90.37 | loss  3.64 | ppl    37.91\n",
      "| epoch  78 | 800/1106 batches | lr 20.0000 | ms/batch 89.10 | loss  3.66 | ppl    38.75\n",
      "| epoch  78 | 1000/1106 batches | lr 20.0000 | ms/batch 89.93 | loss  3.68 | ppl    39.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time: 104.89s | valid loss  4.38 | valid ppl    79.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 | 200/1106 batches | lr 20.0000 | ms/batch 91.34 | loss  3.70 | ppl    40.58\n",
      "| epoch  79 | 400/1106 batches | lr 20.0000 | ms/batch 89.09 | loss  3.60 | ppl    36.72\n",
      "| epoch  79 | 600/1106 batches | lr 20.0000 | ms/batch 88.13 | loss  3.63 | ppl    37.83\n",
      "| epoch  79 | 800/1106 batches | lr 20.0000 | ms/batch 89.84 | loss  3.65 | ppl    38.38\n",
      "| epoch  79 | 1000/1106 batches | lr 20.0000 | ms/batch 89.96 | loss  3.68 | ppl    39.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time: 104.97s | valid loss  4.37 | valid ppl    79.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 | 200/1106 batches | lr 20.0000 | ms/batch 88.43 | loss  3.70 | ppl    40.62\n",
      "| epoch  80 | 400/1106 batches | lr 20.0000 | ms/batch 87.84 | loss  3.61 | ppl    36.93\n",
      "| epoch  80 | 600/1106 batches | lr 20.0000 | ms/batch 89.80 | loss  3.62 | ppl    37.17\n",
      "| epoch  80 | 800/1106 batches | lr 20.0000 | ms/batch 88.48 | loss  3.64 | ppl    38.14\n",
      "| epoch  80 | 1000/1106 batches | lr 20.0000 | ms/batch 93.25 | loss  3.69 | ppl    39.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 105.35s | valid loss  4.37 | valid ppl    79.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 | 200/1106 batches | lr 20.0000 | ms/batch 92.07 | loss  3.71 | ppl    40.77\n",
      "| epoch  81 | 400/1106 batches | lr 20.0000 | ms/batch 92.76 | loss  3.61 | ppl    36.97\n",
      "| epoch  81 | 600/1106 batches | lr 20.0000 | ms/batch 92.88 | loss  3.60 | ppl    36.68\n",
      "| epoch  81 | 800/1106 batches | lr 20.0000 | ms/batch 92.66 | loss  3.65 | ppl    38.29\n",
      "| epoch  81 | 1000/1106 batches | lr 20.0000 | ms/batch 92.20 | loss  3.68 | ppl    39.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time: 107.52s | valid loss  4.38 | valid ppl    79.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 | 200/1106 batches | lr 20.0000 | ms/batch 91.86 | loss  3.71 | ppl    40.78\n",
      "| epoch  82 | 400/1106 batches | lr 20.0000 | ms/batch 92.41 | loss  3.60 | ppl    36.69\n",
      "| epoch  82 | 600/1106 batches | lr 20.0000 | ms/batch 95.35 | loss  3.61 | ppl    36.82\n",
      "| epoch  82 | 800/1106 batches | lr 20.0000 | ms/batch 95.49 | loss  3.64 | ppl    38.16\n",
      "| epoch  82 | 1000/1106 batches | lr 20.0000 | ms/batch 91.49 | loss  3.67 | ppl    39.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time: 109.26s | valid loss  4.38 | valid ppl    79.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 | 200/1106 batches | lr 20.0000 | ms/batch 91.42 | loss  3.69 | ppl    40.00\n",
      "| epoch  83 | 400/1106 batches | lr 20.0000 | ms/batch 94.12 | loss  3.61 | ppl    36.89\n",
      "| epoch  83 | 600/1106 batches | lr 20.0000 | ms/batch 90.82 | loss  3.59 | ppl    36.26\n",
      "| epoch  83 | 800/1106 batches | lr 20.0000 | ms/batch 95.64 | loss  3.63 | ppl    37.63\n",
      "| epoch  83 | 1000/1106 batches | lr 20.0000 | ms/batch 93.09 | loss  3.66 | ppl    39.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time: 108.92s | valid loss  4.37 | valid ppl    78.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  84 | 200/1106 batches | lr 20.0000 | ms/batch 90.01 | loss  3.70 | ppl    40.30\n",
      "| epoch  84 | 400/1106 batches | lr 20.0000 | ms/batch 92.40 | loss  3.58 | ppl    35.95\n",
      "| epoch  84 | 600/1106 batches | lr 20.0000 | ms/batch 91.54 | loss  3.60 | ppl    36.77\n",
      "| epoch  84 | 800/1106 batches | lr 20.0000 | ms/batch 96.05 | loss  3.63 | ppl    37.69\n",
      "| epoch  84 | 1000/1106 batches | lr 20.0000 | ms/batch 93.17 | loss  3.66 | ppl    38.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time: 108.82s | valid loss  4.38 | valid ppl    79.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 | 200/1106 batches | lr 20.0000 | ms/batch 91.44 | loss  3.69 | ppl    40.09\n",
      "| epoch  85 | 400/1106 batches | lr 20.0000 | ms/batch 93.19 | loss  3.58 | ppl    35.90\n",
      "| epoch  85 | 600/1106 batches | lr 20.0000 | ms/batch 89.01 | loss  3.61 | ppl    37.02\n",
      "| epoch  85 | 800/1106 batches | lr 20.0000 | ms/batch 94.57 | loss  3.62 | ppl    37.32\n",
      "| epoch  85 | 1000/1106 batches | lr 20.0000 | ms/batch 93.80 | loss  3.67 | ppl    39.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 108.92s | valid loss  4.37 | valid ppl    78.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 | 200/1106 batches | lr 20.0000 | ms/batch 91.59 | loss  3.69 | ppl    39.99\n",
      "| epoch  86 | 400/1106 batches | lr 20.0000 | ms/batch 93.11 | loss  3.60 | ppl    36.68\n",
      "| epoch  86 | 600/1106 batches | lr 20.0000 | ms/batch 92.46 | loss  3.62 | ppl    37.17\n",
      "| epoch  86 | 800/1106 batches | lr 20.0000 | ms/batch 93.59 | loss  3.62 | ppl    37.40\n",
      "| epoch  86 | 1000/1106 batches | lr 20.0000 | ms/batch 92.12 | loss  3.67 | ppl    39.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time: 108.30s | valid loss  4.37 | valid ppl    79.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 | 200/1106 batches | lr 20.0000 | ms/batch 91.58 | loss  3.68 | ppl    39.84\n",
      "| epoch  87 | 400/1106 batches | lr 20.0000 | ms/batch 90.88 | loss  3.58 | ppl    35.92\n",
      "| epoch  87 | 600/1106 batches | lr 20.0000 | ms/batch 91.81 | loss  3.60 | ppl    36.43\n",
      "| epoch  87 | 800/1106 batches | lr 20.0000 | ms/batch 91.92 | loss  3.61 | ppl    36.99\n",
      "| epoch  87 | 1000/1106 batches | lr 20.0000 | ms/batch 94.28 | loss  3.65 | ppl    38.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time: 108.01s | valid loss  4.37 | valid ppl    79.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 | 200/1106 batches | lr 20.0000 | ms/batch 91.17 | loss  3.68 | ppl    39.50\n",
      "| epoch  88 | 400/1106 batches | lr 20.0000 | ms/batch 91.73 | loss  3.58 | ppl    35.90\n",
      "| epoch  88 | 600/1106 batches | lr 20.0000 | ms/batch 91.26 | loss  3.58 | ppl    35.87\n",
      "| epoch  88 | 800/1106 batches | lr 20.0000 | ms/batch 94.95 | loss  3.62 | ppl    37.30\n",
      "| epoch  88 | 1000/1106 batches | lr 20.0000 | ms/batch 92.62 | loss  3.66 | ppl    38.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time: 108.91s | valid loss  4.37 | valid ppl    79.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 | 200/1106 batches | lr 20.0000 | ms/batch 93.25 | loss  3.68 | ppl    39.66\n",
      "| epoch  89 | 400/1106 batches | lr 20.0000 | ms/batch 92.25 | loss  3.57 | ppl    35.61\n",
      "| epoch  89 | 600/1106 batches | lr 20.0000 | ms/batch 89.94 | loss  3.60 | ppl    36.66\n",
      "| epoch  89 | 800/1106 batches | lr 20.0000 | ms/batch 93.53 | loss  3.61 | ppl    36.97\n",
      "| epoch  89 | 1000/1106 batches | lr 20.0000 | ms/batch 95.31 | loss  3.64 | ppl    38.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time: 108.77s | valid loss  4.37 | valid ppl    78.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 | 200/1106 batches | lr 20.0000 | ms/batch 92.07 | loss  3.68 | ppl    39.71\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(parallel_model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(parallel_model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt)     \n",
    "        val_loss = evaluate(val_data, parallel_model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(parallel_model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, parallel_model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
