{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Softmaxes (RNN LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* MoS (just straight update from https://github.com/zihangdai/mos which is PyTorch 0.2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"./mos/\")\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "\n",
    "import gc\n",
    "\n",
    "import mos_data as data\n",
    "import model as m\n",
    "\n",
    "from utils import batchify, get_batch, repackage_hidden, create_exp_dir, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logging(s, print_=True, log_=True):\n",
    "    if print_:\n",
    "        print(s)\n",
    "    if log_:\n",
    "        with open(os.path.join(args.save, 'log.txt'), 'a+') as f_log:\n",
    "            f_log.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of generated data = torch.Size([77465, 12])\n",
      "Size of generated data = torch.Size([7376, 10])\n",
      "Size of generated data = torch.Size([82430, 1])\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "datafile = \"./data/penn/\"\n",
    "train_batch_size = 12\n",
    "eval_batch_size = 10\n",
    "test_batch_size = 1\n",
    "corpus = data.Corpus(datafile)\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "train_data = batchify(corpus.train, train_batch_size, is_cuda)\n",
    "val_data = batchify(corpus.valid, eval_batch_size, is_cuda)\n",
    "test_data = batchify(corpus.test, test_batch_size, is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Applying weight drop of 0.5 to weight_hh_l0\n",
      "Param size: 24300620\n",
      "Model total parameters: 24300620\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "is_keep_training = False\n",
    "path2saved_model = \"\"\n",
    "# Use parameters from first example in original repository\n",
    "# python main.py --data data/penn --dropouti 0.4 --dropoutl 0.29 --dropouth 0.225 --seed 28 --batch_size 12 \n",
    "# --lr 20.0 --epoch 1000 --nhid 960 --nhidlast 620 --emsize 280 --n_experts 15 --save PTB --single_gpu\n",
    "\n",
    "# Type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)\n",
    "model_type = \"LSTM\"\n",
    "# Size of embedding dimension\n",
    "emsize = 280\n",
    "# Number of hidden units per every RNN layer except the last one\n",
    "nhid = 960\n",
    "# Number of hidden units for the last RNN layer\n",
    "nhidlast = 620\n",
    "# Number of RNN layers\n",
    "nlayers = 3\n",
    "# Dropout after the last RNN layer\n",
    "dropout = 0.3 # default\n",
    "# Dropout for RNN layers\n",
    "dropouth = 0.225\n",
    "# Dropout for input embedding layers\n",
    "dropouti = 0.4\n",
    "# Dropout to remove words from embedding layer\n",
    "dropoute = 0.1 # default\n",
    "# Dropout for latent representation, before decoding\n",
    "dropoutl = 0.29\n",
    "# Amount of weight dropout to apply to the RNN hidden to hidden matrix\n",
    "# Strange dropout\n",
    "wdrop = 0.5 # default\n",
    "# Tie the word embedding and softmax weights\n",
    "tied = False\n",
    "# Number of softmaxes to mix\n",
    "n_experts = 15\n",
    "\n",
    "if is_keep_training:\n",
    "    model = torch.load(os.path.join(path2saved_model, 'model.pt'))\n",
    "else:\n",
    "    model = m.RNNModel(model_type, ntokens, emsize, nhid, nhidlast, nlayers, \n",
    "                       dropout, dropouth, dropouti, dropoute, wdrop, \n",
    "                       tied, dropoutl, n_experts)\n",
    "\n",
    "if is_cuda:\n",
    "    parallel_model = model.cuda()\n",
    "else:\n",
    "    parallel_model = model\n",
    "\n",
    "total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "# logging('Args: {}'.format(args))\n",
    "logging('Model total parameters: {}'.format(total_params), log_=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# seq_lenght is strange parameter\n",
    "def evaluate(data_source, model, ntokens, batch_size, seq_lenght):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, seq_lenght):\n",
    "        data, targets = get_batch(data_source, i, seq_lenght, evaluation=True)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        log_prob, hidden = model(data, hidden)\n",
    "        loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), targets).data\n",
    "\n",
    "        total_loss += loss * len(data)\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "    return total_loss.item() / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set parameters of training\n",
    "batch_size = 12\n",
    "# The batch size for computation. batch_size should be divisible by small_batch_size\n",
    "# In our implementation, we compute gradients with small_batch_size multiple times, and accumulate the gradients\\\n",
    "# until batch_size is reached. An update step is then performed.\n",
    "small_batch_size = batch_size\n",
    "# Gradient clipping\n",
    "clip = 0.25 # default\n",
    "# Regularization weight on RNN activations\n",
    "alpha = 2 # default\n",
    "# Sequence lenght\n",
    "bptt = 70 # default\n",
    "# Max sequence length delta\n",
    "max_seq_len_delta = 40 # default\n",
    "# Interval to print loss\n",
    "log_interval = 200 # default\n",
    "# Use logfile\n",
    "is_logfile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model for single epoch\n",
    "def train(model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt0):\n",
    "    assert batch_size % small_batch_size == 0, 'batch_size must be divisible by small_batch_size'\n",
    "\n",
    "    # Turn on training mode which enables dropout.\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    hidden = [model.init_hidden(small_batch_size) for _ in range(batch_size // small_batch_size)]\n",
    "    batch, i = 0, 0\n",
    "    while i < train_data.size(0) - 1 - 1:\n",
    "        bptt = bptt0 if np.random.random() < 0.95 else bptt0 / 2.\n",
    "        # Prevent excessively small or negative sequence lengths\n",
    "        seq_len = max(5, int(np.random.normal(bptt, 5))) # loc 70, scale 5\n",
    "        # There's a very small chance that it could select a very long sequence length resulting in OOM\n",
    "        seq_len = min(seq_len, bptt + max_seq_len_delta)\n",
    "\n",
    "        lr2 = optimizer.param_groups[0]['lr']\n",
    "        optimizer.param_groups[0]['lr'] = lr2 * seq_len / bptt\n",
    "        model.train()\n",
    "        data, targets = get_batch(train_data, i, seq_len=seq_len)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start, end, s_id = 0, small_batch_size, 0\n",
    "        while start < batch_size:\n",
    "            cur_data, cur_targets = data[:, start: end], targets[:, start: end].contiguous().view(-1)\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden[s_id] = repackage_hidden(hidden[s_id])\n",
    "            log_prob, hidden[s_id], rnn_hs, dropped_rnn_hs = model(cur_data.cuda(), hidden[s_id], return_h=True)\n",
    "            raw_loss = nn.functional.nll_loss(log_prob.view(-1, log_prob.size(2)), cur_targets)\n",
    "\n",
    "            loss = raw_loss\n",
    "            # Activation Regularization\n",
    "            loss = loss + sum(alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n",
    "            # Temporal activation Regularization (slowness)\n",
    "            loss = loss + sum(beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n",
    "            loss *= small_batch_size / batch_size\n",
    "            total_loss += raw_loss.data * small_batch_size / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            s_id += 1\n",
    "            start = end\n",
    "            end = start + small_batch_size\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # total_loss += raw_loss.data\n",
    "        optimizer.param_groups[0]['lr'] = lr2\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss.item() / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            logging('| epoch {:3d} | {}/{} batches | lr {:02.4f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt0, optimizer.param_groups[0]['lr'],\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)), log_=is_logfile)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "        ###\n",
    "        batch += 1\n",
    "        i += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer parameters\n",
    "# Learning rate\n",
    "lr = 20\n",
    "# Weight decay applied to all weights\n",
    "wdecay = 1.2e-6\n",
    "# Numbr of epochs\n",
    "num_epoch = 100\n",
    "# Beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\n",
    "beta = 1\n",
    "epoch = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : PTB-20180522-212715\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = []\n",
    "stored_loss = 100000000\n",
    "exp_dir = '{}-{}'.format(\"PTB\", time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "create_exp_dir(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-py3.6/lib/python3.6/site-packages/torch/nn/modules/module.py:491: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | 200/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  6.98 | ppl  1078.04\n",
      "| epoch   1 | 400/1106 batches | lr 20.0000 | ms/batch 85.19 | loss  6.48 | ppl   650.80\n",
      "| epoch   1 | 600/1106 batches | lr 20.0000 | ms/batch 85.16 | loss  6.24 | ppl   511.66\n",
      "| epoch   1 | 800/1106 batches | lr 20.0000 | ms/batch 83.85 | loss  6.11 | ppl   449.99\n",
      "| epoch   1 | 1000/1106 batches | lr 20.0000 | ms/batch 85.88 | loss  5.97 | ppl   390.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 100.17s | valid loss  5.75 | valid ppl   315.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   2 | 200/1106 batches | lr 20.0000 | ms/batch 83.62 | loss  5.81 | ppl   332.66\n",
      "| epoch   2 | 400/1106 batches | lr 20.0000 | ms/batch 85.33 | loss  5.66 | ppl   287.57\n",
      "| epoch   2 | 600/1106 batches | lr 20.0000 | ms/batch 83.66 | loss  5.57 | ppl   261.23\n",
      "| epoch   2 | 800/1106 batches | lr 20.0000 | ms/batch 85.65 | loss  5.54 | ppl   254.76\n",
      "| epoch   2 | 1000/1106 batches | lr 20.0000 | ms/batch 84.75 | loss  5.49 | ppl   243.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 99.35s | valid loss  5.33 | valid ppl   206.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   3 | 200/1106 batches | lr 20.0000 | ms/batch 82.03 | loss  5.42 | ppl   226.86\n",
      "| epoch   3 | 400/1106 batches | lr 20.0000 | ms/batch 84.53 | loss  5.31 | ppl   201.59\n",
      "| epoch   3 | 600/1106 batches | lr 20.0000 | ms/batch 85.28 | loss  5.25 | ppl   190.78\n",
      "| epoch   3 | 800/1106 batches | lr 20.0000 | ms/batch 85.75 | loss  5.25 | ppl   191.47\n",
      "| epoch   3 | 1000/1106 batches | lr 20.0000 | ms/batch 84.91 | loss  5.24 | ppl   188.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 99.82s | valid loss  5.13 | valid ppl   169.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   4 | 200/1106 batches | lr 20.0000 | ms/batch 85.24 | loss  5.19 | ppl   179.45\n",
      "| epoch   4 | 400/1106 batches | lr 20.0000 | ms/batch 83.69 | loss  5.08 | ppl   160.55\n",
      "| epoch   4 | 600/1106 batches | lr 20.0000 | ms/batch 86.53 | loss  5.04 | ppl   154.13\n",
      "| epoch   4 | 800/1106 batches | lr 20.0000 | ms/batch 86.53 | loss  5.07 | ppl   158.40\n",
      "| epoch   4 | 1000/1106 batches | lr 20.0000 | ms/batch 85.56 | loss  5.05 | ppl   156.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 99.90s | valid loss  4.99 | valid ppl   146.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   5 | 200/1106 batches | lr 20.0000 | ms/batch 86.17 | loss  5.05 | ppl   156.06\n",
      "| epoch   5 | 400/1106 batches | lr 20.0000 | ms/batch 84.89 | loss  4.94 | ppl   139.75\n",
      "| epoch   5 | 600/1106 batches | lr 20.0000 | ms/batch 85.60 | loss  4.90 | ppl   134.14\n",
      "| epoch   5 | 800/1106 batches | lr 20.0000 | ms/batch 84.03 | loss  4.92 | ppl   137.33\n",
      "| epoch   5 | 1000/1106 batches | lr 20.0000 | ms/batch 86.46 | loss  4.93 | ppl   138.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 100.52s | valid loss  4.89 | valid ppl   132.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   6 | 200/1106 batches | lr 20.0000 | ms/batch 85.42 | loss  4.92 | ppl   136.77\n",
      "| epoch   6 | 400/1106 batches | lr 20.0000 | ms/batch 86.71 | loss  4.81 | ppl   122.63\n",
      "| epoch   6 | 600/1106 batches | lr 20.0000 | ms/batch 85.40 | loss  4.78 | ppl   119.43\n",
      "| epoch   6 | 800/1106 batches | lr 20.0000 | ms/batch 85.66 | loss  4.81 | ppl   122.57\n",
      "| epoch   6 | 1000/1106 batches | lr 20.0000 | ms/batch 85.37 | loss  4.82 | ppl   123.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 100.25s | valid loss  4.80 | valid ppl   121.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   7 | 200/1106 batches | lr 20.0000 | ms/batch 86.24 | loss  4.82 | ppl   124.37\n",
      "| epoch   7 | 400/1106 batches | lr 20.0000 | ms/batch 85.02 | loss  4.70 | ppl   110.23\n",
      "| epoch   7 | 600/1106 batches | lr 20.0000 | ms/batch 85.52 | loss  4.70 | ppl   109.48\n",
      "| epoch   7 | 800/1106 batches | lr 20.0000 | ms/batch 86.50 | loss  4.72 | ppl   111.93\n",
      "| epoch   7 | 1000/1106 batches | lr 20.0000 | ms/batch 84.51 | loss  4.72 | ppl   112.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 100.62s | valid loss  4.74 | valid ppl   114.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   8 | 200/1106 batches | lr 20.0000 | ms/batch 86.20 | loss  4.74 | ppl   114.17\n",
      "| epoch   8 | 400/1106 batches | lr 20.0000 | ms/batch 83.87 | loss  4.61 | ppl   100.24\n",
      "| epoch   8 | 600/1106 batches | lr 20.0000 | ms/batch 85.80 | loss  4.61 | ppl   100.36\n",
      "| epoch   8 | 800/1106 batches | lr 20.0000 | ms/batch 85.58 | loss  4.63 | ppl   102.43\n",
      "| epoch   8 | 1000/1106 batches | lr 20.0000 | ms/batch 85.37 | loss  4.67 | ppl   106.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 100.62s | valid loss  4.68 | valid ppl   107.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch   9 | 200/1106 batches | lr 20.0000 | ms/batch 85.31 | loss  4.67 | ppl   106.49\n",
      "| epoch   9 | 400/1106 batches | lr 20.0000 | ms/batch 86.72 | loss  4.56 | ppl    95.49\n",
      "| epoch   9 | 600/1106 batches | lr 20.0000 | ms/batch 84.47 | loss  4.54 | ppl    93.35\n",
      "| epoch   9 | 800/1106 batches | lr 20.0000 | ms/batch 83.86 | loss  4.57 | ppl    96.96\n",
      "| epoch   9 | 1000/1106 batches | lr 20.0000 | ms/batch 85.05 | loss  4.59 | ppl    98.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 99.68s | valid loss  4.66 | valid ppl   105.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  10 | 200/1106 batches | lr 20.0000 | ms/batch 84.40 | loss  4.61 | ppl   100.22\n",
      "| epoch  10 | 400/1106 batches | lr 20.0000 | ms/batch 84.63 | loss  4.48 | ppl    88.40\n",
      "| epoch  10 | 600/1106 batches | lr 20.0000 | ms/batch 84.40 | loss  4.48 | ppl    88.58\n",
      "| epoch  10 | 800/1106 batches | lr 20.0000 | ms/batch 83.06 | loss  4.51 | ppl    91.26\n",
      "| epoch  10 | 1000/1106 batches | lr 20.0000 | ms/batch 87.14 | loss  4.53 | ppl    92.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 99.91s | valid loss  4.62 | valid ppl   101.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  11 | 200/1106 batches | lr 20.0000 | ms/batch 84.23 | loss  4.55 | ppl    94.40\n",
      "| epoch  11 | 400/1106 batches | lr 20.0000 | ms/batch 86.22 | loss  4.43 | ppl    83.78\n",
      "| epoch  11 | 600/1106 batches | lr 20.0000 | ms/batch 85.47 | loss  4.43 | ppl    83.97\n",
      "| epoch  11 | 800/1106 batches | lr 20.0000 | ms/batch 84.77 | loss  4.46 | ppl    86.29\n",
      "| epoch  11 | 1000/1106 batches | lr 20.0000 | ms/batch 84.44 | loss  4.48 | ppl    88.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 100.03s | valid loss  4.59 | valid ppl    98.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  12 | 200/1106 batches | lr 20.0000 | ms/batch 87.45 | loss  4.51 | ppl    90.65\n",
      "| epoch  12 | 400/1106 batches | lr 20.0000 | ms/batch 87.09 | loss  4.38 | ppl    79.97\n",
      "| epoch  12 | 600/1106 batches | lr 20.0000 | ms/batch 85.74 | loss  4.38 | ppl    79.89\n",
      "| epoch  12 | 800/1106 batches | lr 20.0000 | ms/batch 85.08 | loss  4.42 | ppl    83.28\n",
      "| epoch  12 | 1000/1106 batches | lr 20.0000 | ms/batch 84.69 | loss  4.44 | ppl    85.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 100.66s | valid loss  4.56 | valid ppl    95.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  13 | 200/1106 batches | lr 20.0000 | ms/batch 83.58 | loss  4.46 | ppl    86.63\n",
      "| epoch  13 | 400/1106 batches | lr 20.0000 | ms/batch 84.76 | loss  4.35 | ppl    77.66\n",
      "| epoch  13 | 600/1106 batches | lr 20.0000 | ms/batch 84.85 | loss  4.34 | ppl    76.83\n",
      "| epoch  13 | 800/1106 batches | lr 20.0000 | ms/batch 85.83 | loss  4.37 | ppl    79.17\n",
      "| epoch  13 | 1000/1106 batches | lr 20.0000 | ms/batch 87.00 | loss  4.40 | ppl    81.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 100.50s | valid loss  4.54 | valid ppl    93.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  14 | 200/1106 batches | lr 20.0000 | ms/batch 89.35 | loss  4.41 | ppl    82.32\n",
      "| epoch  14 | 400/1106 batches | lr 20.0000 | ms/batch 86.85 | loss  4.29 | ppl    73.26\n",
      "| epoch  14 | 600/1106 batches | lr 20.0000 | ms/batch 86.02 | loss  4.30 | ppl    74.04\n",
      "| epoch  14 | 800/1106 batches | lr 20.0000 | ms/batch 87.29 | loss  4.33 | ppl    76.16\n",
      "| epoch  14 | 1000/1106 batches | lr 20.0000 | ms/batch 86.27 | loss  4.36 | ppl    78.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 101.24s | valid loss  4.53 | valid ppl    92.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  15 | 200/1106 batches | lr 20.0000 | ms/batch 86.60 | loss  4.38 | ppl    79.83\n",
      "| epoch  15 | 400/1106 batches | lr 20.0000 | ms/batch 86.22 | loss  4.26 | ppl    71.03\n",
      "| epoch  15 | 600/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  4.27 | ppl    71.82\n",
      "| epoch  15 | 800/1106 batches | lr 20.0000 | ms/batch 85.50 | loss  4.29 | ppl    72.89\n",
      "| epoch  15 | 1000/1106 batches | lr 20.0000 | ms/batch 87.74 | loss  4.33 | ppl    76.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 101.77s | valid loss  4.51 | valid ppl    91.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  16 | 200/1106 batches | lr 20.0000 | ms/batch 86.13 | loss  4.36 | ppl    77.98\n",
      "| epoch  16 | 400/1106 batches | lr 20.0000 | ms/batch 85.87 | loss  4.23 | ppl    68.77\n",
      "| epoch  16 | 600/1106 batches | lr 20.0000 | ms/batch 85.60 | loss  4.24 | ppl    69.35\n",
      "| epoch  16 | 800/1106 batches | lr 20.0000 | ms/batch 86.31 | loss  4.25 | ppl    70.36\n",
      "| epoch  16 | 1000/1106 batches | lr 20.0000 | ms/batch 86.95 | loss  4.30 | ppl    73.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 101.52s | valid loss  4.50 | valid ppl    90.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  17 | 200/1106 batches | lr 20.0000 | ms/batch 87.48 | loss  4.33 | ppl    76.16\n",
      "| epoch  17 | 400/1106 batches | lr 20.0000 | ms/batch 86.07 | loss  4.19 | ppl    66.17\n",
      "| epoch  17 | 600/1106 batches | lr 20.0000 | ms/batch 87.14 | loss  4.21 | ppl    67.60\n",
      "| epoch  17 | 800/1106 batches | lr 20.0000 | ms/batch 89.11 | loss  4.26 | ppl    70.48\n",
      "| epoch  17 | 1000/1106 batches | lr 20.0000 | ms/batch 85.29 | loss  4.27 | ppl    71.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 101.61s | valid loss  4.49 | valid ppl    89.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  18 | 200/1106 batches | lr 20.0000 | ms/batch 86.00 | loss  4.29 | ppl    72.82\n",
      "| epoch  18 | 400/1106 batches | lr 20.0000 | ms/batch 85.13 | loss  4.17 | ppl    65.02\n",
      "| epoch  18 | 600/1106 batches | lr 20.0000 | ms/batch 85.98 | loss  4.18 | ppl    65.55\n",
      "| epoch  18 | 800/1106 batches | lr 20.0000 | ms/batch 88.46 | loss  4.21 | ppl    67.11\n",
      "| epoch  18 | 1000/1106 batches | lr 20.0000 | ms/batch 85.28 | loss  4.24 | ppl    69.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 101.25s | valid loss  4.48 | valid ppl    87.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  19 | 200/1106 batches | lr 20.0000 | ms/batch 87.73 | loss  4.27 | ppl    71.31\n",
      "| epoch  19 | 400/1106 batches | lr 20.0000 | ms/batch 84.12 | loss  4.15 | ppl    63.27\n",
      "| epoch  19 | 600/1106 batches | lr 20.0000 | ms/batch 86.17 | loss  4.16 | ppl    63.98\n",
      "| epoch  19 | 800/1106 batches | lr 20.0000 | ms/batch 86.97 | loss  4.18 | ppl    65.04\n",
      "| epoch  19 | 1000/1106 batches | lr 20.0000 | ms/batch 86.70 | loss  4.22 | ppl    68.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 101.64s | valid loss  4.47 | valid ppl    87.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  20 | 200/1106 batches | lr 20.0000 | ms/batch 88.70 | loss  4.25 | ppl    69.95\n",
      "| epoch  20 | 400/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  4.12 | ppl    61.38\n",
      "| epoch  20 | 600/1106 batches | lr 20.0000 | ms/batch 85.22 | loss  4.14 | ppl    62.49\n",
      "| epoch  20 | 800/1106 batches | lr 20.0000 | ms/batch 86.01 | loss  4.17 | ppl    64.44\n",
      "| epoch  20 | 1000/1106 batches | lr 20.0000 | ms/batch 86.13 | loss  4.19 | ppl    65.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 101.44s | valid loss  4.46 | valid ppl    86.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  21 | 200/1106 batches | lr 20.0000 | ms/batch 86.63 | loss  4.21 | ppl    67.28\n",
      "| epoch  21 | 400/1106 batches | lr 20.0000 | ms/batch 87.07 | loss  4.11 | ppl    60.76\n",
      "| epoch  21 | 600/1106 batches | lr 20.0000 | ms/batch 86.32 | loss  4.12 | ppl    61.51\n",
      "| epoch  21 | 800/1106 batches | lr 20.0000 | ms/batch 87.23 | loss  4.15 | ppl    63.14\n",
      "| epoch  21 | 1000/1106 batches | lr 20.0000 | ms/batch 86.37 | loss  4.16 | ppl    64.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 101.63s | valid loss  4.44 | valid ppl    85.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  22 | 200/1106 batches | lr 20.0000 | ms/batch 88.27 | loss  4.19 | ppl    66.19\n",
      "| epoch  22 | 400/1106 batches | lr 20.0000 | ms/batch 84.47 | loss  4.07 | ppl    58.64\n",
      "| epoch  22 | 600/1106 batches | lr 20.0000 | ms/batch 86.72 | loss  4.09 | ppl    59.99\n",
      "| epoch  22 | 800/1106 batches | lr 20.0000 | ms/batch 85.45 | loss  4.12 | ppl    61.64\n",
      "| epoch  22 | 1000/1106 batches | lr 20.0000 | ms/batch 87.68 | loss  4.16 | ppl    64.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 101.48s | valid loss  4.45 | valid ppl    85.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 200/1106 batches | lr 20.0000 | ms/batch 87.20 | loss  4.17 | ppl    64.82\n",
      "| epoch  23 | 400/1106 batches | lr 20.0000 | ms/batch 86.53 | loss  4.06 | ppl    58.05\n",
      "| epoch  23 | 600/1106 batches | lr 20.0000 | ms/batch 86.58 | loss  4.07 | ppl    58.70\n",
      "| epoch  23 | 800/1106 batches | lr 20.0000 | ms/batch 87.25 | loss  4.11 | ppl    60.80\n",
      "| epoch  23 | 1000/1106 batches | lr 20.0000 | ms/batch 84.99 | loss  4.13 | ppl    62.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 101.54s | valid loss  4.44 | valid ppl    85.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 | 200/1106 batches | lr 20.0000 | ms/batch 85.46 | loss  4.15 | ppl    63.64\n",
      "| epoch  24 | 400/1106 batches | lr 20.0000 | ms/batch 85.46 | loss  4.05 | ppl    57.28\n",
      "| epoch  24 | 600/1106 batches | lr 20.0000 | ms/batch 84.21 | loss  4.06 | ppl    57.93\n",
      "| epoch  24 | 800/1106 batches | lr 20.0000 | ms/batch 85.52 | loss  4.08 | ppl    59.24\n",
      "| epoch  24 | 1000/1106 batches | lr 20.0000 | ms/batch 85.20 | loss  4.13 | ppl    61.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 101.40s | valid loss  4.43 | valid ppl    83.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  25 | 200/1106 batches | lr 20.0000 | ms/batch 87.58 | loss  4.13 | ppl    62.26\n",
      "| epoch  25 | 400/1106 batches | lr 20.0000 | ms/batch 84.84 | loss  4.03 | ppl    56.12\n",
      "| epoch  25 | 600/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  4.04 | ppl    56.82\n",
      "| epoch  25 | 800/1106 batches | lr 20.0000 | ms/batch 87.12 | loss  4.07 | ppl    58.58\n",
      "| epoch  25 | 1000/1106 batches | lr 20.0000 | ms/batch 87.22 | loss  4.09 | ppl    59.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 101.46s | valid loss  4.42 | valid ppl    83.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Saving Normal!\n",
      "| epoch  26 | 200/1106 batches | lr 20.0000 | ms/batch 86.11 | loss  4.12 | ppl    61.42\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    optimizer = torch.optim.SGD(parallel_model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "    while epoch < num_epoch:\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        train(parallel_model, train_data, optimizer, ntokens, batch_size, small_batch_size, bptt)     \n",
    "        val_loss = evaluate(val_data, parallel_model, ntokens, eval_batch_size, bptt)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "        logging('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)), log_=is_logfile)\n",
    "        logging('-' * 89, log_=is_logfile)\n",
    "\n",
    "        if val_loss < stored_loss:\n",
    "            save_checkpoint(parallel_model, optimizer, exp_dir)\n",
    "            logging('Saving Normal!', log_=is_logfile)\n",
    "            stored_loss = val_loss\n",
    "        best_val_loss.append(val_loss)\n",
    "        epoch += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging('-' * 89, log_=is_logfile)\n",
    "    logging('Exiting from training early', log_=is_logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(test_data, parallel_model, ntokens, test_batch_size, bptt)\n",
    "logging('=' * 89, log_=is_logfile)\n",
    "logging('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)), log_=is_logfile)\n",
    "logging('=' * 89, log_=is_logfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
